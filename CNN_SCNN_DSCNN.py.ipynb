{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data preprocessing...\n",
      "saving data to dataX.npy, dataY.npy...\n",
      "data features shape: (35887, 48, 48)\n",
      "data labels shape: (35887,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "print('Start data preprocessing...')\n",
    "data = pd.read_csv(\"fer2013.csv\", delimiter=',')\n",
    "\n",
    "#getting features and labels \n",
    "d_X = data['pixels'].values\n",
    "dataY = data['emotion'].values\n",
    "\n",
    "lenth, width = 48, 48\n",
    "\n",
    "dataX = []\n",
    "for seq in d_X:\n",
    "    x = [float(i) for i in seq.split(' ')]\n",
    "    x = np.array(x).reshape(lenth, width)\n",
    "    dataX.append(x)\n",
    "\n",
    "dataX = np.array(dataX)\n",
    "\n",
    "print('saving data to dataX.npy, dataY.npy...')\n",
    "if(len(dataX)==0 or len(dataY)==0):\n",
    "    raise AssertionError('There is no images inside!')\n",
    "np.save('dataX', dataX)\n",
    "np.save('dataY', dataY)\n",
    "\n",
    "print('data features shape:', dataX.shape)\n",
    "print('data labels shape:', dataY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35887, 48, 48)\n",
      "Image file found!\n",
      "d_sift saved!\n",
      "(35887, 48, 48)\n",
      "Image file found!\n",
      "SIFT descriptors ready to cluster...\n",
      "clustering..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dheeresh\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 6144 or by setting the environment variable OMP_NUM_THREADS=16\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 1190983470.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 1187674047.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 1185950624.0\n",
      "[MiniBatchKMeans] Reassigning 474 cluster centers.\n",
      "Minibatch step 1/41783: mean batch inertia: 96339.8134765625\n",
      "Minibatch step 2/41783: mean batch inertia: 84917.10381870074, ewa inertia: 84917.10381870074\n",
      "Minibatch step 3/41783: mean batch inertia: 80601.11897203472, ewa inertia: 84875.78587687183\n",
      "Minibatch step 4/41783: mean batch inertia: 78586.67528725456, ewa inertia: 84815.57873650196\n",
      "Minibatch step 5/41783: mean batch inertia: 78359.82957812469, ewa inertia: 84753.77632577434\n",
      "Minibatch step 6/41783: mean batch inertia: 76378.92823399755, ewa inertia: 84673.60192790793\n",
      "Minibatch step 7/41783: mean batch inertia: 75708.93896845196, ewa inertia: 84587.78109307328\n",
      "Minibatch step 8/41783: mean batch inertia: 75515.22223724295, ewa inertia: 84500.92734535123\n",
      "Minibatch step 9/41783: mean batch inertia: 75010.81010773743, ewa inertia: 84410.07621266569\n",
      "Minibatch step 10/41783: mean batch inertia: 74101.13860824448, ewa inertia: 84311.38631930652\n",
      "[MiniBatchKMeans] Reassigning 338 cluster centers.\n",
      "Minibatch step 11/41783: mean batch inertia: 73808.75321425682, ewa inertia: 84210.84213319335\n",
      "Minibatch step 12/41783: mean batch inertia: 73076.76950341699, ewa inertia: 84104.25302744353\n",
      "Minibatch step 13/41783: mean batch inertia: 73666.71120102797, ewa inertia: 84004.33197550708\n",
      "Minibatch step 14/41783: mean batch inertia: 73415.31717611456, ewa inertia: 83902.96083704151\n",
      "Minibatch step 15/41783: mean batch inertia: 72780.2686475748, ewa inertia: 83796.4806789334\n",
      "Minibatch step 16/41783: mean batch inertia: 72967.8681071201, ewa inertia: 83692.81581442963\n",
      "Minibatch step 17/41783: mean batch inertia: 72927.3014229735, ewa inertia: 83589.75500370226\n",
      "Minibatch step 18/41783: mean batch inertia: 72088.0578813372, ewa inertia: 83479.64654189831\n",
      "Minibatch step 19/41783: mean batch inertia: 72317.02331977675, ewa inertia: 83372.7841145879\n",
      "Minibatch step 20/41783: mean batch inertia: 72051.51196482568, ewa inertia: 83264.40290364032\n",
      "Minibatch step 21/41783: mean batch inertia: 72258.39432666345, ewa inertia: 83159.03978530054\n",
      "Minibatch step 22/41783: mean batch inertia: 71989.30957520634, ewa inertia: 83052.10932111651\n",
      "Minibatch step 23/41783: mean batch inertia: 72008.40832918788, ewa inertia: 82946.3853643902\n",
      "Minibatch step 24/41783: mean batch inertia: 71203.29265921768, ewa inertia: 82833.96596568481\n",
      "Minibatch step 25/41783: mean batch inertia: 71314.07066183357, ewa inertia: 82723.68328839337\n",
      "[MiniBatchKMeans] Reassigning 134 cluster centers.\n",
      "Minibatch step 26/41783: mean batch inertia: 71488.176282209, ewa inertia: 82616.12312741218\n",
      "Minibatch step 27/41783: mean batch inertia: 71137.58814720843, ewa inertia: 82506.23640226312\n",
      "Minibatch step 28/41783: mean batch inertia: 71179.73332561339, ewa inertia: 82397.80511441756\n",
      "Minibatch step 29/41783: mean batch inertia: 70572.04675276195, ewa inertia: 82284.59433786744\n",
      "Minibatch step 30/41783: mean batch inertia: 70436.26855897883, ewa inertia: 82171.16751810459\n",
      "Minibatch step 31/41783: mean batch inertia: 71297.16873783825, ewa inertia: 82067.06816071279\n",
      "Minibatch step 32/41783: mean batch inertia: 71157.06277993058, ewa inertia: 81962.62410363404\n",
      "Minibatch step 33/41783: mean batch inertia: 70534.6097508709, ewa inertia: 81853.22102437806\n",
      "Minibatch step 34/41783: mean batch inertia: 70262.11301581636, ewa inertia: 81742.25661105296\n",
      "Minibatch step 35/41783: mean batch inertia: 70369.4827558951, ewa inertia: 81633.38236212141\n",
      "Minibatch step 36/41783: mean batch inertia: 70847.3274813571, ewa inertia: 81530.12491244082\n",
      "Minibatch step 37/41783: mean batch inertia: 70377.32516069272, ewa inertia: 81423.35652753137\n",
      "Minibatch step 38/41783: mean batch inertia: 69720.31797942535, ewa inertia: 81311.32057672733\n",
      "Minibatch step 39/41783: mean batch inertia: 70324.50347710245, ewa inertia: 81206.14118293085\n",
      "Minibatch step 40/41783: mean batch inertia: 70501.1086886239, ewa inertia: 81103.65937968309\n",
      "[MiniBatchKMeans] Reassigning 76 cluster centers.\n",
      "Minibatch step 41/41783: mean batch inertia: 70484.12522964142, ewa inertia: 81001.99607226891\n",
      "Minibatch step 42/41783: mean batch inertia: 70387.11718511848, ewa inertia: 80900.37733078538\n",
      "Minibatch step 43/41783: mean batch inertia: 70326.83820516047, ewa inertia: 80799.1543445982\n",
      "Minibatch step 44/41783: mean batch inertia: 70171.29483479343, ewa inertia: 80697.41133654962\n",
      "Minibatch step 45/41783: mean batch inertia: 69514.44400990102, ewa inertia: 80590.35415032387\n",
      "Minibatch step 46/41783: mean batch inertia: 70797.3220440916, ewa inertia: 80496.60314209071\n",
      "Minibatch step 47/41783: mean batch inertia: 70535.8531976629, ewa inertia: 80401.2465313987\n",
      "Minibatch step 48/41783: mean batch inertia: 70171.79648352372, ewa inertia: 80303.3175911833\n",
      "Minibatch step 49/41783: mean batch inertia: 69696.00703644125, ewa inertia: 80201.77130313261\n",
      "Minibatch step 50/41783: mean batch inertia: 70251.69073707971, ewa inertia: 80106.51683291854\n",
      "[MiniBatchKMeans] Reassigning 58 cluster centers.\n",
      "Minibatch step 51/41783: mean batch inertia: 70008.15448506468, ewa inertia: 80009.84282620183\n",
      "Minibatch step 52/41783: mean batch inertia: 70192.91689550248, ewa inertia: 79915.86307674636\n",
      "Minibatch step 53/41783: mean batch inertia: 70141.05274820898, ewa inertia: 79822.28650989277\n",
      "Minibatch step 54/41783: mean batch inertia: 69570.31911193975, ewa inertia: 79724.1420057693\n",
      "Minibatch step 55/41783: mean batch inertia: 69790.77771101339, ewa inertia: 79629.04756436664\n",
      "Minibatch step 56/41783: mean batch inertia: 69339.22903701468, ewa inertia: 79530.54070244577\n",
      "Minibatch step 57/41783: mean batch inertia: 69376.76401278854, ewa inertia: 79433.33620115308\n",
      "Minibatch step 58/41783: mean batch inertia: 69833.24563720309, ewa inertia: 79341.43226785268\n",
      "Minibatch step 59/41783: mean batch inertia: 69969.92823425373, ewa inertia: 79251.71664737067\n",
      "Minibatch step 60/41783: mean batch inertia: 69614.74342026822, ewa inertia: 79159.45962762888\n",
      "Minibatch step 61/41783: mean batch inertia: 69755.28106014695, ewa inertia: 79069.4312061204\n",
      "Minibatch step 62/41783: mean batch inertia: 69863.37317295528, ewa inertia: 78981.29943925804\n",
      "Minibatch step 63/41783: mean batch inertia: 69445.43531673236, ewa inertia: 78890.01036084359\n",
      "Minibatch step 64/41783: mean batch inertia: 69558.30271379823, ewa inertia: 78800.67572056495\n",
      "Minibatch step 65/41783: mean batch inertia: 69575.24060983471, ewa inertia: 78712.35845236463\n",
      "Minibatch step 66/41783: mean batch inertia: 69495.02058105607, ewa inertia: 78624.11870094885\n",
      "Minibatch step 67/41783: mean batch inertia: 69343.66929969017, ewa inertia: 78535.27476795786\n",
      "Minibatch step 68/41783: mean batch inertia: 69789.37345709944, ewa inertia: 78451.54819002918\n",
      "Minibatch step 69/41783: mean batch inertia: 69309.3187337466, ewa inertia: 78364.02746920158\n",
      "Minibatch step 70/41783: mean batch inertia: 68818.43205933142, ewa inertia: 78272.64523087608\n",
      "[MiniBatchKMeans] Reassigning 23 cluster centers.\n",
      "Minibatch step 71/41783: mean batch inertia: 69156.05216695419, ewa inertia: 78185.36993328463\n",
      "Minibatch step 72/41783: mean batch inertia: 69812.72901096837, ewa inertia: 78105.21666517253\n",
      "Minibatch step 73/41783: mean batch inertia: 69570.78391895034, ewa inertia: 78023.51452573533\n",
      "Minibatch step 74/41783: mean batch inertia: 69570.8626519565, ewa inertia: 77942.59529389637\n",
      "Minibatch step 75/41783: mean batch inertia: 69179.36733584326, ewa inertia: 77858.70284388372\n",
      "Minibatch step 76/41783: mean batch inertia: 69788.30289155072, ewa inertia: 77781.44299992849\n",
      "Minibatch step 77/41783: mean batch inertia: 69384.07867165767, ewa inertia: 77701.05304881465\n",
      "Minibatch step 78/41783: mean batch inertia: 69635.14501616475, ewa inertia: 77623.83620706698\n",
      "Minibatch step 79/41783: mean batch inertia: 69574.02082918871, ewa inertia: 77546.7734241025\n",
      "Minibatch step 80/41783: mean batch inertia: 69002.18684098207, ewa inertia: 77464.97407958761\n",
      "Minibatch step 81/41783: mean batch inertia: 68642.41224665171, ewa inertia: 77380.51361238353\n",
      "Minibatch step 82/41783: mean batch inertia: 69369.96183692406, ewa inertia: 77303.82670915412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 83/41783: mean batch inertia: 69642.61715419982, ewa inertia: 77230.48414147222\n",
      "Minibatch step 84/41783: mean batch inertia: 69738.7741177091, ewa inertia: 77158.76423281684\n",
      "Minibatch step 85/41783: mean batch inertia: 69132.9834839711, ewa inertia: 77081.93153903005\n",
      "[MiniBatchKMeans] Reassigning 14 cluster centers.\n",
      "Minibatch step 86/41783: mean batch inertia: 68956.6974467517, ewa inertia: 77004.14675490822\n",
      "Minibatch step 87/41783: mean batch inertia: 69136.04736218198, ewa inertia: 76928.82358196801\n",
      "Minibatch step 88/41783: mean batch inertia: 68797.34272340819, ewa inertia: 76850.97899607787\n",
      "Minibatch step 89/41783: mean batch inertia: 69434.65882735046, ewa inertia: 76779.98081230232\n",
      "Minibatch step 90/41783: mean batch inertia: 68289.58557644144, ewa inertia: 76698.70025434793\n",
      "Minibatch step 91/41783: mean batch inertia: 68758.62076147571, ewa inertia: 76622.6879989164\n",
      "Minibatch step 92/41783: mean batch inertia: 69131.2834305347, ewa inertia: 76550.9710144575\n",
      "Minibatch step 93/41783: mean batch inertia: 68467.0645228769, ewa inertia: 76473.58186921352\n",
      "Minibatch step 94/41783: mean batch inertia: 69032.2216315452, ewa inertia: 76402.3439709467\n",
      "Minibatch step 95/41783: mean batch inertia: 68489.5100058677, ewa inertia: 76326.5925433847\n",
      "Minibatch step 96/41783: mean batch inertia: 68992.74513956418, ewa inertia: 76256.3838908573\n",
      "Minibatch step 97/41783: mean batch inertia: 68923.44211077242, ewa inertia: 76186.1839080797\n",
      "Minibatch step 98/41783: mean batch inertia: 68978.16338161333, ewa inertia: 76117.17982595366\n",
      "Minibatch step 99/41783: mean batch inertia: 68921.50183485613, ewa inertia: 76048.29390183245\n",
      "Minibatch step 100/41783: mean batch inertia: 69080.95348911616, ewa inertia: 75981.59390725366\n",
      "Minibatch step 101/41783: mean batch inertia: 69126.88616894605, ewa inertia: 75915.97217185661\n",
      "Minibatch step 102/41783: mean batch inertia: 68981.57189147516, ewa inertia: 75849.58752094097\n",
      "Minibatch step 103/41783: mean batch inertia: 68594.0690689401, ewa inertia: 75780.12872996076\n",
      "Minibatch step 104/41783: mean batch inertia: 69031.4484625548, ewa inertia: 75715.522020573\n",
      "Minibatch step 105/41783: mean batch inertia: 68910.44879292307, ewa inertia: 75650.3754480639\n",
      "[MiniBatchKMeans] Reassigning 10 cluster centers.\n",
      "Minibatch step 106/41783: mean batch inertia: 68489.65387277093, ewa inertia: 75581.8241699635\n",
      "Minibatch step 107/41783: mean batch inertia: 68916.82916210112, ewa inertia: 75518.01859932125\n",
      "Minibatch step 108/41783: mean batch inertia: 68941.31593597264, ewa inertia: 75455.05827213656\n",
      "Minibatch step 109/41783: mean batch inertia: 68848.27127990141, ewa inertia: 75391.8099405693\n",
      "Minibatch step 110/41783: mean batch inertia: 69248.02543903136, ewa inertia: 75332.99404114834\n",
      "Minibatch step 111/41783: mean batch inertia: 68956.73110701123, ewa inertia: 75271.95257081\n",
      "Minibatch step 112/41783: mean batch inertia: 68929.25318529338, ewa inertia: 75211.23241224607\n",
      "Minibatch step 113/41783: mean batch inertia: 68553.92790832276, ewa inertia: 75147.50046461356\n",
      "Minibatch step 114/41783: mean batch inertia: 69202.709261713, ewa inertia: 75090.58957501761\n",
      "Minibatch step 115/41783: mean batch inertia: 68612.16804848127, ewa inertia: 75028.57011635724\n",
      "Minibatch step 116/41783: mean batch inertia: 68397.93739767766, ewa inertia: 74965.09350402138\n",
      "Minibatch step 117/41783: mean batch inertia: 69105.67308779729, ewa inertia: 74908.9998891201\n",
      "Minibatch step 118/41783: mean batch inertia: 69132.77579758834, ewa inertia: 74853.70273227557\n",
      "Minibatch step 119/41783: mean batch inertia: 68741.76943623196, ewa inertia: 74795.19175196406\n",
      "Minibatch step 120/41783: mean batch inertia: 68663.44517036134, ewa inertia: 74736.49109439248\n",
      "[MiniBatchKMeans] Reassigning 6 cluster centers.\n",
      "Minibatch step 121/41783: mean batch inertia: 68844.64316296317, ewa inertia: 74680.08704323285\n",
      "Minibatch step 122/41783: mean batch inertia: 68972.36579390052, ewa inertia: 74625.44568026844\n",
      "Minibatch step 123/41783: mean batch inertia: 68986.74297221607, ewa inertia: 74571.46504809319\n",
      "Minibatch step 124/41783: mean batch inertia: 68880.73232284855, ewa inertia: 74516.98632028043\n",
      "Minibatch step 125/41783: mean batch inertia: 68711.67391859514, ewa inertia: 74461.41069417582\n",
      "Minibatch step 126/41783: mean batch inertia: 68791.60327136403, ewa inertia: 74407.13228922317\n",
      "Minibatch step 127/41783: mean batch inertia: 68469.3007211854, ewa inertia: 74350.28802585474\n",
      "Minibatch step 128/41783: mean batch inertia: 68613.84055570285, ewa inertia: 74295.37165999692\n",
      "Minibatch step 129/41783: mean batch inertia: 68837.30965903902, ewa inertia: 74243.12034395526\n",
      "Minibatch step 130/41783: mean batch inertia: 68619.58459129346, ewa inertia: 74189.2849086245\n",
      "Minibatch step 131/41783: mean batch inertia: 68202.34890310996, ewa inertia: 74131.97055688383\n",
      "Minibatch step 132/41783: mean batch inertia: 68458.94466185952, ewa inertia: 74077.66134073689\n",
      "Minibatch step 133/41783: mean batch inertia: 68928.34988722275, ewa inertia: 74028.36576657389\n",
      "Minibatch step 134/41783: mean batch inertia: 68423.53618514893, ewa inertia: 73974.70940983642\n",
      "Minibatch step 135/41783: mean batch inertia: 68862.16239451544, ewa inertia: 73925.76579031936\n",
      "Minibatch step 136/41783: mean batch inertia: 68667.08261383937, ewa inertia: 73875.42317483258\n",
      "Minibatch step 137/41783: mean batch inertia: 68227.83773795963, ewa inertia: 73821.35750619735\n",
      "Minibatch step 138/41783: mean batch inertia: 68760.55949549745, ewa inertia: 73772.90929211813\n",
      "Minibatch step 139/41783: mean batch inertia: 68214.74018629288, ewa inertia: 73719.6996271289\n",
      "Minibatch step 140/41783: mean batch inertia: 69379.4496769252, ewa inertia: 73678.14938973547\n",
      "[MiniBatchKMeans] Reassigning 3 cluster centers.\n",
      "Minibatch step 141/41783: mean batch inertia: 69195.82616038692, ewa inertia: 73635.23905130097\n",
      "Minibatch step 142/41783: mean batch inertia: 68289.18791611683, ewa inertia: 73584.06004172022\n",
      "Minibatch step 143/41783: mean batch inertia: 68539.47754270342, ewa inertia: 73535.7670625624\n",
      "Minibatch step 144/41783: mean batch inertia: 68693.36720324687, ewa inertia: 73489.4096255445\n",
      "Minibatch step 145/41783: mean batch inertia: 69084.83764706987, ewa inertia: 73447.24361818968\n",
      "Minibatch step 146/41783: mean batch inertia: 68120.13172341339, ewa inertia: 73396.24591842841\n",
      "Minibatch step 147/41783: mean batch inertia: 68381.73796845094, ewa inertia: 73348.24085002836\n",
      "Minibatch step 148/41783: mean batch inertia: 68423.78650190172, ewa inertia: 73301.09788601815\n",
      "Minibatch step 149/41783: mean batch inertia: 68942.9199990052, ewa inertia: 73259.37602025332\n",
      "Minibatch step 150/41783: mean batch inertia: 68495.36873069442, ewa inertia: 73213.76905381214\n",
      "Minibatch step 151/41783: mean batch inertia: 68478.15316755856, ewa inertia: 73168.43388497729\n",
      "Minibatch step 152/41783: mean batch inertia: 68416.51999608747, ewa inertia: 73122.94269151567\n",
      "Minibatch step 153/41783: mean batch inertia: 68816.80353608113, ewa inertia: 73081.71900481323\n",
      "Minibatch step 154/41783: mean batch inertia: 68430.40172374691, ewa inertia: 73037.19084642654\n",
      "Minibatch step 155/41783: mean batch inertia: 68448.2148562335, ewa inertia: 72993.25949593454\n",
      "Minibatch step 156/41783: mean batch inertia: 68493.40196590178, ewa inertia: 72950.18129750021\n",
      "Minibatch step 157/41783: mean batch inertia: 67970.81854353228, ewa inertia: 72902.51268235843\n",
      "Minibatch step 158/41783: mean batch inertia: 68237.84248865406, ewa inertia: 72857.85669338677\n",
      "Minibatch step 159/41783: mean batch inertia: 68475.07924605961, ewa inertia: 72815.89933022374\n",
      "Minibatch step 160/41783: mean batch inertia: 68367.98745350234, ewa inertia: 72773.31841979157\n",
      "Minibatch step 161/41783: mean batch inertia: 68070.05707215285, ewa inertia: 72728.29298859295\n",
      "Minibatch step 162/41783: mean batch inertia: 68254.31853599762, ewa inertia: 72685.46257496923\n",
      "Minibatch step 163/41783: mean batch inertia: 68479.14052692366, ewa inertia: 72645.19446100005\n",
      "Minibatch step 164/41783: mean batch inertia: 68209.1472163596, ewa inertia: 72602.72713349172\n",
      "Minibatch step 165/41783: mean batch inertia: 68984.85250141936, ewa inertia: 72568.09236555277\n",
      "Minibatch step 166/41783: mean batch inertia: 67935.42787888608, ewa inertia: 72523.74277477032\n",
      "Minibatch step 167/41783: mean batch inertia: 68227.04543082466, ewa inertia: 72482.60947675823\n",
      "Minibatch step 168/41783: mean batch inertia: 68484.090672881, ewa inertia: 72444.33071230241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 169/41783: mean batch inertia: 68584.42927354982, ewa inertia: 72407.37896460689\n",
      "Minibatch step 170/41783: mean batch inertia: 68573.29429146013, ewa inertia: 72370.67436690319\n",
      "Minibatch step 171/41783: mean batch inertia: 68606.41034444468, ewa inertia: 72334.63817877135\n",
      "Minibatch step 172/41783: mean batch inertia: 68322.05368588505, ewa inertia: 72296.2247601541\n",
      "Minibatch step 173/41783: mean batch inertia: 68528.5510131205, ewa inertia: 72260.15592992396\n",
      "Minibatch step 174/41783: mean batch inertia: 68081.72930033083, ewa inertia: 72220.15486538102\n",
      "Minibatch step 175/41783: mean batch inertia: 68838.95663749572, ewa inertia: 72187.7858565325\n",
      "[MiniBatchKMeans] Reassigning 4 cluster centers.\n",
      "Minibatch step 176/41783: mean batch inertia: 68431.55973737699, ewa inertia: 72151.82661714652\n",
      "Minibatch step 177/41783: mean batch inertia: 68411.8605073335, ewa inertia: 72116.02303866854\n",
      "Minibatch step 178/41783: mean batch inertia: 68650.11327028974, ewa inertia: 72082.84306624469\n",
      "Minibatch step 179/41783: mean batch inertia: 68467.41822565027, ewa inertia: 72048.2317507379\n",
      "Minibatch step 180/41783: mean batch inertia: 68051.68496019988, ewa inertia: 72009.97186483133\n",
      "Minibatch step 181/41783: mean batch inertia: 68651.13196713354, ewa inertia: 71977.81689755592\n",
      "Minibatch step 182/41783: mean batch inertia: 68535.47481871786, ewa inertia: 71944.86254418777\n",
      "Minibatch step 183/41783: mean batch inertia: 68291.51060078168, ewa inertia: 71909.88814357216\n",
      "Minibatch step 184/41783: mean batch inertia: 68329.8810590289, ewa inertia: 71875.61589060526\n",
      "Minibatch step 185/41783: mean batch inertia: 68812.30864993083, ewa inertia: 71846.29012722311\n",
      "Minibatch step 186/41783: mean batch inertia: 68211.0021862299, ewa inertia: 71811.48865756705\n",
      "Minibatch step 187/41783: mean batch inertia: 67966.80828094378, ewa inertia: 71774.68262469259\n",
      "Minibatch step 188/41783: mean batch inertia: 68743.34946960953, ewa inertia: 71745.66295678007\n",
      "Minibatch step 189/41783: mean batch inertia: 68700.42961095987, ewa inertia: 71716.51021906013\n",
      "Minibatch step 190/41783: mean batch inertia: 68216.56346604938, ewa inertia: 71683.00440254714\n",
      "[MiniBatchKMeans] Reassigning 4 cluster centers.\n",
      "Minibatch step 191/41783: mean batch inertia: 68449.04247893304, ewa inertia: 71652.04492159808\n",
      "Minibatch step 192/41783: mean batch inertia: 68686.76264995994, ewa inertia: 71623.65757438565\n",
      "Minibatch step 193/41783: mean batch inertia: 68222.850412214, ewa inertia: 71591.10084458005\n",
      "Minibatch step 194/41783: mean batch inertia: 68587.39736323549, ewa inertia: 71562.34568205687\n",
      "Minibatch step 195/41783: mean batch inertia: 68652.09705885134, ewa inertia: 71534.48518495291\n",
      "Minibatch step 196/41783: mean batch inertia: 68470.29251137526, ewa inertia: 71505.15094511256\n",
      "Minibatch step 197/41783: mean batch inertia: 67923.88616067581, ewa inertia: 71470.86665188767\n",
      "Minibatch step 198/41783: mean batch inertia: 68709.67055408166, ewa inertia: 71444.4330697968\n",
      "Minibatch step 199/41783: mean batch inertia: 68077.73453958042, ewa inertia: 71412.20286997707\n",
      "Minibatch step 200/41783: mean batch inertia: 68527.53299690333, ewa inertia: 71384.58724428661\n",
      "Minibatch step 201/41783: mean batch inertia: 68257.45306467781, ewa inertia: 71354.65045055017\n",
      "Minibatch step 202/41783: mean batch inertia: 69032.06995823148, ewa inertia: 71332.41583919636\n",
      "Minibatch step 203/41783: mean batch inertia: 67993.31373860363, ewa inertia: 71300.44982651221\n",
      "Minibatch step 204/41783: mean batch inertia: 68062.47486814886, ewa inertia: 71269.45192783412\n",
      "Minibatch step 205/41783: mean batch inertia: 68386.45336153073, ewa inertia: 71241.85230195792\n",
      "[MiniBatchKMeans] Reassigning 2 cluster centers.\n",
      "Minibatch step 206/41783: mean batch inertia: 68096.11251343662, ewa inertia: 71211.73739233505\n",
      "Minibatch step 207/41783: mean batch inertia: 68188.78696171814, ewa inertia: 71182.79797422279\n",
      "Minibatch step 208/41783: mean batch inertia: 68001.44444935952, ewa inertia: 71152.34212589446\n",
      "Minibatch step 209/41783: mean batch inertia: 67623.1527592363, ewa inertia: 71118.55636293761\n",
      "Minibatch step 210/41783: mean batch inertia: 68119.60503601504, ewa inertia: 71089.84669391057\n",
      "Minibatch step 211/41783: mean batch inertia: 68252.78231661268, ewa inertia: 71062.68680686959\n",
      "Minibatch step 212/41783: mean batch inertia: 68208.241130273, ewa inertia: 71035.36052454714\n",
      "Minibatch step 213/41783: mean batch inertia: 68451.66165149695, ewa inertia: 71010.62616529132\n",
      "Minibatch step 214/41783: mean batch inertia: 68325.68503321313, ewa inertia: 70984.92258998916\n",
      "Minibatch step 215/41783: mean batch inertia: 67907.41782358766, ewa inertia: 70955.46091034156\n",
      "Minibatch step 216/41783: mean batch inertia: 68633.63911794465, ewa inertia: 70933.2335622012\n",
      "Minibatch step 217/41783: mean batch inertia: 68521.1372354079, ewa inertia: 70910.1419946316\n",
      "Minibatch step 218/41783: mean batch inertia: 68237.1602757044, ewa inertia: 70884.55290961484\n",
      "Minibatch step 219/41783: mean batch inertia: 68175.62596718341, ewa inertia: 70858.61971248775\n",
      "Minibatch step 220/41783: mean batch inertia: 68425.82238128793, ewa inertia: 70835.329969316\n",
      "Minibatch step 221/41783: mean batch inertia: 67774.87300429396, ewa inertia: 70806.03149229554\n",
      "Minibatch step 222/41783: mean batch inertia: 68089.69565620011, ewa inertia: 70780.02736808035\n",
      "Minibatch step 223/41783: mean batch inertia: 68608.38649882974, ewa inertia: 70759.23773737108\n",
      "Minibatch step 224/41783: mean batch inertia: 68389.92265785052, ewa inertia: 70736.55572478115\n",
      "Minibatch step 225/41783: mean batch inertia: 68572.88746977753, ewa inertia: 70715.84241779019\n",
      "[MiniBatchKMeans] Reassigning 2 cluster centers.\n",
      "Minibatch step 226/41783: mean batch inertia: 68645.6763099445, ewa inertia: 70696.0242289274\n",
      "Minibatch step 227/41783: mean batch inertia: 67768.74162765036, ewa inertia: 70668.00066153008\n",
      "Minibatch step 228/41783: mean batch inertia: 68089.37072990624, ewa inertia: 70643.31482844718\n",
      "Minibatch step 229/41783: mean batch inertia: 68241.2393837594, ewa inertia: 70620.31919314765\n",
      "Minibatch step 230/41783: mean batch inertia: 67854.30834478415, ewa inertia: 70593.83951831303\n",
      "Minibatch step 231/41783: mean batch inertia: 68049.5065189144, ewa inertia: 70569.48201785894\n",
      "Minibatch step 232/41783: mean batch inertia: 68436.42014933635, ewa inertia: 70549.06171303114\n",
      "Minibatch step 233/41783: mean batch inertia: 68427.32360045816, ewa inertia: 70528.74981319229\n",
      "Minibatch step 234/41783: mean batch inertia: 68061.27344439461, ewa inertia: 70505.12807940679\n",
      "Minibatch step 235/41783: mean batch inertia: 68154.802463233, ewa inertia: 70482.62785743241\n",
      "Minibatch step 236/41783: mean batch inertia: 68524.16697274488, ewa inertia: 70463.87904904161\n",
      "Minibatch step 237/41783: mean batch inertia: 67941.2865115917, ewa inertia: 70439.7296751621\n",
      "Minibatch step 238/41783: mean batch inertia: 68105.03407307598, ewa inertia: 70417.37908300242\n",
      "Minibatch step 239/41783: mean batch inertia: 68110.68606332962, ewa inertia: 70395.29656617486\n",
      "Minibatch step 240/41783: mean batch inertia: 68495.94301101172, ewa inertia: 70377.1136062033\n",
      "Minibatch step 241/41783: mean batch inertia: 68095.93077504725, ewa inertia: 70355.2753044327\n",
      "Minibatch step 242/41783: mean batch inertia: 67917.2606000911, ewa inertia: 70331.93561411617\n",
      "Minibatch step 243/41783: mean batch inertia: 68374.0959305701, ewa inertia: 70313.19275263055\n",
      "Minibatch step 244/41783: mean batch inertia: 68497.204109091, ewa inertia: 70295.80786463826\n",
      "Minibatch step 245/41783: mean batch inertia: 68711.02267715266, ewa inertia: 70280.6363419119\n",
      "Minibatch step 246/41783: mean batch inertia: 68171.17245644069, ewa inertia: 70260.44194614672\n",
      "Minibatch step 247/41783: mean batch inertia: 68334.63188974533, ewa inertia: 70242.00571184313\n",
      "Minibatch step 248/41783: mean batch inertia: 68235.609785203, ewa inertia: 70222.79800997951\n",
      "Minibatch step 249/41783: mean batch inertia: 68306.72302324827, ewa inertia: 70204.45497179605\n",
      "Minibatch step 250/41783: mean batch inertia: 68340.16821813224, ewa inertia: 70186.6077145936\n",
      "Minibatch step 251/41783: mean batch inertia: 68057.96540955933, ewa inertia: 70166.22971929041\n",
      "Minibatch step 252/41783: mean batch inertia: 68479.33742926657, ewa inertia: 70150.08070116662\n",
      "Minibatch step 253/41783: mean batch inertia: 67852.78467181859, ewa inertia: 70128.08814394583\n",
      "Minibatch step 254/41783: mean batch inertia: 67956.01949896626, ewa inertia: 70107.29441803852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 255/41783: mean batch inertia: 68032.14805335019, ewa inertia: 70087.42855200128\n",
      "Minibatch step 256/41783: mean batch inertia: 67897.68501828863, ewa inertia: 70066.46562021154\n",
      "Minibatch step 257/41783: mean batch inertia: 68396.52960153544, ewa inertia: 70050.4789284763\n",
      "Minibatch step 258/41783: mean batch inertia: 68485.7108471678, ewa inertia: 70035.49903423288\n",
      "Minibatch step 259/41783: mean batch inertia: 68373.93079161814, ewa inertia: 70019.59244919328\n",
      "Minibatch step 260/41783: mean batch inertia: 67610.70097643892, ewa inertia: 69996.53156244797\n",
      "Minibatch step 261/41783: mean batch inertia: 67824.87457775087, ewa inertia: 69975.74177746172\n",
      "Minibatch step 262/41783: mean batch inertia: 68570.99996017401, ewa inertia: 69962.2938524218\n",
      "Minibatch step 263/41783: mean batch inertia: 68767.61505882132, ewa inertia: 69950.85691029807\n",
      "Minibatch step 264/41783: mean batch inertia: 68094.69655615477, ewa inertia: 69933.08744903666\n",
      "Minibatch step 265/41783: mean batch inertia: 68581.47841136421, ewa inertia: 69920.14817663927\n",
      "Minibatch step 266/41783: mean batch inertia: 67940.27868424606, ewa inertia: 69901.19441859289\n",
      "Minibatch step 267/41783: mean batch inertia: 68412.64270063922, ewa inertia: 69886.94416158857\n",
      "Minibatch step 268/41783: mean batch inertia: 68218.83400398547, ewa inertia: 69870.97494925241\n",
      "Minibatch step 269/41783: mean batch inertia: 67830.38732168931, ewa inertia: 69851.43992216408\n",
      "Minibatch step 270/41783: mean batch inertia: 68047.75255110417, ewa inertia: 69834.17279715724\n",
      "Minibatch step 271/41783: mean batch inertia: 67653.2142885185, ewa inertia: 69813.29396648647\n",
      "Minibatch step 272/41783: mean batch inertia: 68465.25958604224, ewa inertia: 69800.38891512662\n",
      "Minibatch step 273/41783: mean batch inertia: 67923.15013037501, ewa inertia: 69782.41766507294\n",
      "Minibatch step 274/41783: mean batch inertia: 68099.11911814378, ewa inertia: 69766.30305070017\n",
      "Minibatch step 275/41783: mean batch inertia: 68054.80781748552, ewa inertia: 69749.91850279308\n",
      "Minibatch step 276/41783: mean batch inertia: 67931.71668706159, ewa inertia: 69732.51242758092\n",
      "Minibatch step 277/41783: mean batch inertia: 68260.73236340292, ewa inertia: 69718.42272957755\n",
      "Minibatch step 278/41783: mean batch inertia: 68785.03300539622, ewa inertia: 69709.48716939893\n",
      "Minibatch step 279/41783: mean batch inertia: 68306.71405860425, ewa inertia: 69696.05809125103\n",
      "Minibatch step 280/41783: mean batch inertia: 68337.35633379963, ewa inertia: 69683.0509185728\n",
      "Minibatch step 281/41783: mean batch inertia: 68119.36616473972, ewa inertia: 69668.08139527906\n",
      "Minibatch step 282/41783: mean batch inertia: 67769.78628968685, ewa inertia: 69649.90856809512\n",
      "Minibatch step 283/41783: mean batch inertia: 67994.82097339994, ewa inertia: 69634.06402382797\n",
      "Minibatch step 284/41783: mean batch inertia: 68383.02588961634, ewa inertia: 69622.0875404316\n",
      "Minibatch step 285/41783: mean batch inertia: 68236.65296108469, ewa inertia: 69608.82444813845\n",
      "Minibatch step 286/41783: mean batch inertia: 68154.87762168857, ewa inertia: 69594.90547192981\n",
      "Minibatch step 287/41783: mean batch inertia: 68164.8344797367, ewa inertia: 69581.21506471897\n",
      "Minibatch step 288/41783: mean batch inertia: 68349.85524153331, ewa inertia: 69569.42696643958\n",
      "Minibatch step 289/41783: mean batch inertia: 68020.35382850372, ewa inertia: 69554.59732359425\n",
      "Minibatch step 290/41783: mean batch inertia: 68609.86864313268, ewa inertia: 69545.55321291032\n",
      "Minibatch step 291/41783: mean batch inertia: 67661.14992186158, ewa inertia: 69527.5133753465\n",
      "Minibatch step 292/41783: mean batch inertia: 68434.63245683006, ewa inertia: 69517.05096831279\n",
      "Minibatch step 293/41783: mean batch inertia: 68404.62828640873, ewa inertia: 69506.40148336485\n",
      "Minibatch step 294/41783: mean batch inertia: 68285.9430209912, ewa inertia: 69494.71774638606\n",
      "Minibatch step 295/41783: mean batch inertia: 68027.80735680486, ewa inertia: 69480.6746669272\n",
      "Minibatch step 296/41783: mean batch inertia: 68159.18740093517, ewa inertia: 69468.02375736108\n",
      "Minibatch step 297/41783: mean batch inertia: 68214.64362313118, ewa inertia: 69456.02485344565\n",
      "Minibatch step 298/41783: mean batch inertia: 68165.46418429402, ewa inertia: 69443.67001149205\n",
      "Minibatch step 299/41783: mean batch inertia: 67633.90851367908, ewa inertia: 69426.34473743587\n",
      "Minibatch step 300/41783: mean batch inertia: 67754.58497424603, ewa inertia: 69410.34058656385\n",
      "Minibatch step 301/41783: mean batch inertia: 67998.01200224366, ewa inertia: 69396.8200316121\n",
      "Minibatch step 302/41783: mean batch inertia: 68171.02004124367, ewa inertia: 69385.08515892479\n",
      "Minibatch step 303/41783: mean batch inertia: 68490.82699713853, ewa inertia: 69376.52421443092\n",
      "Minibatch step 304/41783: mean batch inertia: 68241.02122719189, ewa inertia: 69365.65377627134\n",
      "Minibatch step 305/41783: mean batch inertia: 68019.96867608854, ewa inertia: 69352.77121512603\n",
      "Minibatch step 306/41783: mean batch inertia: 68046.82233918723, ewa inertia: 69340.26905823611\n",
      "Minibatch step 307/41783: mean batch inertia: 68275.96332810131, ewa inertia: 69330.08020822662\n",
      "Minibatch step 308/41783: mean batch inertia: 68327.32389029939, ewa inertia: 69320.4805852704\n",
      "Minibatch step 309/41783: mean batch inertia: 68186.76307534253, ewa inertia: 69309.62723990664\n",
      "Minibatch step 310/41783: mean batch inertia: 67806.58495391213, ewa inertia: 69295.23826127361\n",
      "Minibatch step 311/41783: mean batch inertia: 68130.85608623721, ewa inertia: 69284.09135583164\n",
      "Minibatch step 312/41783: mean batch inertia: 67823.72942144844, ewa inertia: 69270.1109662803\n",
      "Minibatch step 313/41783: mean batch inertia: 67508.43542833501, ewa inertia: 69253.24603046979\n",
      "Minibatch step 314/41783: mean batch inertia: 68054.31797245578, ewa inertia: 69241.76840913467\n",
      "Minibatch step 315/41783: mean batch inertia: 68305.12403591734, ewa inertia: 69232.80169143248\n",
      "Minibatch step 316/41783: mean batch inertia: 67957.66664210483, ewa inertia: 69220.59452257905\n",
      "Minibatch step 317/41783: mean batch inertia: 67845.46486907496, ewa inertia: 69207.43008177371\n",
      "Minibatch step 318/41783: mean batch inertia: 67903.74607867659, ewa inertia: 69194.94960704618\n",
      "Minibatch step 319/41783: mean batch inertia: 67933.98393680569, ewa inertia: 69182.87808500376\n",
      "Minibatch step 320/41783: mean batch inertia: 68129.14910517696, ewa inertia: 69172.79048872186\n",
      "Minibatch step 321/41783: mean batch inertia: 68194.58187186408, ewa inertia: 69163.42586670307\n",
      "Minibatch step 322/41783: mean batch inertia: 68071.11900135035, ewa inertia: 69152.96895521581\n",
      "Minibatch step 323/41783: mean batch inertia: 68434.49642230765, ewa inertia: 69146.09084804558\n",
      "Minibatch step 324/41783: mean batch inertia: 68018.65380431044, ewa inertia: 69135.29762706734\n",
      "Minibatch step 325/41783: mean batch inertia: 68263.94076000678, ewa inertia: 69126.95592207427\n",
      "Minibatch step 326/41783: mean batch inertia: 68084.64895579588, ewa inertia: 69116.97767142452\n",
      "Minibatch step 327/41783: mean batch inertia: 68279.8129574687, ewa inertia: 69108.96329598501\n",
      "Minibatch step 328/41783: mean batch inertia: 68345.96667299437, ewa inertia: 69101.65894918947\n",
      "Minibatch step 329/41783: mean batch inertia: 67713.79740637977, ewa inertia: 69088.37262300213\n",
      "Minibatch step 330/41783: mean batch inertia: 68145.81161539174, ewa inertia: 69079.34926396213\n",
      "Minibatch step 331/41783: mean batch inertia: 68185.47456264538, ewa inertia: 69070.79199042585\n",
      "Minibatch step 332/41783: mean batch inertia: 68396.22789428613, ewa inertia: 69064.33422908645\n",
      "Minibatch step 333/41783: mean batch inertia: 67979.42551569377, ewa inertia: 69053.9481418544\n",
      "Minibatch step 334/41783: mean batch inertia: 68217.83876691891, ewa inertia: 69045.94386942446\n",
      "Minibatch step 335/41783: mean batch inertia: 67861.48500975873, ewa inertia: 69034.60476514095\n",
      "Minibatch step 336/41783: mean batch inertia: 67880.69077037653, ewa inertia: 69023.55807405998\n",
      "Minibatch step 337/41783: mean batch inertia: 68412.85869984608, ewa inertia: 69017.71170478039\n",
      "Minibatch step 338/41783: mean batch inertia: 67793.4870578412, ewa inertia: 69005.99191322764\n",
      "Minibatch step 339/41783: mean batch inertia: 68228.61386179752, ewa inertia: 68998.54988962256\n",
      "Minibatch step 340/41783: mean batch inertia: 67848.27465452418, ewa inertia: 68987.53803324689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 341/41783: mean batch inertia: 67883.80667479531, ewa inertia: 68976.97175239016\n",
      "Minibatch step 342/41783: mean batch inertia: 67753.74269797583, ewa inertia: 68965.26149187968\n",
      "Minibatch step 343/41783: mean batch inertia: 67554.35463252936, ewa inertia: 68951.7545474367\n",
      "Minibatch step 344/41783: mean batch inertia: 68310.12917747496, ewa inertia: 68945.61211629899\n",
      "Minibatch step 345/41783: mean batch inertia: 68461.59278338964, ewa inertia: 68940.97848496067\n",
      "Minibatch step 346/41783: mean batch inertia: 68090.22964101467, ewa inertia: 68932.83406543788\n",
      "Minibatch step 347/41783: mean batch inertia: 68414.79739132749, ewa inertia: 68927.87477806164\n",
      "Minibatch step 348/41783: mean batch inertia: 67649.12872520238, ewa inertia: 68915.63304021893\n",
      "Minibatch step 349/41783: mean batch inertia: 68053.03745284732, ewa inertia: 68907.37520902464\n",
      "Minibatch step 350/41783: mean batch inertia: 67853.4154585575, ewa inertia: 68897.28540352093\n",
      "Minibatch step 351/41783: mean batch inertia: 67685.60842048738, ewa inertia: 68885.68573371692\n",
      "Minibatch step 352/41783: mean batch inertia: 68347.87056002206, ewa inertia: 68880.53710209497\n",
      "Minibatch step 353/41783: mean batch inertia: 67936.65378429073, ewa inertia: 68871.50108426732\n",
      "Minibatch step 354/41783: mean batch inertia: 68416.83024441286, ewa inertia: 68867.14841298142\n",
      "Minibatch step 355/41783: mean batch inertia: 67567.67996865377, ewa inertia: 68854.70829479316\n",
      "Minibatch step 356/41783: mean batch inertia: 68095.21726887026, ewa inertia: 68847.43750790591\n",
      "Minibatch step 357/41783: mean batch inertia: 67651.6072455572, ewa inertia: 68835.98954250001\n",
      "Minibatch step 358/41783: mean batch inertia: 67652.19890718229, ewa inertia: 68824.65683528595\n",
      "Minibatch step 359/41783: mean batch inertia: 68230.86661262465, ewa inertia: 68818.97234130597\n",
      "Minibatch step 360/41783: mean batch inertia: 68323.70643753152, ewa inertia: 68814.23104388973\n",
      "Minibatch step 361/41783: mean batch inertia: 67842.52295338991, ewa inertia: 68804.92865294436\n",
      "Minibatch step 362/41783: mean batch inertia: 67926.98523614231, ewa inertia: 68796.52389335571\n",
      "Minibatch step 363/41783: mean batch inertia: 67769.72781636761, ewa inertia: 68786.69413211086\n",
      "Minibatch step 364/41783: mean batch inertia: 67755.5389616713, ewa inertia: 68776.82264023533\n",
      "Minibatch step 365/41783: mean batch inertia: 67765.24133800896, ewa inertia: 68767.13853362108\n",
      "Minibatch step 366/41783: mean batch inertia: 67652.02830944578, ewa inertia: 68756.46332019652\n",
      "Minibatch step 367/41783: mean batch inertia: 68270.08047692334, ewa inertia: 68751.80706241552\n",
      "Minibatch step 368/41783: mean batch inertia: 67880.15979913107, ewa inertia: 68743.46257739085\n",
      "Minibatch step 369/41783: mean batch inertia: 68207.8015268979, ewa inertia: 68738.33456769884\n",
      "Minibatch step 370/41783: mean batch inertia: 68535.85518315062, ewa inertia: 68736.39618475044\n",
      "Minibatch step 371/41783: mean batch inertia: 67852.87252137253, ewa inertia: 68727.93800414396\n",
      "Minibatch step 372/41783: mean batch inertia: 67969.63012377755, ewa inertia: 68720.67854378844\n",
      "Minibatch step 373/41783: mean batch inertia: 68378.94433857025, ewa inertia: 68717.40704156732\n",
      "Minibatch step 374/41783: mean batch inertia: 67944.42374607563, ewa inertia: 68710.00708999833\n",
      "Minibatch step 375/41783: mean batch inertia: 67733.92847265268, ewa inertia: 68700.6628589677\n",
      "Minibatch step 376/41783: mean batch inertia: 67632.89549000791, ewa inertia: 68690.44086987255\n",
      "Minibatch step 377/41783: mean batch inertia: 68136.31708266791, ewa inertia: 68685.13611204342\n",
      "Minibatch step 378/41783: mean batch inertia: 67746.96971233879, ewa inertia: 68676.15482362235\n",
      "Minibatch step 379/41783: mean batch inertia: 68285.16935533399, ewa inertia: 68672.41182743292\n",
      "Minibatch step 380/41783: mean batch inertia: 68124.83750207942, ewa inertia: 68667.16976914825\n",
      "[MiniBatchKMeans] Reassigning 1 cluster centers.\n",
      "Minibatch step 381/41783: mean batch inertia: 68295.80040582662, ewa inertia: 68663.61456256254\n",
      "Minibatch step 382/41783: mean batch inertia: 67850.00704440879, ewa inertia: 68655.82570571968\n",
      "Minibatch step 383/41783: mean batch inertia: 67755.78253592523, ewa inertia: 68647.20937997845\n",
      "Minibatch step 384/41783: mean batch inertia: 68071.6797651053, ewa inertia: 68641.6996991077\n",
      "Minibatch step 385/41783: mean batch inertia: 68268.40979822083, ewa inertia: 68638.12610676247\n",
      "Minibatch step 386/41783: mean batch inertia: 68466.79322177845, ewa inertia: 68636.48589660728\n",
      "Minibatch step 387/41783: mean batch inertia: 67983.94304182834, ewa inertia: 68630.23894981008\n",
      "Minibatch step 388/41783: mean batch inertia: 68019.39245937421, ewa inertia: 68624.39117215217\n",
      "Minibatch step 389/41783: mean batch inertia: 68431.42165406133, ewa inertia: 68622.54382940041\n",
      "Minibatch step 390/41783: mean batch inertia: 67854.58035268621, ewa inertia: 68615.19193374162\n",
      "Minibatch step 391/41783: mean batch inertia: 68237.76401999082, ewa inertia: 68611.57872722237\n",
      "Minibatch step 392/41783: mean batch inertia: 67957.44286652221, ewa inertia: 68605.31653020342\n",
      "Minibatch step 393/41783: mean batch inertia: 68014.30098416153, ewa inertia: 68599.65859885776\n",
      "Minibatch step 394/41783: mean batch inertia: 67813.25621786242, ewa inertia: 68592.13018321534\n",
      "Minibatch step 395/41783: mean batch inertia: 67726.02653321419, ewa inertia: 68583.83876850933\n",
      "Minibatch step 396/41783: mean batch inertia: 68162.17355449713, ewa inertia: 68579.8020678714\n",
      "Minibatch step 397/41783: mean batch inertia: 67663.8250559876, ewa inertia: 68571.03320369829\n",
      "Minibatch step 398/41783: mean batch inertia: 68527.78721543848, ewa inertia: 68570.61919964342\n",
      "Minibatch step 399/41783: mean batch inertia: 68012.46723655693, ewa inertia: 68565.27587913563\n",
      "Minibatch step 400/41783: mean batch inertia: 67837.26936151132, ewa inertia: 68558.306500879\n",
      "Minibatch step 401/41783: mean batch inertia: 68311.31847282441, ewa inertia: 68555.94202617898\n",
      "Minibatch step 402/41783: mean batch inertia: 67888.95504790089, ewa inertia: 68549.55680237764\n",
      "Minibatch step 403/41783: mean batch inertia: 68085.8041214011, ewa inertia: 68545.11718848269\n",
      "Minibatch step 404/41783: mean batch inertia: 67895.69477091629, ewa inertia: 68538.90011436759\n",
      "Minibatch step 405/41783: mean batch inertia: 67857.44057434227, ewa inertia: 68532.37634131603\n",
      "Minibatch step 406/41783: mean batch inertia: 68072.82391178203, ewa inertia: 68527.97693741973\n",
      "Minibatch step 407/41783: mean batch inertia: 68324.10619343598, ewa inertia: 68526.025234659\n",
      "Minibatch step 408/41783: mean batch inertia: 68009.55939102336, ewa inertia: 68521.08098521372\n",
      "Minibatch step 409/41783: mean batch inertia: 67621.87725916655, ewa inertia: 68512.47269566565\n",
      "Minibatch step 410/41783: mean batch inertia: 67542.54857705023, ewa inertia: 68503.18738310429\n",
      "Minibatch step 411/41783: mean batch inertia: 68144.85657964542, ewa inertia: 68499.75699773019\n",
      "Minibatch step 412/41783: mean batch inertia: 67871.0467537744, ewa inertia: 68493.73820614266\n",
      "Minibatch step 413/41783: mean batch inertia: 68040.15736296083, ewa inertia: 68489.39596965224\n",
      "Minibatch step 414/41783: mean batch inertia: 67752.61916487209, ewa inertia: 68482.34263136623\n",
      "Minibatch step 415/41783: mean batch inertia: 67868.07615921085, ewa inertia: 68476.46211341595\n",
      "Minibatch step 416/41783: mean batch inertia: 68092.62091737716, ewa inertia: 68472.78751103135\n",
      "Minibatch step 417/41783: mean batch inertia: 68217.46944831636, ewa inertia: 68470.34329094301\n",
      "Minibatch step 418/41783: mean batch inertia: 68149.3596518029, ewa inertia: 68467.27043878964\n",
      "Minibatch step 419/41783: mean batch inertia: 68074.97190635167, ewa inertia: 68463.51487232713\n",
      "Minibatch step 420/41783: mean batch inertia: 68099.97399823637, ewa inertia: 68460.0346097169\n",
      "Minibatch step 421/41783: mean batch inertia: 67584.61777224067, ewa inertia: 68451.65403766859\n",
      "Minibatch step 422/41783: mean batch inertia: 67739.30837086521, ewa inertia: 68444.834584434\n",
      "Minibatch step 423/41783: mean batch inertia: 68218.24031505537, ewa inertia: 68442.66534400021\n",
      "Minibatch step 424/41783: mean batch inertia: 67840.42567289813, ewa inertia: 68436.89996145562\n",
      "Minibatch step 425/41783: mean batch inertia: 68340.48263961327, ewa inertia: 68435.976935672\n",
      "Minibatch step 426/41783: mean batch inertia: 67810.9657769963, ewa inertia: 68429.99355630111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 427/41783: mean batch inertia: 68214.29211239459, ewa inertia: 68427.92859545708\n",
      "Minibatch step 428/41783: mean batch inertia: 67583.65540880793, ewa inertia: 68419.84616893034\n",
      "Minibatch step 429/41783: mean batch inertia: 68187.43569115068, ewa inertia: 68417.62124856048\n",
      "Minibatch step 430/41783: mean batch inertia: 67745.7298930876, ewa inertia: 68411.18907399847\n",
      "Minibatch step 431/41783: mean batch inertia: 67463.73386166836, ewa inertia: 68402.1188615813\n",
      "Minibatch step 432/41783: mean batch inertia: 67714.53005164002, ewa inertia: 68395.5364115819\n",
      "Minibatch step 433/41783: mean batch inertia: 67878.36144643482, ewa inertia: 68390.58537354898\n",
      "Minibatch step 434/41783: mean batch inertia: 67890.46919659743, ewa inertia: 68385.7976433226\n",
      "Minibatch step 435/41783: mean batch inertia: 67724.41312872636, ewa inertia: 68379.46605322868\n",
      "Minibatch step 436/41783: mean batch inertia: 68445.9611490904, ewa inertia: 68380.10262647913\n",
      "Minibatch step 437/41783: mean batch inertia: 68142.70863240157, ewa inertia: 68377.82999773127\n",
      "Minibatch step 438/41783: mean batch inertia: 67606.8369524386, ewa inertia: 68370.44909929727\n",
      "Minibatch step 439/41783: mean batch inertia: 68642.50261106848, ewa inertia: 68373.05353179021\n",
      "Minibatch step 440/41783: mean batch inertia: 67792.95200677008, ewa inertia: 68367.5000829443\n",
      "Minibatch step 441/41783: mean batch inertia: 67857.53101116244, ewa inertia: 68362.6180286297\n",
      "Minibatch step 442/41783: mean batch inertia: 68329.04930548804, ewa inertia: 68362.29666731837\n",
      "Minibatch step 443/41783: mean batch inertia: 67851.34198767699, ewa inertia: 68357.40517754704\n",
      "Minibatch step 444/41783: mean batch inertia: 68165.24446933891, ewa inertia: 68355.56557772322\n",
      "Minibatch step 445/41783: mean batch inertia: 67797.6123318297, ewa inertia: 68350.22415958202\n",
      "Minibatch step 446/41783: mean batch inertia: 67870.71953862875, ewa inertia: 68345.63374864706\n",
      "Minibatch step 447/41783: mean batch inertia: 68245.06261377924, ewa inertia: 68344.67095743073\n",
      "Minibatch step 448/41783: mean batch inertia: 67698.7552214148, ewa inertia: 68338.48745360594\n",
      "Minibatch step 449/41783: mean batch inertia: 68117.0589269054, ewa inertia: 68336.36766604631\n",
      "Minibatch step 450/41783: mean batch inertia: 68038.57219763822, ewa inertia: 68333.51679972546\n",
      "Minibatch step 451/41783: mean batch inertia: 67915.71306068101, ewa inertia: 68329.517065899\n",
      "Minibatch step 452/41783: mean batch inertia: 68088.63112135466, ewa inertia: 68327.21100788497\n",
      "Minibatch step 453/41783: mean batch inertia: 67743.50379163118, ewa inertia: 68321.62304090567\n",
      "Minibatch step 454/41783: mean batch inertia: 67635.29466100289, ewa inertia: 68315.05265730058\n",
      "Minibatch step 455/41783: mean batch inertia: 67646.33018672575, ewa inertia: 68308.65081922177\n",
      "Minibatch step 456/41783: mean batch inertia: 67879.0057097038, ewa inertia: 68304.53772516032\n",
      "Minibatch step 457/41783: mean batch inertia: 68187.46141658943, ewa inertia: 68303.4169260197\n",
      "Minibatch step 458/41783: mean batch inertia: 68050.5896646394, ewa inertia: 68300.99655096055\n",
      "Minibatch step 459/41783: mean batch inertia: 68256.86005107388, ewa inertia: 68300.57402182765\n",
      "Minibatch step 460/41783: mean batch inertia: 67790.78255163172, ewa inertia: 68295.69366773494\n",
      "Minibatch step 461/41783: mean batch inertia: 67846.59681974111, ewa inertia: 68291.39435758902\n",
      "Minibatch step 462/41783: mean batch inertia: 68125.02612028178, ewa inertia: 68289.80167517805\n",
      "Minibatch step 463/41783: mean batch inertia: 68210.57846176958, ewa inertia: 68289.04325265354\n",
      "Minibatch step 464/41783: mean batch inertia: 67968.5477964787, ewa inertia: 68285.97507399094\n",
      "Minibatch step 465/41783: mean batch inertia: 68333.82812563867, ewa inertia: 68286.43318255122\n",
      "Minibatch step 466/41783: mean batch inertia: 68486.8837517743, ewa inertia: 68288.35214317155\n",
      "Minibatch step 467/41783: mean batch inertia: 67738.32620259421, ewa inertia: 68283.08661499558\n",
      "Minibatch step 468/41783: mean batch inertia: 67744.21712791274, ewa inertia: 68277.92789018266\n",
      "Minibatch step 469/41783: mean batch inertia: 68132.34888211313, ewa inertia: 68276.53422797099\n",
      "Minibatch step 470/41783: mean batch inertia: 67548.96381859809, ewa inertia: 68269.56902468161\n",
      "Minibatch step 471/41783: mean batch inertia: 67403.7436606041, ewa inertia: 68261.28027407244\n",
      "Minibatch step 472/41783: mean batch inertia: 68297.47369210505, ewa inertia: 68261.62676220758\n",
      "Minibatch step 473/41783: mean batch inertia: 68567.48196529987, ewa inertia: 68264.55478627124\n",
      "Minibatch step 474/41783: mean batch inertia: 67711.22860284001, ewa inertia: 68259.25766409133\n",
      "Minibatch step 475/41783: mean batch inertia: 67655.38761948839, ewa inertia: 68253.47667359633\n",
      "Minibatch step 476/41783: mean batch inertia: 67809.00201533857, ewa inertia: 68249.22161276393\n",
      "Minibatch step 477/41783: mean batch inertia: 67839.38965854628, ewa inertia: 68245.29819471553\n",
      "Minibatch step 478/41783: mean batch inertia: 67947.17534639676, ewa inertia: 68242.44419430953\n",
      "Minibatch step 479/41783: mean batch inertia: 68659.41369944617, ewa inertia: 68246.43594181786\n",
      "Minibatch step 480/41783: mean batch inertia: 68128.00450915337, ewa inertia: 68245.30216975437\n",
      "Minibatch step 481/41783: mean batch inertia: 67992.8597347236, ewa inertia: 68242.88547872873\n",
      "Minibatch step 482/41783: mean batch inertia: 67403.01221299179, ewa inertia: 68234.84517368359\n",
      "Minibatch step 483/41783: mean batch inertia: 67671.36706306825, ewa inertia: 68229.45086470814\n",
      "Minibatch step 484/41783: mean batch inertia: 67676.08188426313, ewa inertia: 68224.1533328223\n",
      "Minibatch step 485/41783: mean batch inertia: 68023.07391145521, ewa inertia: 68222.22835205193\n",
      "Minibatch step 486/41783: mean batch inertia: 67600.13973953026, ewa inertia: 68216.27295090532\n",
      "Minibatch step 487/41783: mean batch inertia: 68189.184079941, ewa inertia: 68216.0136227486\n",
      "Minibatch step 488/41783: mean batch inertia: 67798.29679256666, ewa inertia: 68212.01472092117\n",
      "Minibatch step 489/41783: mean batch inertia: 68029.66560305003, ewa inertia: 68210.2690497679\n",
      "Minibatch step 490/41783: mean batch inertia: 68015.30361759926, ewa inertia: 68208.40259965947\n",
      "Minibatch step 491/41783: mean batch inertia: 68226.61786472442, ewa inertia: 68208.57697869207\n",
      "Minibatch step 492/41783: mean batch inertia: 68297.33241425933, ewa inertia: 68209.4266554296\n",
      "Minibatch step 493/41783: mean batch inertia: 68009.25891213308, ewa inertia: 68207.51040236863\n",
      "Minibatch step 494/41783: mean batch inertia: 67262.96039303314, ewa inertia: 68198.46800214557\n",
      "Minibatch step 495/41783: mean batch inertia: 68409.07429955844, ewa inertia: 68200.48418594936\n",
      "Minibatch step 496/41783: mean batch inertia: 67434.96182809722, ewa inertia: 68193.15565969772\n",
      "Minibatch step 497/41783: mean batch inertia: 68074.86334127774, ewa inertia: 68192.02321940773\n",
      "Minibatch step 498/41783: mean batch inertia: 68054.7940887846, ewa inertia: 68190.70949254403\n",
      "Minibatch step 499/41783: mean batch inertia: 68208.57215661567, ewa inertia: 68190.8804960441\n",
      "Minibatch step 500/41783: mean batch inertia: 67909.22319956622, ewa inertia: 68188.18412425296\n",
      "Minibatch step 501/41783: mean batch inertia: 68001.13800233924, ewa inertia: 68186.39348757116\n",
      "Minibatch step 502/41783: mean batch inertia: 68382.48906000021, ewa inertia: 68188.2707567791\n",
      "Minibatch step 503/41783: mean batch inertia: 68547.99190386334, ewa inertia: 68191.71445224094\n",
      "Minibatch step 504/41783: mean batch inertia: 68108.79890169171, ewa inertia: 68190.9206821013\n",
      "Minibatch step 505/41783: mean batch inertia: 68032.81677194136, ewa inertia: 68189.40711604511\n",
      "Minibatch step 506/41783: mean batch inertia: 68014.60096382478, ewa inertia: 68187.73365548272\n",
      "Minibatch step 507/41783: mean batch inertia: 68277.96887103855, ewa inertia: 68188.59749850341\n",
      "Minibatch step 508/41783: mean batch inertia: 67804.15969925775, ewa inertia: 68184.91718469546\n",
      "Minibatch step 509/41783: mean batch inertia: 68461.73209349737, ewa inertia: 68187.56719916622\n",
      "Minibatch step 510/41783: mean batch inertia: 68250.72139965722, ewa inertia: 68188.17178923659\n",
      "Minibatch step 511/41783: mean batch inertia: 67546.20439577612, ewa inertia: 68182.02608382718\n",
      "Minibatch step 512/41783: mean batch inertia: 67313.75619543201, ewa inertia: 68173.71393120963\n",
      "Minibatch step 513/41783: mean batch inertia: 67923.12960916013, ewa inertia: 68171.3150283382\n",
      "Minibatch step 514/41783: mean batch inertia: 67736.56277986175, ewa inertia: 68167.15304242984\n",
      "Minibatch step 515/41783: mean batch inertia: 67794.00337074595, ewa inertia: 68163.58079253187\n",
      "Minibatch step 516/41783: mean batch inertia: 67721.21540444315, ewa inertia: 68159.34592424073\n",
      "Minibatch step 517/41783: mean batch inertia: 68710.10280714993, ewa inertia: 68164.61844990018\n",
      "Minibatch step 518/41783: mean batch inertia: 67816.36766827232, ewa inertia: 68161.28456295471\n",
      "Minibatch step 519/41783: mean batch inertia: 68345.8422253103, ewa inertia: 68163.0513770257\n",
      "Minibatch step 520/41783: mean batch inertia: 68591.76857800575, ewa inertia: 68167.15558799966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 521/41783: mean batch inertia: 68146.26467918731, ewa inertia: 68166.95559439779\n",
      "Minibatch step 522/41783: mean batch inertia: 67890.87302835585, ewa inertia: 68164.31259081715\n",
      "Minibatch step 523/41783: mean batch inertia: 67985.64222557958, ewa inertia: 68162.6021372313\n",
      "Minibatch step 524/41783: mean batch inertia: 67850.28187571485, ewa inertia: 68159.6122216371\n",
      "Minibatch step 525/41783: mean batch inertia: 68114.35930655731, ewa inertia: 68159.179004798\n",
      "Minibatch step 526/41783: mean batch inertia: 67538.59090682853, ewa inertia: 68153.23796843142\n",
      "Minibatch step 527/41783: mean batch inertia: 67812.55211092705, ewa inertia: 68149.97650229046\n",
      "Minibatch step 528/41783: mean batch inertia: 67981.11402710309, ewa inertia: 68148.35994195145\n",
      "Minibatch step 529/41783: mean batch inertia: 67625.4774620513, ewa inertia: 68143.3542645324\n",
      "Minibatch step 530/41783: mean batch inertia: 68161.92116680791, ewa inertia: 68143.53200987105\n",
      "Minibatch step 531/41783: mean batch inertia: 67477.70913327496, ewa inertia: 68137.15793028993\n",
      "Minibatch step 532/41783: mean batch inertia: 67431.0068638945, ewa inertia: 68130.39777942743\n",
      "Minibatch step 533/41783: mean batch inertia: 67852.74844313302, ewa inertia: 68127.73977678528\n",
      "Minibatch step 534/41783: mean batch inertia: 68280.00510549179, ewa inertia: 68129.19744872287\n",
      "Minibatch step 535/41783: mean batch inertia: 68597.70744500413, ewa inertia: 68133.6826055203\n",
      "Minibatch step 536/41783: mean batch inertia: 67739.68813192747, ewa inertia: 68129.91080341274\n",
      "Minibatch step 537/41783: mean batch inertia: 67712.04926904883, ewa inertia: 68125.91051629803\n",
      "Minibatch step 538/41783: mean batch inertia: 67682.69364292617, ewa inertia: 68121.66749653725\n",
      "Minibatch step 539/41783: mean batch inertia: 67960.9891692999, ewa inertia: 68120.12928497863\n",
      "Minibatch step 540/41783: mean batch inertia: 68087.16770700704, ewa inertia: 68119.81373601132\n",
      "Minibatch step 541/41783: mean batch inertia: 67617.61881934087, ewa inertia: 68115.00610551887\n",
      "Minibatch step 542/41783: mean batch inertia: 68282.47547005031, ewa inertia: 68116.60932928069\n",
      "Minibatch step 543/41783: mean batch inertia: 68409.2322278075, ewa inertia: 68119.41067736894\n",
      "Minibatch step 544/41783: mean batch inertia: 68218.79304723738, ewa inertia: 68120.36208825729\n",
      "Minibatch step 545/41783: mean batch inertia: 67340.31262923432, ewa inertia: 68112.89449063667\n",
      "Minibatch step 546/41783: mean batch inertia: 68045.36797100349, ewa inertia: 68112.24804332295\n",
      "Minibatch step 547/41783: mean batch inertia: 68435.68974476289, ewa inertia: 68115.344427087\n",
      "Minibatch step 548/41783: mean batch inertia: 67590.90317817163, ewa inertia: 68110.32382720419\n",
      "Minibatch step 549/41783: mean batch inertia: 68267.71466186007, ewa inertia: 68111.83056682022\n",
      "Minibatch step 550/41783: mean batch inertia: 68115.87400331094, ewa inertia: 68111.8692755923\n",
      "Minibatch step 551/41783: mean batch inertia: 67718.42913083566, ewa inertia: 68108.10278020555\n",
      "Minibatch step 552/41783: mean batch inertia: 67955.83587277729, ewa inertia: 68106.64509315448\n",
      "Minibatch step 553/41783: mean batch inertia: 68079.14909803736, ewa inertia: 68106.38186750213\n",
      "Minibatch step 554/41783: mean batch inertia: 67823.3740203108, ewa inertia: 68103.67256657018\n",
      "Minibatch step 555/41783: mean batch inertia: 67925.82664034856, ewa inertia: 68101.97000553366\n",
      "Minibatch step 556/41783: mean batch inertia: 67883.47764368654, ewa inertia: 68099.87832657332\n",
      "Minibatch step 557/41783: mean batch inertia: 68031.24536313328, ewa inertia: 68099.22128701184\n",
      "Minibatch step 558/41783: mean batch inertia: 68220.88841180003, ewa inertia: 68100.38603511988\n",
      "Minibatch step 559/41783: mean batch inertia: 68436.14555775694, ewa inertia: 68103.60034029417\n",
      "Minibatch step 560/41783: mean batch inertia: 67468.78617521527, ewa inertia: 68097.52311442875\n",
      "Minibatch step 561/41783: mean batch inertia: 67453.3950926226, ewa inertia: 68091.35672481412\n",
      "Minibatch step 562/41783: mean batch inertia: 67959.43390114783, ewa inertia: 68090.09379647962\n",
      "Minibatch step 563/41783: mean batch inertia: 67566.6397067807, ewa inertia: 68085.08264690504\n",
      "Minibatch step 564/41783: mean batch inertia: 67876.73009796099, ewa inertia: 68083.08803876738\n",
      "Minibatch step 565/41783: mean batch inertia: 67885.35932021365, ewa inertia: 68081.19513506607\n",
      "Minibatch step 566/41783: mean batch inertia: 67949.33578554228, ewa inertia: 68079.9328143845\n",
      "Minibatch step 567/41783: mean batch inertia: 67771.19380969326, ewa inertia: 68076.97718300734\n",
      "Minibatch step 568/41783: mean batch inertia: 68278.71149258282, ewa inertia: 68078.90843317713\n",
      "Minibatch step 569/41783: mean batch inertia: 68036.9988130136, ewa inertia: 68078.50722248953\n",
      "Minibatch step 570/41783: mean batch inertia: 67910.68008331706, ewa inertia: 68076.9005736666\n",
      "Minibatch step 571/41783: mean batch inertia: 67169.701874493, ewa inertia: 68068.21574635338\n",
      "Minibatch step 572/41783: mean batch inertia: 67637.2222161554, ewa inertia: 68064.08974354243\n",
      "Minibatch step 573/41783: mean batch inertia: 67966.39465457023, ewa inertia: 68063.15448539243\n",
      "Minibatch step 574/41783: mean batch inertia: 67378.45829491662, ewa inertia: 68056.59972712204\n",
      "Minibatch step 575/41783: mean batch inertia: 67489.80555301685, ewa inertia: 68051.17367268796\n",
      "Minibatch step 576/41783: mean batch inertia: 67958.77774883836, ewa inertia: 68050.28914469668\n",
      "Minibatch step 577/41783: mean batch inertia: 67964.34851990432, ewa inertia: 68049.46641480719\n",
      "Minibatch step 578/41783: mean batch inertia: 68000.21965734992, ewa inertia: 68048.99496397217\n",
      "Minibatch step 579/41783: mean batch inertia: 67892.204319447, ewa inertia: 68047.49397011794\n",
      "Minibatch step 580/41783: mean batch inertia: 67940.33579461777, ewa inertia: 68046.46811960661\n",
      "Minibatch step 581/41783: mean batch inertia: 67945.84536506432, ewa inertia: 68045.50483422296\n",
      "Minibatch step 582/41783: mean batch inertia: 67640.77449744535, ewa inertia: 68041.63025516267\n",
      "Minibatch step 583/41783: mean batch inertia: 67925.69383740818, ewa inertia: 68040.52036846594\n",
      "Minibatch step 584/41783: mean batch inertia: 67823.64265999505, ewa inertia: 68038.44414696355\n",
      "Minibatch step 585/41783: mean batch inertia: 67768.29696474926, ewa inertia: 68035.85796421349\n",
      "Minibatch step 586/41783: mean batch inertia: 67658.41429576358, ewa inertia: 68032.24460687078\n",
      "Minibatch step 587/41783: mean batch inertia: 67989.92280833975, ewa inertia: 68031.83945030236\n",
      "Minibatch step 588/41783: mean batch inertia: 68094.9084085291, ewa inertia: 68032.4432243284\n",
      "Minibatch step 589/41783: mean batch inertia: 68416.83922179855, ewa inertia: 68036.12313795807\n",
      "Minibatch step 590/41783: mean batch inertia: 67054.26126976906, ewa inertia: 68026.72354250184\n",
      "Minibatch step 591/41783: mean batch inertia: 67427.29697840726, ewa inertia: 68020.98509049474\n",
      "Minibatch step 592/41783: mean batch inertia: 67692.7533499763, ewa inertia: 68017.84285055577\n",
      "Minibatch step 593/41783: mean batch inertia: 67823.13225474092, ewa inertia: 68015.97884005592\n",
      "Minibatch step 594/41783: mean batch inertia: 68326.0267125351, ewa inertia: 68018.9470015334\n",
      "Minibatch step 595/41783: mean batch inertia: 68089.58195207537, ewa inertia: 68019.62320659001\n",
      "Minibatch step 596/41783: mean batch inertia: 68125.56359701211, ewa inertia: 68020.63739895729\n",
      "Minibatch step 597/41783: mean batch inertia: 68151.45453704541, ewa inertia: 68021.88974230272\n",
      "Minibatch step 598/41783: mean batch inertia: 67738.34294456824, ewa inertia: 68019.17528186999\n",
      "Minibatch step 599/41783: mean batch inertia: 68193.85744123429, ewa inertia: 68020.84755541949\n",
      "Minibatch step 600/41783: mean batch inertia: 67256.8937596618, ewa inertia: 68013.53404538307\n",
      "Minibatch step 601/41783: mean batch inertia: 68097.47002126477, ewa inertia: 68014.3375842953\n",
      "Minibatch step 602/41783: mean batch inertia: 67221.39503095661, ewa inertia: 68006.74655803906\n",
      "Minibatch step 603/41783: mean batch inertia: 67887.54031208075, ewa inertia: 68005.605368505\n",
      "Minibatch step 604/41783: mean batch inertia: 67689.44539193161, ewa inertia: 68002.57869441212\n",
      "Minibatch step 605/41783: mean batch inertia: 68064.30797998955, ewa inertia: 68003.16964343563\n",
      "Minibatch step 606/41783: mean batch inertia: 68371.75123418464, ewa inertia: 68006.69816201639\n",
      "Minibatch step 607/41783: mean batch inertia: 67698.16932894649, ewa inertia: 68003.74454266176\n",
      "Minibatch step 608/41783: mean batch inertia: 67800.47723448681, ewa inertia: 68001.79861673448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 609/41783: mean batch inertia: 67703.19403301936, ewa inertia: 67998.9400045618\n",
      "Minibatch step 610/41783: mean batch inertia: 67555.01789006639, ewa inertia: 67994.69023336127\n",
      "Minibatch step 611/41783: mean batch inertia: 68285.00743384697, ewa inertia: 67997.46950845786\n",
      "Minibatch step 612/41783: mean batch inertia: 67829.41730929194, ewa inertia: 67995.8607050825\n",
      "Minibatch step 613/41783: mean batch inertia: 67665.28101542265, ewa inertia: 67992.69598767212\n",
      "Minibatch step 614/41783: mean batch inertia: 68110.53700152569, ewa inertia: 67993.82410751695\n",
      "Minibatch step 615/41783: mean batch inertia: 67793.49065817484, ewa inertia: 67991.90626811289\n",
      "Minibatch step 616/41783: mean batch inertia: 67713.06397480227, ewa inertia: 67989.2368450116\n",
      "Minibatch step 617/41783: mean batch inertia: 67920.61255793738, ewa inertia: 67988.57988851103\n",
      "Minibatch step 618/41783: mean batch inertia: 68104.74827090497, ewa inertia: 67989.69199586002\n",
      "Minibatch step 619/41783: mean batch inertia: 67860.68651465332, ewa inertia: 67988.45699593358\n",
      "Minibatch step 620/41783: mean batch inertia: 67394.1678977594, ewa inertia: 67982.76772610054\n",
      "Minibatch step 621/41783: mean batch inertia: 67681.81984095891, ewa inertia: 67979.8866809501\n",
      "Minibatch step 622/41783: mean batch inertia: 68194.36905643616, ewa inertia: 67981.93997136434\n",
      "Minibatch step 623/41783: mean batch inertia: 67504.93280303523, ewa inertia: 67977.37346913294\n",
      "Minibatch step 624/41783: mean batch inertia: 67805.5698159278, ewa inertia: 67975.72875220243\n",
      "Minibatch step 625/41783: mean batch inertia: 67951.50442773163, ewa inertia: 67975.49684702553\n",
      "Minibatch step 626/41783: mean batch inertia: 67139.37354350806, ewa inertia: 67967.49244125398\n",
      "Minibatch step 627/41783: mean batch inertia: 68213.81229178616, ewa inertia: 67969.85051933283\n",
      "Minibatch step 628/41783: mean batch inertia: 67804.51359048324, ewa inertia: 67968.26770988121\n",
      "Minibatch step 629/41783: mean batch inertia: 67462.94133314567, ewa inertia: 67963.43010118231\n",
      "Minibatch step 630/41783: mean batch inertia: 67450.14674922943, ewa inertia: 67958.51631848121\n",
      "Minibatch step 631/41783: mean batch inertia: 67969.2144666316, ewa inertia: 67958.61873437901\n",
      "Minibatch step 632/41783: mean batch inertia: 67741.71917200406, ewa inertia: 67956.54230366404\n",
      "Minibatch step 633/41783: mean batch inertia: 67852.86544679129, ewa inertia: 67955.54978063778\n",
      "Minibatch step 634/41783: mean batch inertia: 67983.35188603467, ewa inertia: 67955.8159367561\n",
      "Minibatch step 635/41783: mean batch inertia: 67656.7895692438, ewa inertia: 67952.95328674756\n",
      "Minibatch step 636/41783: mean batch inertia: 67684.27457842733, ewa inertia: 67950.38116204477\n",
      "Minibatch step 637/41783: mean batch inertia: 67676.3078094962, ewa inertia: 67947.75739313923\n",
      "Minibatch step 638/41783: mean batch inertia: 67935.01728970812, ewa inertia: 67947.63542912147\n",
      "Minibatch step 639/41783: mean batch inertia: 68265.05422067514, ewa inertia: 67950.67415414733\n",
      "Minibatch step 640/41783: mean batch inertia: 67620.84320350442, ewa inertia: 67947.51660459231\n",
      "Minibatch step 641/41783: mean batch inertia: 68324.9795279628, ewa inertia: 67951.1301462669\n",
      "Minibatch step 642/41783: mean batch inertia: 68023.28843924342, ewa inertia: 67951.82093464027\n",
      "Minibatch step 643/41783: mean batch inertia: 68054.05371446843, ewa inertia: 67952.79963317605\n",
      "Minibatch step 644/41783: mean batch inertia: 67578.98454041436, ewa inertia: 67949.22101304503\n",
      "Minibatch step 645/41783: mean batch inertia: 68079.85286313902, ewa inertia: 67950.47158258475\n",
      "Minibatch step 646/41783: mean batch inertia: 67832.29328724311, ewa inertia: 67949.34023386463\n",
      "Minibatch step 647/41783: mean batch inertia: 67902.91244517251, ewa inertia: 67948.89576968308\n",
      "Minibatch step 648/41783: mean batch inertia: 68218.97965336758, ewa inertia: 67951.48134646137\n",
      "Minibatch step 649/41783: mean batch inertia: 67866.04003485777, ewa inertia: 67950.66339661492\n",
      "Minibatch step 650/41783: mean batch inertia: 67619.22426561672, ewa inertia: 67947.49045156971\n",
      "Minibatch step 651/41783: mean batch inertia: 67578.27368915018, ewa inertia: 67943.95585234059\n",
      "Minibatch step 652/41783: mean batch inertia: 68074.73644307429, ewa inertia: 67945.20784580958\n",
      "Minibatch step 653/41783: mean batch inertia: 68182.87404263126, ewa inertia: 67947.48308041856\n",
      "Minibatch step 654/41783: mean batch inertia: 67677.72473874087, ewa inertia: 67944.90062013075\n",
      "Minibatch step 655/41783: mean batch inertia: 67861.50920540794, ewa inertia: 67944.10229443105\n",
      "Minibatch step 656/41783: mean batch inertia: 67862.60139276598, ewa inertia: 67943.32206705917\n",
      "Minibatch step 657/41783: mean batch inertia: 67683.75514173569, ewa inertia: 67940.83717160605\n",
      "Minibatch step 658/41783: mean batch inertia: 67907.75205952952, ewa inertia: 67940.5204400176\n",
      "Minibatch step 659/41783: mean batch inertia: 67513.2053005323, ewa inertia: 67936.42965130931\n",
      "Minibatch step 660/41783: mean batch inertia: 67929.24846684447, ewa inertia: 67936.36090413513\n",
      "Minibatch step 661/41783: mean batch inertia: 67933.74626658687, ewa inertia: 67936.33587359263\n",
      "Minibatch step 662/41783: mean batch inertia: 67775.9055826278, ewa inertia: 67934.8000365438\n",
      "Minibatch step 663/41783: mean batch inertia: 67941.09532586944, ewa inertia: 67934.86030283467\n",
      "Minibatch step 664/41783: mean batch inertia: 67831.8905461506, ewa inertia: 67933.87454904546\n",
      "Minibatch step 665/41783: mean batch inertia: 67274.75446846573, ewa inertia: 67927.56463691295\n",
      "Minibatch step 666/41783: mean batch inertia: 67744.9959159557, ewa inertia: 67925.8168634475\n",
      "Minibatch step 667/41783: mean batch inertia: 67384.72384790683, ewa inertia: 67920.63685227174\n",
      "Minibatch step 668/41783: mean batch inertia: 68200.82173502902, ewa inertia: 67923.31912829872\n",
      "Minibatch step 669/41783: mean batch inertia: 68275.40778380651, ewa inertia: 67926.68975611687\n",
      "Minibatch step 670/41783: mean batch inertia: 68179.83262463643, ewa inertia: 67929.11315255765\n",
      "Minibatch step 671/41783: mean batch inertia: 67766.70379855554, ewa inertia: 67927.55836947115\n",
      "Minibatch step 672/41783: mean batch inertia: 67786.04157047109, ewa inertia: 67926.20359574589\n",
      "Minibatch step 673/41783: mean batch inertia: 67673.50840514169, ewa inertia: 67923.78448503146\n",
      "Minibatch step 674/41783: mean batch inertia: 67981.69576118194, ewa inertia: 67924.33888334938\n",
      "Minibatch step 675/41783: mean batch inertia: 67742.5743326739, ewa inertia: 67922.59880839588\n",
      "Minibatch step 676/41783: mean batch inertia: 67999.04101187969, ewa inertia: 67923.33060765585\n",
      "Minibatch step 677/41783: mean batch inertia: 68438.68388748346, ewa inertia: 67928.26420626514\n",
      "Minibatch step 678/41783: mean batch inertia: 67465.75689509786, ewa inertia: 67923.83651458939\n",
      "Minibatch step 679/41783: mean batch inertia: 67842.98965559482, ewa inertia: 67923.06254852239\n",
      "Minibatch step 680/41783: mean batch inertia: 67423.78533146677, ewa inertia: 67918.28284985713\n",
      "Minibatch step 681/41783: mean batch inertia: 67978.74523785402, ewa inertia: 67918.86167057102\n",
      "Minibatch step 682/41783: mean batch inertia: 67414.50991600977, ewa inertia: 67914.03339216027\n",
      "Minibatch step 683/41783: mean batch inertia: 68103.27150283009, ewa inertia: 67915.8450132679\n",
      "Minibatch step 684/41783: mean batch inertia: 67473.39564052562, ewa inertia: 67911.60934097185\n",
      "Minibatch step 685/41783: mean batch inertia: 68121.13220488919, ewa inertia: 67913.61515281102\n",
      "Minibatch step 686/41783: mean batch inertia: 67702.25381147224, ewa inertia: 67911.59174079346\n",
      "Minibatch step 687/41783: mean batch inertia: 67426.06473580493, ewa inertia: 67906.9436761544\n",
      "Minibatch step 688/41783: mean batch inertia: 68337.0978906575, ewa inertia: 67911.06164399807\n",
      "Minibatch step 689/41783: mean batch inertia: 67869.96145449908, ewa inertia: 67910.66818218131\n",
      "Minibatch step 690/41783: mean batch inertia: 68007.57292639658, ewa inertia: 67911.59587417437\n",
      "Minibatch step 691/41783: mean batch inertia: 67835.92248278909, ewa inertia: 67910.87143493412\n",
      "Minibatch step 692/41783: mean batch inertia: 67828.46585453965, ewa inertia: 67910.08254685918\n",
      "Minibatch step 693/41783: mean batch inertia: 67869.81689101034, ewa inertia: 67909.69707423006\n",
      "Minibatch step 694/41783: mean batch inertia: 67850.88576621462, ewa inertia: 67909.13405969461\n",
      "Minibatch step 695/41783: mean batch inertia: 68069.07789591947, ewa inertia: 67910.66523979737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 696/41783: mean batch inertia: 67749.37100886667, ewa inertia: 67909.1211320473\n",
      "Minibatch step 697/41783: mean batch inertia: 68158.92482404989, ewa inertia: 67911.512561763\n",
      "Minibatch step 698/41783: mean batch inertia: 67782.60945375153, ewa inertia: 67910.27854187934\n",
      "Minibatch step 699/41783: mean batch inertia: 67771.93033717583, ewa inertia: 67908.95410185528\n",
      "Minibatch step 700/41783: mean batch inertia: 67859.77789787317, ewa inertia: 67908.48332644533\n",
      "Minibatch step 701/41783: mean batch inertia: 67952.98914789628, ewa inertia: 67908.90939118074\n",
      "Minibatch step 702/41783: mean batch inertia: 67834.00096742738, ewa inertia: 67908.19227515621\n",
      "Minibatch step 703/41783: mean batch inertia: 68148.87087897974, ewa inertia: 67910.49634824858\n",
      "Minibatch step 704/41783: mean batch inertia: 68012.82499918557, ewa inertia: 67911.47596458113\n",
      "Minibatch step 705/41783: mean batch inertia: 67553.30950119768, ewa inertia: 67908.04715247336\n",
      "Minibatch step 706/41783: mean batch inertia: 67905.51131596598, ewa inertia: 67908.02287631182\n",
      "Minibatch step 707/41783: mean batch inertia: 67695.9708989635, ewa inertia: 67905.9928526727\n",
      "Minibatch step 708/41783: mean batch inertia: 67587.30290372427, ewa inertia: 67902.941958557\n",
      "Minibatch step 709/41783: mean batch inertia: 67782.02642798652, ewa inertia: 67901.78440563784\n",
      "Minibatch step 710/41783: mean batch inertia: 68049.77223480256, ewa inertia: 67903.20112806252\n",
      "Minibatch step 711/41783: mean batch inertia: 67275.10973677349, ewa inertia: 67897.18826089766\n",
      "Minibatch step 712/41783: mean batch inertia: 67459.0397758908, ewa inertia: 67892.99376201522\n",
      "Minibatch step 713/41783: mean batch inertia: 67649.8111835396, ewa inertia: 67890.6657177824\n",
      "Minibatch step 714/41783: mean batch inertia: 67701.21735396488, ewa inertia: 67888.85208387172\n",
      "Minibatch step 715/41783: mean batch inertia: 67669.44236009294, ewa inertia: 67886.75162278904\n",
      "Minibatch step 716/41783: mean batch inertia: 67919.92475845307, ewa inertia: 67887.06919704807\n",
      "Minibatch step 717/41783: mean batch inertia: 67021.79310047782, ewa inertia: 67878.78570470642\n",
      "Minibatch step 718/41783: mean batch inertia: 68331.35228904319, ewa inertia: 67883.11823145747\n",
      "Minibatch step 719/41783: mean batch inertia: 67544.4722878182, ewa inertia: 67879.87629389351\n",
      "Minibatch step 720/41783: mean batch inertia: 67584.66321731347, ewa inertia: 67877.05014941927\n",
      "Minibatch step 721/41783: mean batch inertia: 67643.67502426161, ewa inertia: 67874.81599425233\n",
      "Minibatch step 722/41783: mean batch inertia: 67843.18813009493, ewa inertia: 67874.51321324223\n",
      "Minibatch step 723/41783: mean batch inertia: 67688.58945624744, ewa inertia: 67872.73332122475\n",
      "Minibatch step 724/41783: mean batch inertia: 67799.49650086921, ewa inertia: 67872.03220785416\n",
      "Minibatch step 725/41783: mean batch inertia: 67738.3219585461, ewa inertia: 67870.75216807202\n",
      "Minibatch step 726/41783: mean batch inertia: 67427.92913744734, ewa inertia: 67866.51291866084\n",
      "Minibatch step 727/41783: mean batch inertia: 67607.74360468192, ewa inertia: 67864.03565892941\n",
      "Minibatch step 728/41783: mean batch inertia: 67645.74897390562, ewa inertia: 67861.94594896186\n",
      "Minibatch step 729/41783: mean batch inertia: 67733.6270735954, ewa inertia: 67860.71752207523\n",
      "Minibatch step 730/41783: mean batch inertia: 67583.81094010486, ewa inertia: 67858.06662999558\n",
      "Minibatch step 731/41783: mean batch inertia: 67704.64662913294, ewa inertia: 67856.59790410886\n",
      "Minibatch step 732/41783: mean batch inertia: 68050.36479146787, ewa inertia: 67858.45288026487\n",
      "Minibatch step 733/41783: mean batch inertia: 67434.46643443672, ewa inertia: 67854.39395792679\n",
      "Minibatch step 734/41783: mean batch inertia: 67853.45490593516, ewa inertia: 67854.38496816039\n",
      "Minibatch step 735/41783: mean batch inertia: 68174.05319522874, ewa inertia: 67857.44522756345\n",
      "Minibatch step 736/41783: mean batch inertia: 67637.74397478142, ewa inertia: 67855.34197560481\n",
      "Minibatch step 737/41783: mean batch inertia: 67500.58561266193, ewa inertia: 67851.94580919361\n",
      "Minibatch step 738/41783: mean batch inertia: 67573.62533196277, ewa inertia: 67849.28138156085\n",
      "Minibatch step 739/41783: mean batch inertia: 67796.16083214017, ewa inertia: 67848.77284600087\n",
      "Minibatch step 740/41783: mean batch inertia: 68289.95607856265, ewa inertia: 67852.99639723806\n",
      "Minibatch step 741/41783: mean batch inertia: 67559.12241607388, ewa inertia: 67850.18307224038\n",
      "Minibatch step 742/41783: mean batch inertia: 68176.97348052466, ewa inertia: 67853.3115139656\n",
      "Minibatch step 743/41783: mean batch inertia: 67751.35554994359, ewa inertia: 67852.33546545287\n",
      "Minibatch step 744/41783: mean batch inertia: 67428.55131123467, ewa inertia: 67848.27847970012\n",
      "Minibatch step 745/41783: mean batch inertia: 68056.70476025224, ewa inertia: 67850.27379368787\n",
      "Minibatch step 746/41783: mean batch inertia: 67519.98421542163, ewa inertia: 67847.11185358235\n",
      "Minibatch step 747/41783: mean batch inertia: 67610.78710609129, ewa inertia: 67844.8494609845\n",
      "Minibatch step 748/41783: mean batch inertia: 68386.57131684019, ewa inertia: 67850.03549219703\n",
      "Minibatch step 749/41783: mean batch inertia: 67558.20891773562, ewa inertia: 67847.24176750705\n",
      "Minibatch step 750/41783: mean batch inertia: 67918.71360226793, ewa inertia: 67847.92598425384\n",
      "Minibatch step 751/41783: mean batch inertia: 67382.81681141921, ewa inertia: 67843.47338434211\n",
      "Minibatch step 752/41783: mean batch inertia: 68114.29604455725, ewa inertia: 67846.06603360253\n",
      "Minibatch step 753/41783: mean batch inertia: 67584.04515649153, ewa inertia: 67843.55764588962\n",
      "Minibatch step 754/41783: mean batch inertia: 68003.66516265564, ewa inertia: 67845.09039294485\n",
      "Minibatch step 755/41783: mean batch inertia: 67977.16915960672, ewa inertia: 67846.35481415848\n",
      "Minibatch step 756/41783: mean batch inertia: 67743.90123879834, ewa inertia: 67845.37400189493\n",
      "Minibatch step 757/41783: mean batch inertia: 68010.75752383145, ewa inertia: 67846.95725739317\n",
      "Minibatch step 758/41783: mean batch inertia: 67523.0950425001, ewa inertia: 67843.85684795454\n",
      "Minibatch step 759/41783: mean batch inertia: 67948.78657655489, ewa inertia: 67844.8613650176\n",
      "Minibatch step 760/41783: mean batch inertia: 67487.0759011344, ewa inertia: 67841.43620030799\n",
      "Minibatch step 761/41783: mean batch inertia: 67830.92050924331, ewa inertia: 67841.33553111494\n",
      "Minibatch step 762/41783: mean batch inertia: 67681.1995151013, ewa inertia: 67839.80251122967\n",
      "Minibatch step 763/41783: mean batch inertia: 67829.1088425745, ewa inertia: 67839.70013821514\n",
      "Minibatch step 764/41783: mean batch inertia: 68442.56785035164, ewa inertia: 67845.4715331448\n",
      "Minibatch step 765/41783: mean batch inertia: 67863.48693492706, ewa inertia: 67845.64399883902\n",
      "Minibatch step 766/41783: mean batch inertia: 67638.76760444448, ewa inertia: 67843.66352227735\n",
      "Minibatch step 767/41783: mean batch inertia: 67490.738475833, ewa inertia: 67840.2848874913\n",
      "Minibatch step 768/41783: mean batch inertia: 67427.36974663706, ewa inertia: 67836.33195336937\n",
      "Minibatch step 769/41783: mean batch inertia: 67364.34964494313, ewa inertia: 67831.81355530805\n",
      "Minibatch step 770/41783: mean batch inertia: 67990.42111101077, ewa inertia: 67833.33194288191\n",
      "Minibatch step 771/41783: mean batch inertia: 68102.2713910925, ewa inertia: 67835.90656370924\n",
      "Minibatch step 772/41783: mean batch inertia: 67838.5544210142, ewa inertia: 67835.93191227231\n",
      "Minibatch step 773/41783: mean batch inertia: 68186.49203672018, ewa inertia: 67839.28790710177\n",
      "Minibatch step 774/41783: mean batch inertia: 67776.36259067726, ewa inertia: 67838.6855081926\n",
      "Minibatch step 775/41783: mean batch inertia: 67989.66370475228, ewa inertia: 67840.1308581103\n",
      "Minibatch step 776/41783: mean batch inertia: 67720.53360948143, ewa inertia: 67838.98592541537\n",
      "Minibatch step 777/41783: mean batch inertia: 67686.4691761504, ewa inertia: 67837.52584656942\n",
      "Minibatch step 778/41783: mean batch inertia: 67638.67875499935, ewa inertia: 67835.62223641919\n",
      "Minibatch step 779/41783: mean batch inertia: 67374.14157488728, ewa inertia: 67831.20437310277\n",
      "Minibatch step 780/41783: mean batch inertia: 67430.67046019962, ewa inertia: 67827.36996739889\n",
      "Minibatch step 781/41783: mean batch inertia: 68183.5630862602, ewa inertia: 67830.77988821367\n",
      "Minibatch step 782/41783: mean batch inertia: 68261.2023473238, ewa inertia: 67834.90042402627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch step 783/41783: mean batch inertia: 67379.67861293032, ewa inertia: 67830.54247816262\n",
      "Minibatch step 784/41783: mean batch inertia: 67924.81088998188, ewa inertia: 67831.44493192328\n",
      "Minibatch step 785/41783: mean batch inertia: 67566.47761843275, ewa inertia: 67828.90833727934\n",
      "Minibatch step 786/41783: mean batch inertia: 67626.5484592414, ewa inertia: 67826.97109839498\n",
      "Minibatch step 787/41783: mean batch inertia: 67885.04354548854, ewa inertia: 67827.5270396404\n",
      "Minibatch step 788/41783: mean batch inertia: 67533.6203612067, ewa inertia: 67824.71340162402\n",
      "Minibatch step 789/41783: mean batch inertia: 68055.06136967588, ewa inertia: 67826.91857710133\n",
      "Minibatch step 790/41783: mean batch inertia: 67371.4085041706, ewa inertia: 67822.55787163907\n",
      "Minibatch step 791/41783: mean batch inertia: 68094.00998147736, ewa inertia: 67825.15654676933\n",
      "Minibatch step 792/41783: mean batch inertia: 68132.6733252854, ewa inertia: 67828.10047748678\n",
      "Minibatch step 793/41783: mean batch inertia: 67481.01681629919, ewa inertia: 67824.7777636608\n",
      "Minibatch step 794/41783: mean batch inertia: 67721.32053270085, ewa inertia: 67823.78734316524\n",
      "Minibatch step 795/41783: mean batch inertia: 67692.4330907961, ewa inertia: 67822.52985789799\n",
      "Minibatch step 796/41783: mean batch inertia: 68128.86044853227, ewa inertia: 67825.46243295881\n",
      "Minibatch step 797/41783: mean batch inertia: 68077.91993566552, ewa inertia: 67827.87926823087\n",
      "Minibatch step 798/41783: mean batch inertia: 67277.46394427966, ewa inertia: 67822.61001239595\n",
      "Minibatch step 799/41783: mean batch inertia: 68060.63059722712, ewa inertia: 67824.88863964501\n",
      "Minibatch step 800/41783: mean batch inertia: 67974.93460546562, ewa inertia: 67826.32506509776\n",
      "Minibatch step 801/41783: mean batch inertia: 67807.23361177859, ewa inertia: 67826.14229810814\n",
      "Minibatch step 802/41783: mean batch inertia: 67563.45393570313, ewa inertia: 67823.62752040094\n",
      "Minibatch step 803/41783: mean batch inertia: 67463.36799104682, ewa inertia: 67820.17867087852\n",
      "Minibatch step 804/41783: mean batch inertia: 67647.97803205281, ewa inertia: 67818.53015351095\n",
      "Minibatch step 805/41783: mean batch inertia: 68016.29092356979, ewa inertia: 67820.42336404888\n",
      "Minibatch step 806/41783: mean batch inertia: 67932.7488280809, ewa inertia: 67821.49868223318\n",
      "Minibatch step 807/41783: mean batch inertia: 67724.30529677616, ewa inertia: 67820.56822700937\n",
      "Minibatch step 808/41783: mean batch inertia: 67891.73506147617, ewa inertia: 67821.24952391634\n",
      "Minibatch step 809/41783: mean batch inertia: 68161.44181431954, ewa inertia: 67824.50626502292\n",
      "Minibatch step 810/41783: mean batch inertia: 67602.2847079242, ewa inertia: 67822.37888559609\n",
      "Minibatch step 811/41783: mean batch inertia: 68137.83449277442, ewa inertia: 67825.39881659446\n",
      "Minibatch step 812/41783: mean batch inertia: 67827.87734478855, ewa inertia: 67825.42254412998\n",
      "Minibatch step 813/41783: mean batch inertia: 68203.75158119024, ewa inertia: 67829.04437731541\n",
      "Minibatch step 814/41783: mean batch inertia: 68313.8773011329, ewa inertia: 67833.68579735156\n",
      "Minibatch step 815/41783: mean batch inertia: 67769.6173742655, ewa inertia: 67833.07245521246\n",
      "Minibatch step 816/41783: mean batch inertia: 67983.03081992232, ewa inertia: 67834.50804203909\n",
      "Minibatch step 817/41783: mean batch inertia: 67356.59692272356, ewa inertia: 67829.93288607149\n",
      "Minibatch step 818/41783: mean batch inertia: 67985.18994270351, ewa inertia: 67831.41919852678\n",
      "Minibatch step 819/41783: mean batch inertia: 67640.4305959859, ewa inertia: 67829.59081954721\n",
      "Minibatch step 820/41783: mean batch inertia: 67558.36237561426, ewa inertia: 67826.99428562346\n",
      "Minibatch step 821/41783: mean batch inertia: 67537.68838670242, ewa inertia: 67824.22469195547\n",
      "Minibatch step 822/41783: mean batch inertia: 67585.59409821422, ewa inertia: 67821.94022494712\n",
      "Minibatch step 823/41783: mean batch inertia: 68078.71842109763, ewa inertia: 67824.39842323752\n",
      "Minibatch step 824/41783: mean batch inertia: 68691.35127541935, ewa inertia: 67832.69796755648\n",
      "Minibatch step 825/41783: mean batch inertia: 67975.89265944669, ewa inertia: 67834.06880414647\n",
      "Minibatch step 826/41783: mean batch inertia: 67276.00781039533, ewa inertia: 67828.7263545096\n",
      "Minibatch step 827/41783: mean batch inertia: 67599.82143412548, ewa inertia: 67826.53499366826\n",
      "Minibatch step 828/41783: mean batch inertia: 67666.69695430156, ewa inertia: 67825.00482638381\n",
      "Minibatch step 829/41783: mean batch inertia: 68141.5225620588, ewa inertia: 67828.03492538903\n",
      "Minibatch step 830/41783: mean batch inertia: 67611.67951517222, ewa inertia: 67825.96370397114\n",
      "Minibatch step 831/41783: mean batch inertia: 67683.31147198612, ewa inertia: 67824.59806047786\n",
      "Minibatch step 832/41783: mean batch inertia: 67817.4608363273, ewa inertia: 67824.52973414614\n",
      "Minibatch step 833/41783: mean batch inertia: 68041.7951197421, ewa inertia: 67826.60966697315\n",
      "Minibatch step 834/41783: mean batch inertia: 67623.64857981192, ewa inertia: 67824.66667257193\n",
      "Converged (lack of improvement in inertia) at step 834/41783\n",
      "kMeans Model Saved!\n",
      "kMeans Model loaded...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 170>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dataX.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    169\u001b[0m DSift_extraction(images)\n\u001b[1;32m--> 170\u001b[0m \u001b[43mSift_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m load()\n\u001b[0;32m    172\u001b[0m DSift(images)\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mSift_extraction\u001b[1;34m(images)\u001b[0m\n\u001b[0;32m     38\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(kmeans, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans_model.sav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkMeans Model Saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m \u001b[43mload_Kmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36mload_Kmeans\u001b[1;34m()\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m des \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m des:\n\u001b[1;32m---> 60\u001b[0m             idx \u001b[38;5;241m=\u001b[39m \u001b[43mkmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m             histo[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mnkp\n\u001b[0;32m     63\u001b[0m histo_list\u001b[38;5;241m.\u001b[39mappend(histo)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:2151\u001b[0m, in \u001b[0;36mMiniBatchKMeans.predict\u001b[1;34m(self, X, sample_weight)\u001b[0m\n\u001b[0;32m   2148\u001b[0m x_squared_norms \u001b[38;5;241m=\u001b[39m row_norms(X, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2149\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 2151\u001b[0m labels, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_labels_inertia_threadpool_limit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_squared_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_centers_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m labels\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:755\u001b[0m, in \u001b[0;36m_labels_inertia_threadpool_limit\u001b[1;34m(X, sample_weight, x_squared_norms, centers, n_threads)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_labels_inertia_threadpool_limit\u001b[39m(\n\u001b[0;32m    752\u001b[0m     X, sample_weight, x_squared_norms, centers, n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    753\u001b[0m ):\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;124;03m\"\"\"Same as _labels_inertia but in a threadpool_limits context.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    756\u001b[0m         labels, inertia \u001b[38;5;241m=\u001b[39m _labels_inertia(\n\u001b[0;32m    757\u001b[0m             X, sample_weight, x_squared_norms, centers, n_threads\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels, inertia\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:314\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mthreadpoolctl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_api\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_threadpool_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m \u001b[43m_ThreadpoolInfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m                          \u001b[49m\u001b[43muser_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_api\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 340\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py:373\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_modules_with_enum_process_module_ex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dl_iterate_phdr()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\threadpoolctl.py:478\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m n_size \u001b[38;5;241m=\u001b[39m DWORD()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h_module \u001b[38;5;129;01min\u001b[39;00m h_modules:\n\u001b[0;32m    476\u001b[0m \n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# Get the path of the current module\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mps_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetModuleFileNameExW\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m            \u001b[49m\u001b[43mh_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGetModuleFileNameEx failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    482\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m buf\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path, os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from skimage import io\n",
    "\n",
    "def Sift_extraction(images):\n",
    "    print(images.shape)\n",
    "    #extract sift descriptos of image\n",
    "    desc = []\n",
    "    flag = 'Image file found!'\n",
    "    if not os.path.exists('pics'):\n",
    "        flag = 'Writing images...'\n",
    "        os.makedirs('pics')\n",
    "    print(flag)\n",
    "\n",
    "    for idx in range(len(images)):\n",
    "        filepath = 'pics/'+str(idx)+'.jpg'\n",
    "        if (flag =='Writing images...'):\n",
    "            cv2.imwrite(filepath, images[idx])\n",
    "        img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "        sift = cv2.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "\n",
    "        if des is not None:\n",
    "            for d in des:\n",
    "                 desc.append(d)\n",
    "    print('SIFT descriptors ready to cluster...')\n",
    "\n",
    "    #KMeans clusting\n",
    "    k = 2048\n",
    "    print('clustering..')\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, max_iter = 200, batch_size=k*2,\n",
    "        max_no_improvement=30, verbose=1).fit(desc)\n",
    "    pickle.dump(kmeans, open('kmeans_model.sav', 'wb'))\n",
    "    print(\"kMeans Model Saved!\")\n",
    "    load_Kmeans()\n",
    "\n",
    "#Load kMeans clustering\n",
    "def load_Kmeans():\n",
    "    kmeans =pickle.load(open('kmeans_model.sav', 'rb'))\n",
    "    print('kMeans Model loaded...')\n",
    "    kmeans.verbose=False\n",
    "    histo_list = []\n",
    "    for idx in range(len(images)):\n",
    "        print(idx)\n",
    "        filepath = 'pics/'+str(idx)+'.jpg'\n",
    "        img = cv2.imread(filepath)\n",
    "        sift = cv2.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "        k=2048\n",
    "        histo = np.zeros(k)\n",
    "        nkp = np.size(kp)\n",
    "\n",
    "        if des is not None:\n",
    "            for d in des:\n",
    "                idx = kmeans.predict([d])\n",
    "                histo[idx] +=1/nkp\n",
    "    \n",
    "    histo_list.append(histo)\n",
    "    print(np.array(histo_list).shape)\n",
    "    np.save('sift_histogram', histo_list)\n",
    "    print('sift_histogram saved!')\n",
    "\n",
    "\n",
    "def DSift_extraction(images):\n",
    "    print(images.shape)\n",
    "    descriptors = []\n",
    "    flag = 'Image file found!'\n",
    "    if not os.path.exists('pics'):\n",
    "        flag = 'Writing images...'\n",
    "        os.makedirs('pics')\n",
    "    print(flag)\n",
    "    for idx in range(len(images)):\n",
    "        #print(idx)\n",
    "        filepath = 'pics/'+str(idx)+'.jpg'\n",
    "        if(flag == 'Writing images...'):\n",
    "            if not cv2.imwrite(filepath, images[idx]):\n",
    "                raise Exception('cannot write image')\n",
    "        img = cv2.imread(filepath)\n",
    "        gray= cv2.cvtColor(img ,cv2.COLOR_BGR2GRAY)\n",
    "        sift = cv2.SIFT_create()\n",
    "\n",
    "        step_size = 12\n",
    "        kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, gray.shape[0], step_size) \n",
    "                                            for x in range(0, gray.shape[1], step_size)]\n",
    "\n",
    "        dense_feat, desc = sift.compute(gray, kp)\n",
    "\n",
    "        if desc is not None:\n",
    "            desc = desc.flatten()\n",
    "            descriptors.append(desc)\n",
    "\n",
    "    np.save('d_sift', descriptors)\n",
    "    print('d_sift saved!')\n",
    "\n",
    "    return descriptors\n",
    "\n",
    "initXyStep = 12\n",
    "initFeatureScale = 12\n",
    "initImgBound = 6\n",
    "\n",
    "def detect(img):\n",
    "    keypoints = []\n",
    "    rows, cols = img.shape[:2]\n",
    "    for x in range(initImgBound, rows, initFeatureScale):\n",
    "        for y in range(initImgBound, cols, initFeatureScale):\n",
    "            keypoints.append(cv2.KeyPoint(float(x), float(y), initXyStep))\n",
    "    return keypoints \n",
    "\n",
    "#Dense SIFT feature extraction\t\n",
    "def DSift(images):\n",
    "    desp = []\n",
    "    flag = 'Image file found!'\n",
    "    if not os.path.exists('pics'):\n",
    "        flag = 'Writing images...'\n",
    "        os.makedirs('pics')\n",
    "    print(flag)\n",
    "    for idx in range(len(images)):\n",
    "        filepath = 'pics/'+str(idx)+'.jpg'\n",
    "        if(flag == 'Writing images...'):\n",
    "            if not cv2.imwrite(filepath, images[idx]):\n",
    "                raise Exception('cannot write image')\n",
    "\n",
    "        input_image = cv2.imread(filepath)\n",
    "        keypoints = detect(input_image)\n",
    "\n",
    "        sift = cv2.SIFT_create()\n",
    "        dense_feat, desc = sift.compute(input_image, keypoints)\n",
    "        \n",
    "        temp = []\n",
    "        for d in desc:\n",
    "            for i in d:\n",
    "                temp.append(i)\n",
    "        desp.append(temp)\n",
    "    print(np.array(desp).shape)\n",
    "    np.save('d_sift', desp) \n",
    "    print('d_sift saved!')\n",
    "\n",
    "    return desp\n",
    "\n",
    "\n",
    "def checkfile():\n",
    "    filepath = './pics'\n",
    "    if os.path.exists('pics'):\n",
    "        print('file exists!')\n",
    "    else:\n",
    "        print('no file')\n",
    "    print(os.path.getsize('pics'))\n",
    "    print(os.stat('./pics').st_size)\n",
    "\n",
    "images = np.load('./dataX.npy')\n",
    "DSift_extraction(images)\n",
    "Sift_extraction(images)\n",
    "load()\n",
    "DSift(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 23, 23, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 23, 23, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 23, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 19, 19, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 9, 9, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 9, 9, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              1050624   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 14343     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,352,295\n",
      "Trainable params: 1,351,847\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n",
      "Custon CNN Model created successfully...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x000002054AFD2820>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.layers import Input\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import Model\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "#SIFT layers\n",
    "def Sift_layer():\n",
    "    #sift features as input\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4096, input_shape=(2048,), kernel_regularizer=l2(0.01)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    return model\n",
    "\n",
    "#cnn model\n",
    "def custom_cnn ():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(48, 48 ,1), padding='same', activation = 'relu'))\n",
    "    model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation ='relu'))\n",
    "    model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides = (2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    #Fully connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    print('Custon CNN Model created successfully...')\n",
    "    return model\n",
    "\n",
    "#cnn layer\n",
    "def cnn_layer():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(48, 48 ,1), padding='same', activation = 'relu'))\n",
    "    model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides = (2,2)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "    model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides = (2,2)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'relu'))\n",
    "    model.add(Conv2D(128,(3,3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides = (2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    print('CNN layer created successfully...')\n",
    "    return model\n",
    "\n",
    "#fully-connected layer\n",
    "def FC_layer(mergemodel):\n",
    "    fc = Dense(2048, activation='relu')(mergemodel)\n",
    "    fc = Dropout(0.5)(fc)\n",
    "    fc = Dense(num_classes, activation = 'softmax')(fc)\n",
    "    print('Fully-connected Layer created successfully...')\n",
    "    return fc\n",
    "\n",
    "a = custom_cnn()\n",
    "a.summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, os, argparse\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Save model results\n",
    "def save_model(model, history):\n",
    "    np.save('Result/'+fileName+'_histo', history)\n",
    "    print(fileName+'_histo.npy has been saved!')\n",
    "    #serialize model to json\n",
    "    model_json = model.to_json()\n",
    "    with open('Result/'+fileName+'.json', 'w') as file:\n",
    "        file.write(model_json)\n",
    "    print(fileName+'.json has been saved!')\n",
    "    print('Mode saved!')\n",
    "\n",
    "def SIFT_data_split(X_sift):\n",
    "    X_sift_train, X_sift_test = train_test_split(X_sift, test_size=0.2, random_state=42)\n",
    "    X_sift_train, X_sift_val = train_test_split(X_sift_train, test_size = 0.2, random_state= 40)\n",
    "    return X_sift_train, X_sift_val, X_sift_test\n",
    "\n",
    "\n",
    "def load_sift():\n",
    "    if not os.path.exists('Result/sift_histogram.npy'):\n",
    "        X_sift = Sift_extraction(x)\n",
    "    else:\n",
    "        print('sift histogram exist!')\n",
    "        X_sift = np.load('Result/sift_histogram.npy')\n",
    "    return X_sift\n",
    "\n",
    "def load_dsift():\n",
    "    if not os.path.exists('Result/d_sift.npy'):\n",
    "        X_dsift = DSift_extraction(x)\n",
    "    else:\n",
    "        print('dsift descriptors exist!')\n",
    "        X_dsift = np.load('Result/d_sift.npy')\n",
    "    return X_dsift\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training Model on cnn...\n",
      "DataSize (35887, 48, 48) (35887,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 46, 46, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 23, 23, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 23, 23, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 23, 23, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 21, 21, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 19, 19, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 9, 9, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 9, 9, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 5, 5, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 2, 2, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2048)              1050624   \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 14343     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,352,295\n",
      "Trainable params: 1,351,847\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n",
      "Custon CNN Model created successfully...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dheeresh\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\Dheeresh\\AppData\\Local\\Temp\\ipykernel_8472\\2509476563.py:53: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  training_history = model.fit_generator(data_generator.flow(data_input,y_train,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227/229 [============================>.] - ETA: 0s - loss: 2.3150 - accuracy: 0.2085\n",
      "Epoch 1: val_accuracy improved from -inf to 0.27935, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 10s 18ms/step - loss: 2.3111 - accuracy: 0.2092 - val_loss: 1.7884 - val_accuracy: 0.2793\n",
      "Epoch 2/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0992 - accuracy: 0.2454\n",
      "Epoch 2: val_accuracy improved from 0.27935 to 0.28144, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0993 - accuracy: 0.2454 - val_loss: 1.8419 - val_accuracy: 0.2814\n",
      "Epoch 3/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 2.0432 - accuracy: 0.2577\n",
      "Epoch 3: val_accuracy improved from 0.28144 to 0.31034, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 2.0432 - accuracy: 0.2577 - val_loss: 1.8761 - val_accuracy: 0.3103\n",
      "Epoch 4/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9918 - accuracy: 0.2788\n",
      "Epoch 4: val_accuracy improved from 0.31034 to 0.33734, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.9934 - accuracy: 0.2784 - val_loss: 1.7554 - val_accuracy: 0.3373\n",
      "Epoch 5/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9491 - accuracy: 0.2927\n",
      "Epoch 5: val_accuracy did not improve from 0.33734\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.9489 - accuracy: 0.2927 - val_loss: 2.1631 - val_accuracy: 0.2132\n",
      "Epoch 6/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9182 - accuracy: 0.3071\n",
      "Epoch 6: val_accuracy improved from 0.33734 to 0.33856, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.9189 - accuracy: 0.3071 - val_loss: 1.8620 - val_accuracy: 0.3386\n",
      "Epoch 7/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8676 - accuracy: 0.3244\n",
      "Epoch 7: val_accuracy improved from 0.33856 to 0.34831, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.8676 - accuracy: 0.3244 - val_loss: 1.7747 - val_accuracy: 0.3483\n",
      "Epoch 8/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8331 - accuracy: 0.3347\n",
      "Epoch 8: val_accuracy improved from 0.34831 to 0.35772, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.8331 - accuracy: 0.3347 - val_loss: 1.9023 - val_accuracy: 0.3577\n",
      "Epoch 9/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8016 - accuracy: 0.3428\n",
      "Epoch 9: val_accuracy improved from 0.35772 to 0.39464, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8018 - accuracy: 0.3431 - val_loss: 1.7605 - val_accuracy: 0.3946\n",
      "Epoch 10/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.7665 - accuracy: 0.3517\n",
      "Epoch 10: val_accuracy did not improve from 0.39464\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.7663 - accuracy: 0.3515 - val_loss: 1.7739 - val_accuracy: 0.3699\n",
      "Epoch 11/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.7480 - accuracy: 0.3631\n",
      "Epoch 11: val_accuracy improved from 0.39464 to 0.40996, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.7481 - accuracy: 0.3633 - val_loss: 1.6971 - val_accuracy: 0.4100\n",
      "Epoch 12/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.7187 - accuracy: 0.3693\n",
      "Epoch 12: val_accuracy did not improve from 0.40996\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.7182 - accuracy: 0.3695 - val_loss: 1.8042 - val_accuracy: 0.3403\n",
      "Epoch 13/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.7215 - accuracy: 0.3784\n",
      "Epoch 13: val_accuracy did not improve from 0.40996\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.7222 - accuracy: 0.3782 - val_loss: 1.6121 - val_accuracy: 0.4061\n",
      "Epoch 14/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.6887 - accuracy: 0.3859\n",
      "Epoch 14: val_accuracy improved from 0.40996 to 0.43905, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.6877 - accuracy: 0.3863 - val_loss: 1.5232 - val_accuracy: 0.4390\n",
      "Epoch 15/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.6555 - accuracy: 0.3950\n",
      "Epoch 15: val_accuracy did not improve from 0.43905\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.6556 - accuracy: 0.3948 - val_loss: 1.5855 - val_accuracy: 0.4356\n",
      "Epoch 16/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.6472 - accuracy: 0.3955\n",
      "Epoch 16: val_accuracy improved from 0.43905 to 0.44340, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.6471 - accuracy: 0.3957 - val_loss: 1.6393 - val_accuracy: 0.4434\n",
      "Epoch 17/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.6095 - accuracy: 0.4087\n",
      "Epoch 17: val_accuracy improved from 0.44340 to 0.45524, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.6086 - accuracy: 0.4088 - val_loss: 1.5188 - val_accuracy: 0.4552\n",
      "Epoch 18/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.6076 - accuracy: 0.4128\n",
      "Epoch 18: val_accuracy improved from 0.45524 to 0.45925, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.6080 - accuracy: 0.4128 - val_loss: 1.5102 - val_accuracy: 0.4592\n",
      "Epoch 19/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5839 - accuracy: 0.4205\n",
      "Epoch 19: val_accuracy improved from 0.45925 to 0.47335, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.5838 - accuracy: 0.4206 - val_loss: 1.4545 - val_accuracy: 0.4734\n",
      "Epoch 20/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5698 - accuracy: 0.4223\n",
      "Epoch 20: val_accuracy did not improve from 0.47335\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.5698 - accuracy: 0.4221 - val_loss: 1.5520 - val_accuracy: 0.4302\n",
      "Epoch 21/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.4298\n",
      "Epoch 21: val_accuracy did not improve from 0.47335\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.5344 - accuracy: 0.4297 - val_loss: 1.6191 - val_accuracy: 0.4523\n",
      "Epoch 22/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5321 - accuracy: 0.4356\n",
      "Epoch 22: val_accuracy improved from 0.47335 to 0.48502, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.5322 - accuracy: 0.4355 - val_loss: 1.4391 - val_accuracy: 0.4850\n",
      "Epoch 23/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5069 - accuracy: 0.4419\n",
      "Epoch 23: val_accuracy did not improve from 0.48502\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.5069 - accuracy: 0.4419 - val_loss: 1.4999 - val_accuracy: 0.4584\n",
      "Epoch 24/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4923 - accuracy: 0.4512\n",
      "Epoch 24: val_accuracy improved from 0.48502 to 0.49843, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.4918 - accuracy: 0.4515 - val_loss: 1.4337 - val_accuracy: 0.4984\n",
      "Epoch 25/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4714 - accuracy: 0.4527\n",
      "Epoch 25: val_accuracy did not improve from 0.49843\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.4714 - accuracy: 0.4526 - val_loss: 1.4074 - val_accuracy: 0.4782\n",
      "Epoch 26/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.4595 - accuracy: 0.4583\n",
      "Epoch 26: val_accuracy improved from 0.49843 to 0.51062, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.4595 - accuracy: 0.4583 - val_loss: 1.3762 - val_accuracy: 0.5106\n",
      "Epoch 27/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4459 - accuracy: 0.4640\n",
      "Epoch 27: val_accuracy did not improve from 0.51062\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.4457 - accuracy: 0.4642 - val_loss: 1.3851 - val_accuracy: 0.4908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.4345 - accuracy: 0.4643\n",
      "Epoch 28: val_accuracy did not improve from 0.51062\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.4345 - accuracy: 0.4641 - val_loss: 1.3497 - val_accuracy: 0.5106\n",
      "Epoch 29/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4214 - accuracy: 0.4681\n",
      "Epoch 29: val_accuracy did not improve from 0.51062\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.4213 - accuracy: 0.4681 - val_loss: 1.3204 - val_accuracy: 0.5094\n",
      "Epoch 30/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4113 - accuracy: 0.4768\n",
      "Epoch 30: val_accuracy improved from 0.51062 to 0.51968, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.4109 - accuracy: 0.4769 - val_loss: 1.3052 - val_accuracy: 0.5197\n",
      "Epoch 31/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3848 - accuracy: 0.4790\n",
      "Epoch 31: val_accuracy did not improve from 0.51968\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.3846 - accuracy: 0.4792 - val_loss: 1.3126 - val_accuracy: 0.5075\n",
      "Epoch 32/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3737 - accuracy: 0.4821\n",
      "Epoch 32: val_accuracy improved from 0.51968 to 0.52264, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.3735 - accuracy: 0.4823 - val_loss: 1.2845 - val_accuracy: 0.5226\n",
      "Epoch 33/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.3617 - accuracy: 0.4838\n",
      "Epoch 33: val_accuracy did not improve from 0.52264\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.3612 - accuracy: 0.4840 - val_loss: 1.3395 - val_accuracy: 0.5005\n",
      "Epoch 34/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3493 - accuracy: 0.4885\n",
      "Epoch 34: val_accuracy improved from 0.52264 to 0.53135, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.3491 - accuracy: 0.4884 - val_loss: 1.2537 - val_accuracy: 0.5313\n",
      "Epoch 35/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.3378 - accuracy: 0.4924\n",
      "Epoch 35: val_accuracy did not improve from 0.53135\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.3383 - accuracy: 0.4923 - val_loss: 1.2663 - val_accuracy: 0.5256\n",
      "Epoch 36/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3280 - accuracy: 0.4964\n",
      "Epoch 36: val_accuracy did not improve from 0.53135\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.3287 - accuracy: 0.4964 - val_loss: 1.3838 - val_accuracy: 0.4929\n",
      "Epoch 37/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.3163 - accuracy: 0.5020\n",
      "Epoch 37: val_accuracy did not improve from 0.53135\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.3163 - accuracy: 0.5020 - val_loss: 1.2941 - val_accuracy: 0.5131\n",
      "Epoch 38/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3079 - accuracy: 0.5052\n",
      "Epoch 38: val_accuracy improved from 0.53135 to 0.54180, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.3077 - accuracy: 0.5053 - val_loss: 1.2219 - val_accuracy: 0.5418\n",
      "Epoch 39/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.3019 - accuracy: 0.5060\n",
      "Epoch 39: val_accuracy improved from 0.54180 to 0.56130, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.3019 - accuracy: 0.5060 - val_loss: 1.1764 - val_accuracy: 0.5613\n",
      "Epoch 40/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2940 - accuracy: 0.5076\n",
      "Epoch 40: val_accuracy did not improve from 0.56130\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.2940 - accuracy: 0.5076 - val_loss: 1.2415 - val_accuracy: 0.5303\n",
      "Epoch 41/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2893 - accuracy: 0.5118\n",
      "Epoch 41: val_accuracy did not improve from 0.56130\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.2891 - accuracy: 0.5119 - val_loss: 1.2071 - val_accuracy: 0.5441\n",
      "Epoch 42/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2731 - accuracy: 0.5204\n",
      "Epoch 42: val_accuracy did not improve from 0.56130\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.2731 - accuracy: 0.5204 - val_loss: 1.2408 - val_accuracy: 0.5289\n",
      "Epoch 43/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2722 - accuracy: 0.5217\n",
      "Epoch 43: val_accuracy did not improve from 0.56130\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.2722 - accuracy: 0.5217 - val_loss: 1.1956 - val_accuracy: 0.5451\n",
      "Epoch 44/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2630 - accuracy: 0.5241\n",
      "Epoch 44: val_accuracy improved from 0.56130 to 0.56618, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2630 - accuracy: 0.5241 - val_loss: 1.1524 - val_accuracy: 0.5662\n",
      "Epoch 45/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2485 - accuracy: 0.5313\n",
      "Epoch 45: val_accuracy did not improve from 0.56618\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2487 - accuracy: 0.5313 - val_loss: 1.1619 - val_accuracy: 0.5583\n",
      "Epoch 46/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2545 - accuracy: 0.5265\n",
      "Epoch 46: val_accuracy did not improve from 0.56618\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2545 - accuracy: 0.5265 - val_loss: 1.2057 - val_accuracy: 0.5381\n",
      "Epoch 47/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2403 - accuracy: 0.5335\n",
      "Epoch 47: val_accuracy improved from 0.56618 to 0.57140, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2403 - accuracy: 0.5335 - val_loss: 1.1454 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2326 - accuracy: 0.5308\n",
      "Epoch 48: val_accuracy did not improve from 0.57140\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2324 - accuracy: 0.5308 - val_loss: 1.1742 - val_accuracy: 0.5617\n",
      "Epoch 49/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.2383 - accuracy: 0.5348\n",
      "Epoch 49: val_accuracy did not improve from 0.57140\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2377 - accuracy: 0.5350 - val_loss: 1.1487 - val_accuracy: 0.5684\n",
      "Epoch 50/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2226 - accuracy: 0.5363\n",
      "Epoch 50: val_accuracy improved from 0.57140 to 0.57767, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2228 - accuracy: 0.5363 - val_loss: 1.1333 - val_accuracy: 0.5777\n",
      "Epoch 51/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.2242 - accuracy: 0.5399\n",
      "Epoch 51: val_accuracy did not improve from 0.57767\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2237 - accuracy: 0.5400 - val_loss: 1.2057 - val_accuracy: 0.5359\n",
      "Epoch 52/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2143 - accuracy: 0.5442\n",
      "Epoch 52: val_accuracy did not improve from 0.57767\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2144 - accuracy: 0.5443 - val_loss: 1.1936 - val_accuracy: 0.5519\n",
      "Epoch 53/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.2094 - accuracy: 0.5428\n",
      "Epoch 53: val_accuracy improved from 0.57767 to 0.57802, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2105 - accuracy: 0.5426 - val_loss: 1.1306 - val_accuracy: 0.5780\n",
      "Epoch 54/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2050 - accuracy: 0.5464\n",
      "Epoch 54: val_accuracy did not improve from 0.57802\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2053 - accuracy: 0.5462 - val_loss: 1.1891 - val_accuracy: 0.5468\n",
      "Epoch 55/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1984 - accuracy: 0.5472\n",
      "Epoch 55: val_accuracy did not improve from 0.57802\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1986 - accuracy: 0.5472 - val_loss: 1.1690 - val_accuracy: 0.5550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1973 - accuracy: 0.5458\n",
      "Epoch 56: val_accuracy did not improve from 0.57802\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1966 - accuracy: 0.5460 - val_loss: 1.2053 - val_accuracy: 0.5392\n",
      "Epoch 57/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1917 - accuracy: 0.5522\n",
      "Epoch 57: val_accuracy did not improve from 0.57802\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1918 - accuracy: 0.5524 - val_loss: 1.1696 - val_accuracy: 0.5552\n",
      "Epoch 58/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1894 - accuracy: 0.5560\n",
      "Epoch 58: val_accuracy improved from 0.57802 to 0.57820, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1904 - accuracy: 0.5554 - val_loss: 1.1229 - val_accuracy: 0.5782\n",
      "Epoch 59/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1802 - accuracy: 0.5515\n",
      "Epoch 59: val_accuracy did not improve from 0.57820\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1803 - accuracy: 0.5514 - val_loss: 1.1439 - val_accuracy: 0.5679\n",
      "Epoch 60/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1799 - accuracy: 0.5538\n",
      "Epoch 60: val_accuracy did not improve from 0.57820\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1796 - accuracy: 0.5539 - val_loss: 1.1291 - val_accuracy: 0.5782\n",
      "Epoch 61/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1737 - accuracy: 0.5542\n",
      "Epoch 61: val_accuracy did not improve from 0.57820\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1733 - accuracy: 0.5544 - val_loss: 1.1439 - val_accuracy: 0.5744\n",
      "Epoch 62/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1703 - accuracy: 0.5599\n",
      "Epoch 62: val_accuracy did not improve from 0.57820\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1703 - accuracy: 0.5601 - val_loss: 1.1449 - val_accuracy: 0.5705\n",
      "Epoch 63/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1712 - accuracy: 0.5548\n",
      "Epoch 63: val_accuracy did not improve from 0.57820\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1707 - accuracy: 0.5550 - val_loss: 1.1618 - val_accuracy: 0.5629\n",
      "Epoch 64/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1616 - accuracy: 0.5605\n",
      "Epoch 64: val_accuracy improved from 0.57820 to 0.58203, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1612 - accuracy: 0.5607 - val_loss: 1.1184 - val_accuracy: 0.5820\n",
      "Epoch 65/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1583 - accuracy: 0.5633\n",
      "Epoch 65: val_accuracy did not improve from 0.58203\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1585 - accuracy: 0.5631 - val_loss: 1.1319 - val_accuracy: 0.5707\n",
      "Epoch 66/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1514 - accuracy: 0.5650\n",
      "Epoch 66: val_accuracy did not improve from 0.58203\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1511 - accuracy: 0.5649 - val_loss: 1.1588 - val_accuracy: 0.5738\n",
      "Epoch 67/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1523 - accuracy: 0.5671\n",
      "Epoch 67: val_accuracy improved from 0.58203 to 0.58865, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1527 - accuracy: 0.5670 - val_loss: 1.1154 - val_accuracy: 0.5886\n",
      "Epoch 68/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1431 - accuracy: 0.5693\n",
      "Epoch 68: val_accuracy did not improve from 0.58865\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1440 - accuracy: 0.5692 - val_loss: 1.2130 - val_accuracy: 0.5364\n",
      "Epoch 69/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1441 - accuracy: 0.5671\n",
      "Epoch 69: val_accuracy did not improve from 0.58865\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1447 - accuracy: 0.5671 - val_loss: 1.1650 - val_accuracy: 0.5589\n",
      "Epoch 70/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1426 - accuracy: 0.5716\n",
      "Epoch 70: val_accuracy improved from 0.58865 to 0.59021, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1423 - accuracy: 0.5716 - val_loss: 1.0952 - val_accuracy: 0.5902\n",
      "Epoch 71/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1357 - accuracy: 0.5699\n",
      "Epoch 71: val_accuracy improved from 0.59021 to 0.59613, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1360 - accuracy: 0.5697 - val_loss: 1.0899 - val_accuracy: 0.5961\n",
      "Epoch 72/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1350 - accuracy: 0.5703\n",
      "Epoch 72: val_accuracy did not improve from 0.59613\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.1351 - accuracy: 0.5704 - val_loss: 1.1061 - val_accuracy: 0.5892\n",
      "Epoch 73/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1335 - accuracy: 0.5738\n",
      "Epoch 73: val_accuracy did not improve from 0.59613\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1332 - accuracy: 0.5740 - val_loss: 1.1472 - val_accuracy: 0.5754\n",
      "Epoch 74/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1234 - accuracy: 0.5781\n",
      "Epoch 74: val_accuracy improved from 0.59613 to 0.60014, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1233 - accuracy: 0.5781 - val_loss: 1.0788 - val_accuracy: 0.6001\n",
      "Epoch 75/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.1253 - accuracy: 0.5745\n",
      "Epoch 75: val_accuracy did not improve from 0.60014\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.1253 - accuracy: 0.5745 - val_loss: 1.0990 - val_accuracy: 0.5927\n",
      "Epoch 76/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1194 - accuracy: 0.5754\n",
      "Epoch 76: val_accuracy improved from 0.60014 to 0.60432, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.1194 - accuracy: 0.5754 - val_loss: 1.0773 - val_accuracy: 0.6043\n",
      "Epoch 77/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1181 - accuracy: 0.5805\n",
      "Epoch 77: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.1177 - accuracy: 0.5806 - val_loss: 1.1192 - val_accuracy: 0.5794\n",
      "Epoch 78/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1143 - accuracy: 0.5797\n",
      "Epoch 78: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1140 - accuracy: 0.5795 - val_loss: 1.1058 - val_accuracy: 0.5822\n",
      "Epoch 79/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1096 - accuracy: 0.5829\n",
      "Epoch 79: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1105 - accuracy: 0.5824 - val_loss: 1.0834 - val_accuracy: 0.5953\n",
      "Epoch 80/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1131 - accuracy: 0.5807\n",
      "Epoch 80: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1127 - accuracy: 0.5808 - val_loss: 1.0832 - val_accuracy: 0.6012\n",
      "Epoch 81/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1089 - accuracy: 0.5826\n",
      "Epoch 81: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1098 - accuracy: 0.5824 - val_loss: 1.1449 - val_accuracy: 0.5846\n",
      "Epoch 82/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1084 - accuracy: 0.5787\n",
      "Epoch 82: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1086 - accuracy: 0.5788 - val_loss: 1.1185 - val_accuracy: 0.5859\n",
      "Epoch 83/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.5830\n",
      "Epoch 83: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0981 - accuracy: 0.5833 - val_loss: 1.0890 - val_accuracy: 0.5951\n",
      "Epoch 84/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0973 - accuracy: 0.5866\n",
      "Epoch 84: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0978 - accuracy: 0.5864 - val_loss: 1.0897 - val_accuracy: 0.6015\n",
      "Epoch 85/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0917 - accuracy: 0.5851\n",
      "Epoch 85: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0915 - accuracy: 0.5853 - val_loss: 1.0666 - val_accuracy: 0.5979\n",
      "Epoch 86/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.0942 - accuracy: 0.5895\n",
      "Epoch 86: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0942 - accuracy: 0.5895 - val_loss: 1.0595 - val_accuracy: 0.6021\n",
      "Epoch 87/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0873 - accuracy: 0.5884\n",
      "Epoch 87: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0875 - accuracy: 0.5881 - val_loss: 1.1019 - val_accuracy: 0.5947\n",
      "Epoch 88/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0903 - accuracy: 0.5881\n",
      "Epoch 88: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0906 - accuracy: 0.5880 - val_loss: 1.1016 - val_accuracy: 0.5888\n",
      "Epoch 89/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0894 - accuracy: 0.5899\n",
      "Epoch 89: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0890 - accuracy: 0.5901 - val_loss: 1.1103 - val_accuracy: 0.5846\n",
      "Epoch 90/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.0844 - accuracy: 0.5901\n",
      "Epoch 90: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0844 - accuracy: 0.5901 - val_loss: 1.0698 - val_accuracy: 0.6031\n",
      "Epoch 91/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.0806 - accuracy: 0.5931\n",
      "Epoch 91: val_accuracy did not improve from 0.60432\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0806 - accuracy: 0.5931 - val_loss: 1.0866 - val_accuracy: 0.5991\n",
      "Epoch 92/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0807 - accuracy: 0.5952\n",
      "Epoch 92: val_accuracy improved from 0.60432 to 0.60502, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.0807 - accuracy: 0.5950 - val_loss: 1.0742 - val_accuracy: 0.6050\n",
      "Epoch 93/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0761 - accuracy: 0.5931\n",
      "Epoch 93: val_accuracy improved from 0.60502 to 0.60902, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.0766 - accuracy: 0.5931 - val_loss: 1.0641 - val_accuracy: 0.6090\n",
      "Epoch 94/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0725 - accuracy: 0.5945\n",
      "Epoch 94: val_accuracy did not improve from 0.60902\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0731 - accuracy: 0.5944 - val_loss: 1.0776 - val_accuracy: 0.6057\n",
      "Epoch 95/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0717 - accuracy: 0.5961\n",
      "Epoch 95: val_accuracy improved from 0.60902 to 0.61198, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0714 - accuracy: 0.5963 - val_loss: 1.0510 - val_accuracy: 0.6120\n",
      "Epoch 96/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0631 - accuracy: 0.5959\n",
      "Epoch 96: val_accuracy did not improve from 0.61198\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.0633 - accuracy: 0.5959 - val_loss: 1.0641 - val_accuracy: 0.6101\n",
      "Epoch 97/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0691 - accuracy: 0.5992\n",
      "Epoch 97: val_accuracy did not improve from 0.61198\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.0686 - accuracy: 0.5993 - val_loss: 1.0641 - val_accuracy: 0.6069\n",
      "Epoch 98/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0696 - accuracy: 0.5970\n",
      "Epoch 98: val_accuracy improved from 0.61198 to 0.61564, saving model to cnn_model.hdf5\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.0696 - accuracy: 0.5970 - val_loss: 1.0505 - val_accuracy: 0.6156\n",
      "Epoch 99/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0636 - accuracy: 0.5968\n",
      "Epoch 99: val_accuracy did not improve from 0.61564\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0636 - accuracy: 0.5968 - val_loss: 1.0538 - val_accuracy: 0.6134\n",
      "Epoch 100/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0661 - accuracy: 0.6014\n",
      "Epoch 100: val_accuracy did not improve from 0.61564\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.0662 - accuracy: 0.6014 - val_loss: 1.0482 - val_accuracy: 0.6142\n",
      "training time: 367.58370208740234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f4886fc9-3a34-4520-bab2-2ada4ff8415c/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://f4886fc9-3a34-4520-bab2-2ada4ff8415c/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_histo.npy has been saved!\n",
      "cnn.json has been saved!\n",
      "Mode saved!\n"
     ]
    }
   ],
   "source": [
    "model_type = \"cnn\"\n",
    "epochs = 100 \n",
    "batch_size = 100 \n",
    "fileName = \"cnn\"\n",
    "num_classes = 7\n",
    "width, height = 48, 48\n",
    "\n",
    "print('Start Training Model on '+ model_type+'...')\n",
    "\n",
    "#load data\n",
    "x = np.load('./dataX.npy')\n",
    "y = np.load('./dataY.npy')\n",
    "\n",
    "print('DataSize', x.shape, y.shape)\n",
    "\n",
    "\n",
    "xc = np.expand_dims(x,-1) \n",
    "n_values = np.max(y) + 1\n",
    "yc = np.eye(n_values)[y] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xc, yc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state= 40)\n",
    "\n",
    "#construct the training image generator for data augmentation\n",
    "data_generator = ImageDataGenerator(featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "model = custom_cnn()\n",
    "data_input = X_train\n",
    "val_data = (X_val, y_val)\n",
    "\n",
    "checkpoint = ModelCheckpoint(fileName+\"_model.hdf5\", monitor='val_accuracy', \n",
    "\tverbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=100,mode='max')\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "start = time.time()      \n",
    "        \n",
    "training_history = model.fit_generator(data_generator.flow(data_input,y_train,\n",
    "                batch_size=batch_size),\n",
    "                steps_per_epoch= len(y_train)/ batch_size,\n",
    "                epochs = epochs,\n",
    "                verbose = 1,\n",
    "                callbacks = callbacks_list,\n",
    "                validation_data = val_data,\n",
    "                shuffle = True\n",
    "    )\n",
    "print('training time:', time.time()-start)\n",
    "save_model(model, training_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training Model on scnn...\n",
      "DataSize (35887, 48, 48) (35887,)\n",
      "sift histogram exist!\n",
      "CNN layer created successfully...\n",
      "Fully-connected Layer created successfully...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dheeresh\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\Dheeresh\\AppData\\Local\\Temp\\ipykernel_22068\\2197094377.py:63: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  training_history = model.fit_generator(data_generator.flow(data_input,y_train,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/229 [==============================] - ETA: 0s - loss: 12.3392 - accuracy: 0.2254\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25740, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 9s 19ms/step - loss: 12.3392 - accuracy: 0.2254 - val_loss: 3.9662 - val_accuracy: 0.2574\n",
      "Epoch 2/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 2.4833 - accuracy: 0.2511\n",
      "Epoch 2: val_accuracy improved from 0.25740 to 0.25914, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 2.4780 - accuracy: 0.2512 - val_loss: 1.8807 - val_accuracy: 0.2591\n",
      "Epoch 3/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8217 - accuracy: 0.2590\n",
      "Epoch 3: val_accuracy improved from 0.25914 to 0.26402, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.8213 - accuracy: 0.2592 - val_loss: 1.7873 - val_accuracy: 0.2640\n",
      "Epoch 4/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.7912 - accuracy: 0.2566\n",
      "Epoch 4: val_accuracy improved from 0.26402 to 0.27238, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.7911 - accuracy: 0.2565 - val_loss: 1.7610 - val_accuracy: 0.2724\n",
      "Epoch 5/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.7713 - accuracy: 0.2743\n",
      "Epoch 5: val_accuracy improved from 0.27238 to 0.28910, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.7717 - accuracy: 0.2744 - val_loss: 1.7374 - val_accuracy: 0.2891\n",
      "Epoch 6/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.7510 - accuracy: 0.2833\n",
      "Epoch 6: val_accuracy improved from 0.28910 to 0.33612, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.7507 - accuracy: 0.2832 - val_loss: 1.6758 - val_accuracy: 0.3361\n",
      "Epoch 7/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.7113 - accuracy: 0.3108\n",
      "Epoch 7: val_accuracy improved from 0.33612 to 0.33978, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.7114 - accuracy: 0.3108 - val_loss: 1.6286 - val_accuracy: 0.3398\n",
      "Epoch 8/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.6731 - accuracy: 0.3339\n",
      "Epoch 8: val_accuracy improved from 0.33978 to 0.39568, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.6727 - accuracy: 0.3339 - val_loss: 1.5709 - val_accuracy: 0.3957\n",
      "Epoch 9/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.6391 - accuracy: 0.3528\n",
      "Epoch 9: val_accuracy did not improve from 0.39568\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.6389 - accuracy: 0.3528 - val_loss: 1.5400 - val_accuracy: 0.3945\n",
      "Epoch 10/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.6076 - accuracy: 0.3726\n",
      "Epoch 10: val_accuracy improved from 0.39568 to 0.42912, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.6075 - accuracy: 0.3726 - val_loss: 1.4890 - val_accuracy: 0.4291\n",
      "Epoch 11/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5760 - accuracy: 0.3876\n",
      "Epoch 11: val_accuracy improved from 0.42912 to 0.44026, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.5764 - accuracy: 0.3874 - val_loss: 1.4563 - val_accuracy: 0.4403\n",
      "Epoch 12/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5437 - accuracy: 0.4015\n",
      "Epoch 12: val_accuracy improved from 0.44026 to 0.46064, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.5428 - accuracy: 0.4017 - val_loss: 1.4162 - val_accuracy: 0.4606\n",
      "Epoch 13/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.5136 - accuracy: 0.4155\n",
      "Epoch 13: val_accuracy improved from 0.46064 to 0.47248, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.5139 - accuracy: 0.4154 - val_loss: 1.3979 - val_accuracy: 0.4725\n",
      "Epoch 14/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.5021 - accuracy: 0.4217\n",
      "Epoch 14: val_accuracy did not improve from 0.47248\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.5021 - accuracy: 0.4218 - val_loss: 1.3809 - val_accuracy: 0.4700\n",
      "Epoch 15/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4737 - accuracy: 0.4336\n",
      "Epoch 15: val_accuracy improved from 0.47248 to 0.48642, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.4733 - accuracy: 0.4336 - val_loss: 1.3593 - val_accuracy: 0.4864\n",
      "Epoch 16/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.4589 - accuracy: 0.4408\n",
      "Epoch 16: val_accuracy improved from 0.48642 to 0.50139, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.4589 - accuracy: 0.4408 - val_loss: 1.3272 - val_accuracy: 0.5014\n",
      "Epoch 17/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.4321 - accuracy: 0.4528\n",
      "Epoch 17: val_accuracy improved from 0.50139 to 0.50435, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.4337 - accuracy: 0.4523 - val_loss: 1.3143 - val_accuracy: 0.5044\n",
      "Epoch 18/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.4217 - accuracy: 0.4571\n",
      "Epoch 18: val_accuracy improved from 0.50435 to 0.51515, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.4217 - accuracy: 0.4571 - val_loss: 1.2858 - val_accuracy: 0.5152\n",
      "Epoch 19/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.4095 - accuracy: 0.4671\n",
      "Epoch 19: val_accuracy improved from 0.51515 to 0.51672, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.4087 - accuracy: 0.4675 - val_loss: 1.2799 - val_accuracy: 0.5167\n",
      "Epoch 20/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3933 - accuracy: 0.4687\n",
      "Epoch 20: val_accuracy improved from 0.51672 to 0.52247, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.3931 - accuracy: 0.4686 - val_loss: 1.2696 - val_accuracy: 0.5225\n",
      "Epoch 21/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3761 - accuracy: 0.4761\n",
      "Epoch 21: val_accuracy improved from 0.52247 to 0.53013, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.3764 - accuracy: 0.4758 - val_loss: 1.2429 - val_accuracy: 0.5301\n",
      "Epoch 22/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3663 - accuracy: 0.4836\n",
      "Epoch 22: val_accuracy improved from 0.53013 to 0.53239, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.3664 - accuracy: 0.4835 - val_loss: 1.2417 - val_accuracy: 0.5324\n",
      "Epoch 23/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3448 - accuracy: 0.4883\n",
      "Epoch 23: val_accuracy did not improve from 0.53239\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.3447 - accuracy: 0.4881 - val_loss: 1.2561 - val_accuracy: 0.5219\n",
      "Epoch 24/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.3393 - accuracy: 0.4918\n",
      "Epoch 24: val_accuracy did not improve from 0.53239\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.3395 - accuracy: 0.4918 - val_loss: 1.2243 - val_accuracy: 0.5324\n",
      "Epoch 25/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.3282 - accuracy: 0.4987\n",
      "Epoch 25: val_accuracy improved from 0.53239 to 0.54754, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.3280 - accuracy: 0.4988 - val_loss: 1.2021 - val_accuracy: 0.5475\n",
      "Epoch 26/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.3205 - accuracy: 0.4956\n",
      "Epoch 26: val_accuracy did not improve from 0.54754\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.3200 - accuracy: 0.4961 - val_loss: 1.2078 - val_accuracy: 0.5470\n",
      "Epoch 27/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.3097 - accuracy: 0.5000\n",
      "Epoch 27: val_accuracy improved from 0.54754 to 0.54963, saving model to scnn_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 4s 17ms/step - loss: 1.3096 - accuracy: 0.4999 - val_loss: 1.1934 - val_accuracy: 0.5496\n",
      "Epoch 28/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.3018 - accuracy: 0.5036\n",
      "Epoch 28: val_accuracy improved from 0.54963 to 0.55172, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.3014 - accuracy: 0.5040 - val_loss: 1.1887 - val_accuracy: 0.5517\n",
      "Epoch 29/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2921 - accuracy: 0.5131\n",
      "Epoch 29: val_accuracy did not improve from 0.55172\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2919 - accuracy: 0.5130 - val_loss: 1.1914 - val_accuracy: 0.5442\n",
      "Epoch 30/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2798 - accuracy: 0.5171\n",
      "Epoch 30: val_accuracy improved from 0.55172 to 0.56130, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2799 - accuracy: 0.5170 - val_loss: 1.1796 - val_accuracy: 0.5613\n",
      "Epoch 31/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.2782 - accuracy: 0.5139\n",
      "Epoch 31: val_accuracy improved from 0.56130 to 0.56688, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2789 - accuracy: 0.5135 - val_loss: 1.1617 - val_accuracy: 0.5669\n",
      "Epoch 32/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2709 - accuracy: 0.5193\n",
      "Epoch 32: val_accuracy did not improve from 0.56688\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2709 - accuracy: 0.5193 - val_loss: 1.1763 - val_accuracy: 0.5587\n",
      "Epoch 33/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2651 - accuracy: 0.5225\n",
      "Epoch 33: val_accuracy did not improve from 0.56688\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2651 - accuracy: 0.5225 - val_loss: 1.1743 - val_accuracy: 0.5561\n",
      "Epoch 34/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2491 - accuracy: 0.5271\n",
      "Epoch 34: val_accuracy did not improve from 0.56688\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.2492 - accuracy: 0.5274 - val_loss: 1.1502 - val_accuracy: 0.5664\n",
      "Epoch 35/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2444 - accuracy: 0.5293\n",
      "Epoch 35: val_accuracy improved from 0.56688 to 0.56809, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2455 - accuracy: 0.5286 - val_loss: 1.1437 - val_accuracy: 0.5681\n",
      "Epoch 36/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2435 - accuracy: 0.5272\n",
      "Epoch 36: val_accuracy improved from 0.56809 to 0.56949, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.2436 - accuracy: 0.5270 - val_loss: 1.1579 - val_accuracy: 0.5695\n",
      "Epoch 37/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2294 - accuracy: 0.5331\n",
      "Epoch 37: val_accuracy did not improve from 0.56949\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2295 - accuracy: 0.5330 - val_loss: 1.1502 - val_accuracy: 0.5686\n",
      "Epoch 38/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.2300 - accuracy: 0.5353\n",
      "Epoch 38: val_accuracy improved from 0.56949 to 0.57349, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.2300 - accuracy: 0.5353 - val_loss: 1.1284 - val_accuracy: 0.5735\n",
      "Epoch 39/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.2233 - accuracy: 0.5370\n",
      "Epoch 39: val_accuracy improved from 0.57349 to 0.57489, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.2231 - accuracy: 0.5369 - val_loss: 1.1297 - val_accuracy: 0.5749\n",
      "Epoch 40/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.2155 - accuracy: 0.5402\n",
      "Epoch 40: val_accuracy did not improve from 0.57489\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2152 - accuracy: 0.5404 - val_loss: 1.1225 - val_accuracy: 0.5724\n",
      "Epoch 41/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.2174 - accuracy: 0.5399\n",
      "Epoch 41: val_accuracy improved from 0.57489 to 0.57785, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.2184 - accuracy: 0.5397 - val_loss: 1.1319 - val_accuracy: 0.5778\n",
      "Epoch 42/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.2076 - accuracy: 0.5437\n",
      "Epoch 42: val_accuracy improved from 0.57785 to 0.58185, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.2076 - accuracy: 0.5437 - val_loss: 1.1141 - val_accuracy: 0.5819\n",
      "Epoch 43/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1999 - accuracy: 0.5457\n",
      "Epoch 43: val_accuracy did not improve from 0.58185\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1995 - accuracy: 0.5460 - val_loss: 1.1092 - val_accuracy: 0.5784\n",
      "Epoch 44/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1905 - accuracy: 0.5530\n",
      "Epoch 44: val_accuracy did not improve from 0.58185\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1913 - accuracy: 0.5527 - val_loss: 1.1258 - val_accuracy: 0.5775\n",
      "Epoch 45/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1889 - accuracy: 0.5497\n",
      "Epoch 45: val_accuracy did not improve from 0.58185\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1892 - accuracy: 0.5497 - val_loss: 1.1137 - val_accuracy: 0.5810\n",
      "Epoch 46/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1863 - accuracy: 0.5523\n",
      "Epoch 46: val_accuracy improved from 0.58185 to 0.58673, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1858 - accuracy: 0.5528 - val_loss: 1.1013 - val_accuracy: 0.5867\n",
      "Epoch 47/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1824 - accuracy: 0.5520\n",
      "Epoch 47: val_accuracy improved from 0.58673 to 0.58743, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.1821 - accuracy: 0.5517 - val_loss: 1.0944 - val_accuracy: 0.5874\n",
      "Epoch 48/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1765 - accuracy: 0.5560\n",
      "Epoch 48: val_accuracy improved from 0.58743 to 0.59248, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 18ms/step - loss: 1.1770 - accuracy: 0.5558 - val_loss: 1.0961 - val_accuracy: 0.5925\n",
      "Epoch 49/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1782 - accuracy: 0.5576\n",
      "Epoch 49: val_accuracy did not improve from 0.59248\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1776 - accuracy: 0.5579 - val_loss: 1.1169 - val_accuracy: 0.5782\n",
      "Epoch 50/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1690 - accuracy: 0.5609\n",
      "Epoch 50: val_accuracy did not improve from 0.59248\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1692 - accuracy: 0.5607 - val_loss: 1.1051 - val_accuracy: 0.5874\n",
      "Epoch 51/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1622 - accuracy: 0.5645\n",
      "Epoch 51: val_accuracy did not improve from 0.59248\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1623 - accuracy: 0.5642 - val_loss: 1.1029 - val_accuracy: 0.5869\n",
      "Epoch 52/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1569 - accuracy: 0.5605\n",
      "Epoch 52: val_accuracy did not improve from 0.59248\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1568 - accuracy: 0.5605 - val_loss: 1.0867 - val_accuracy: 0.5916\n",
      "Epoch 53/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1524 - accuracy: 0.5643\n",
      "Epoch 53: val_accuracy improved from 0.59248 to 0.59944, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1525 - accuracy: 0.5643 - val_loss: 1.0715 - val_accuracy: 0.5994\n",
      "Epoch 54/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1520 - accuracy: 0.5668\n",
      "Epoch 54: val_accuracy did not improve from 0.59944\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1518 - accuracy: 0.5670 - val_loss: 1.0767 - val_accuracy: 0.5982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1417 - accuracy: 0.5732\n",
      "Epoch 55: val_accuracy did not improve from 0.59944\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1413 - accuracy: 0.5734 - val_loss: 1.0946 - val_accuracy: 0.5886\n",
      "Epoch 56/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1414 - accuracy: 0.5714\n",
      "Epoch 56: val_accuracy improved from 0.59944 to 0.60014, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1415 - accuracy: 0.5713 - val_loss: 1.0764 - val_accuracy: 0.6001\n",
      "Epoch 57/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1458 - accuracy: 0.5673\n",
      "Epoch 57: val_accuracy improved from 0.60014 to 0.60188, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1449 - accuracy: 0.5679 - val_loss: 1.0697 - val_accuracy: 0.6019\n",
      "Epoch 58/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1405 - accuracy: 0.5718\n",
      "Epoch 58: val_accuracy did not improve from 0.60188\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1394 - accuracy: 0.5722 - val_loss: 1.0851 - val_accuracy: 0.5940\n",
      "Epoch 59/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1300 - accuracy: 0.5752\n",
      "Epoch 59: val_accuracy improved from 0.60188 to 0.60432, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1302 - accuracy: 0.5753 - val_loss: 1.0765 - val_accuracy: 0.6043\n",
      "Epoch 60/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1341 - accuracy: 0.5756\n",
      "Epoch 60: val_accuracy improved from 0.60432 to 0.60519, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1336 - accuracy: 0.5756 - val_loss: 1.0689 - val_accuracy: 0.6052\n",
      "Epoch 61/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1254 - accuracy: 0.5763\n",
      "Epoch 61: val_accuracy did not improve from 0.60519\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1260 - accuracy: 0.5760 - val_loss: 1.0721 - val_accuracy: 0.6040\n",
      "Epoch 62/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1163 - accuracy: 0.5836\n",
      "Epoch 62: val_accuracy improved from 0.60519 to 0.60954, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1163 - accuracy: 0.5837 - val_loss: 1.0539 - val_accuracy: 0.6095\n",
      "Epoch 63/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.1217 - accuracy: 0.5776\n",
      "Epoch 63: val_accuracy did not improve from 0.60954\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1217 - accuracy: 0.5776 - val_loss: 1.0629 - val_accuracy: 0.6014\n",
      "Epoch 64/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1171 - accuracy: 0.5785\n",
      "Epoch 64: val_accuracy did not improve from 0.60954\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1170 - accuracy: 0.5787 - val_loss: 1.0744 - val_accuracy: 0.5984\n",
      "Epoch 65/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1147 - accuracy: 0.5799\n",
      "Epoch 65: val_accuracy did not improve from 0.60954\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1139 - accuracy: 0.5804 - val_loss: 1.0523 - val_accuracy: 0.6094\n",
      "Epoch 66/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1149 - accuracy: 0.5831\n",
      "Epoch 66: val_accuracy improved from 0.60954 to 0.60989, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.1143 - accuracy: 0.5836 - val_loss: 1.0473 - val_accuracy: 0.6099\n",
      "Epoch 67/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.5874\n",
      "Epoch 67: val_accuracy did not improve from 0.60989\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0989 - accuracy: 0.5873 - val_loss: 1.0467 - val_accuracy: 0.6099\n",
      "Epoch 68/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.1087 - accuracy: 0.5806\n",
      "Epoch 68: val_accuracy did not improve from 0.60989\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1080 - accuracy: 0.5809 - val_loss: 1.0558 - val_accuracy: 0.6069\n",
      "Epoch 69/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.1005 - accuracy: 0.5854\n",
      "Epoch 69: val_accuracy did not improve from 0.60989\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.1002 - accuracy: 0.5854 - val_loss: 1.0679 - val_accuracy: 0.6092\n",
      "Epoch 70/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0896 - accuracy: 0.5905\n",
      "Epoch 70: val_accuracy did not improve from 0.60989\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0900 - accuracy: 0.5902 - val_loss: 1.0627 - val_accuracy: 0.6088\n",
      "Epoch 71/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0868 - accuracy: 0.5932\n",
      "Epoch 71: val_accuracy improved from 0.60989 to 0.61442, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0862 - accuracy: 0.5934 - val_loss: 1.0420 - val_accuracy: 0.6144\n",
      "Epoch 72/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.0949 - accuracy: 0.5879\n",
      "Epoch 72: val_accuracy did not improve from 0.61442\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0961 - accuracy: 0.5878 - val_loss: 1.0450 - val_accuracy: 0.6095\n",
      "Epoch 73/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0907 - accuracy: 0.5902\n",
      "Epoch 73: val_accuracy did not improve from 0.61442\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0900 - accuracy: 0.5905 - val_loss: 1.0377 - val_accuracy: 0.6113\n",
      "Epoch 74/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.0870 - accuracy: 0.5941\n",
      "Epoch 74: val_accuracy improved from 0.61442 to 0.61773, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0871 - accuracy: 0.5937 - val_loss: 1.0463 - val_accuracy: 0.6177\n",
      "Epoch 75/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0838 - accuracy: 0.5955\n",
      "Epoch 75: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0833 - accuracy: 0.5958 - val_loss: 1.0462 - val_accuracy: 0.6057\n",
      "Epoch 76/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0795 - accuracy: 0.5971\n",
      "Epoch 76: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0800 - accuracy: 0.5966 - val_loss: 1.0279 - val_accuracy: 0.6165\n",
      "Epoch 77/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.0747 - accuracy: 0.5955\n",
      "Epoch 77: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0748 - accuracy: 0.5955 - val_loss: 1.0503 - val_accuracy: 0.6033\n",
      "Epoch 78/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.0760 - accuracy: 0.5985\n",
      "Epoch 78: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0763 - accuracy: 0.5982 - val_loss: 1.0514 - val_accuracy: 0.6062\n",
      "Epoch 79/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0747 - accuracy: 0.5962\n",
      "Epoch 79: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0753 - accuracy: 0.5960 - val_loss: 1.0268 - val_accuracy: 0.6155\n",
      "Epoch 80/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0701 - accuracy: 0.5999\n",
      "Epoch 80: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0695 - accuracy: 0.6000 - val_loss: 1.0302 - val_accuracy: 0.6134\n",
      "Epoch 81/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0635 - accuracy: 0.6016\n",
      "Epoch 81: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0634 - accuracy: 0.6013 - val_loss: 1.0356 - val_accuracy: 0.6130\n",
      "Epoch 82/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.0627 - accuracy: 0.6000\n",
      "Epoch 82: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0634 - accuracy: 0.5999 - val_loss: 1.0348 - val_accuracy: 0.6120\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [============================>.] - ETA: 0s - loss: 1.0627 - accuracy: 0.5997\n",
      "Epoch 83: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0630 - accuracy: 0.5999 - val_loss: 1.0308 - val_accuracy: 0.6160\n",
      "Epoch 84/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0620 - accuracy: 0.6010\n",
      "Epoch 84: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0621 - accuracy: 0.6007 - val_loss: 1.0269 - val_accuracy: 0.6167\n",
      "Epoch 85/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0491 - accuracy: 0.6073\n",
      "Epoch 85: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0491 - accuracy: 0.6074 - val_loss: 1.0343 - val_accuracy: 0.6169\n",
      "Epoch 86/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0545 - accuracy: 0.6019\n",
      "Epoch 86: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0542 - accuracy: 0.6020 - val_loss: 1.0320 - val_accuracy: 0.6087\n",
      "Epoch 87/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0498 - accuracy: 0.6093\n",
      "Epoch 87: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0501 - accuracy: 0.6091 - val_loss: 1.0408 - val_accuracy: 0.6073\n",
      "Epoch 88/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0463 - accuracy: 0.6076\n",
      "Epoch 88: val_accuracy did not improve from 0.61773\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0461 - accuracy: 0.6077 - val_loss: 1.0425 - val_accuracy: 0.6122\n",
      "Epoch 89/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.0469 - accuracy: 0.6049\n",
      "Epoch 89: val_accuracy improved from 0.61773 to 0.61877, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0458 - accuracy: 0.6055 - val_loss: 1.0180 - val_accuracy: 0.6188\n",
      "Epoch 90/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0452 - accuracy: 0.6052\n",
      "Epoch 90: val_accuracy did not improve from 0.61877\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0449 - accuracy: 0.6054 - val_loss: 1.0349 - val_accuracy: 0.6183\n",
      "Epoch 91/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0456 - accuracy: 0.6090\n",
      "Epoch 91: val_accuracy did not improve from 0.61877\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0450 - accuracy: 0.6093 - val_loss: 1.0229 - val_accuracy: 0.6118\n",
      "Epoch 92/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0327 - accuracy: 0.6105\n",
      "Epoch 92: val_accuracy did not improve from 0.61877\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0330 - accuracy: 0.6103 - val_loss: 1.0301 - val_accuracy: 0.6188\n",
      "Epoch 93/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0348 - accuracy: 0.6118\n",
      "Epoch 93: val_accuracy did not improve from 0.61877\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0353 - accuracy: 0.6116 - val_loss: 1.0252 - val_accuracy: 0.6174\n",
      "Epoch 94/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.0317 - accuracy: 0.6145\n",
      "Epoch 94: val_accuracy did not improve from 0.61877\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0313 - accuracy: 0.6146 - val_loss: 1.0224 - val_accuracy: 0.6158\n",
      "Epoch 95/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.6132\n",
      "Epoch 95: val_accuracy improved from 0.61877 to 0.62226, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0331 - accuracy: 0.6138 - val_loss: 1.0150 - val_accuracy: 0.6223\n",
      "Epoch 96/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.0308 - accuracy: 0.6124\n",
      "Epoch 96: val_accuracy did not improve from 0.62226\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0316 - accuracy: 0.6122 - val_loss: 1.0343 - val_accuracy: 0.6113\n",
      "Epoch 97/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0346 - accuracy: 0.6162\n",
      "Epoch 97: val_accuracy did not improve from 0.62226\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0348 - accuracy: 0.6161 - val_loss: 1.0220 - val_accuracy: 0.6177\n",
      "Epoch 98/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0209 - accuracy: 0.6181\n",
      "Epoch 98: val_accuracy did not improve from 0.62226\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0205 - accuracy: 0.6182 - val_loss: 1.0361 - val_accuracy: 0.6113\n",
      "Epoch 99/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0287 - accuracy: 0.6128\n",
      "Epoch 99: val_accuracy improved from 0.62226 to 0.62574, saving model to scnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.0280 - accuracy: 0.6130 - val_loss: 1.0119 - val_accuracy: 0.6257\n",
      "Epoch 100/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.0251 - accuracy: 0.6148\n",
      "Epoch 100: val_accuracy did not improve from 0.62574\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.0259 - accuracy: 0.6142 - val_loss: 1.0154 - val_accuracy: 0.6129\n",
      "training time: 394.38303327560425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0805d8db-e22c-4ece-b634-e844ab4852a0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0805d8db-e22c-4ece-b634-e844ab4852a0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scnn_histo.npy has been saved!\n",
      "scnn.json has been saved!\n",
      "Mode saved!\n"
     ]
    }
   ],
   "source": [
    "model_type = \"scnn\" \n",
    "epochs = 100 \n",
    "batch_size = 100 \n",
    "fileName = \"scnn\"\n",
    "num_classes = 7\n",
    "width, height = 48, 48\n",
    "\n",
    "print('Start Training Model on '+ model_type+'...')\n",
    "\n",
    "#load data\n",
    "x = np.load('./dataX.npy')\n",
    "y = np.load('./dataY.npy')\n",
    "\n",
    "print('DataSize', x.shape, y.shape) \n",
    "\n",
    "\n",
    "xc = np.expand_dims(x,-1) \n",
    "n_values = np.max(y) + 1\n",
    "yc = np.eye(n_values)[y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xc, yc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state= 40)\n",
    "\n",
    "#construct the training image generator for data augmentation\n",
    "data_generator = ImageDataGenerator(featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "X_sift = load_sift()\n",
    "X_sift_train, X_sift_val, X_sift_test = SIFT_data_split(X_sift)\n",
    "\n",
    "sift = Sift_layer()\n",
    "CNN = cnn_layer()\n",
    "\n",
    "MergeModel = concatenate([sift.output, CNN.output])\n",
    "fc = FC_layer(MergeModel)\n",
    "model = Model(inputs=[CNN.input, sift.input], outputs = fc)\n",
    "\n",
    "data_input = [X_train,X_sift_train]\n",
    "val_data = ([X_val,X_sift_val],y_val)\n",
    "\n",
    "checkpoint = ModelCheckpoint(fileName+\"_model.hdf5\", monitor='val_accuracy', \n",
    "    verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=100,mode='max')\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "start = time.time()      \n",
    "        \n",
    "training_history = model.fit_generator(data_generator.flow(data_input,y_train,\n",
    "                batch_size=batch_size),\n",
    "                steps_per_epoch= len(y_train)/ batch_size,\n",
    "                epochs = epochs,\n",
    "                verbose = 1,\n",
    "                callbacks = callbacks_list,\n",
    "                validation_data = val_data,\n",
    "                shuffle = True\n",
    "    )\n",
    "print('training time:', time.time()-start)\n",
    "save_model(model, training_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training Model on dcnn...\n",
      "DataSize (35887, 48, 48) (35887,)\n",
      "dsift descriptors exist!\n",
      "CNN layer created successfully...\n",
      "Fully-connected Layer created successfully...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dheeresh\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "C:\\Users\\Dheeresh\\AppData\\Local\\Temp\\ipykernel_22068\\1531439665.py:63: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  training_history = model.fit_generator(data_generator.flow(data_input,y_train,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230/229 [==============================] - ETA: 0s - loss: 37.6250 - accuracy: 0.2088\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25601, saving model to dcnn_model.hdf5\n",
      "229/229 [==============================] - 5s 17ms/step - loss: 37.6250 - accuracy: 0.2088 - val_loss: 22.6466 - val_accuracy: 0.2560\n",
      "Epoch 2/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 21.9667 - accuracy: 0.2423\n",
      "Epoch 2: val_accuracy improved from 0.25601 to 0.25670, saving model to dcnn_model.hdf5\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 21.9642 - accuracy: 0.2422 - val_loss: 21.2555 - val_accuracy: 0.2567\n",
      "Epoch 3/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 20.8031 - accuracy: 0.2483\n",
      "Epoch 3: val_accuracy did not improve from 0.25670\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 20.8003 - accuracy: 0.2486 - val_loss: 20.1859 - val_accuracy: 0.2565\n",
      "Epoch 4/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 19.7304 - accuracy: 0.2503\n",
      "Epoch 4: val_accuracy did not improve from 0.25670\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 19.7281 - accuracy: 0.2507 - val_loss: 19.1335 - val_accuracy: 0.2560\n",
      "Epoch 5/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 18.6643 - accuracy: 0.2492\n",
      "Epoch 5: val_accuracy improved from 0.25670 to 0.25758, saving model to dcnn_model.hdf5\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 18.6621 - accuracy: 0.2494 - val_loss: 18.0871 - val_accuracy: 0.2576\n",
      "Epoch 6/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 17.6044 - accuracy: 0.2503\n",
      "Epoch 6: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 17.6021 - accuracy: 0.2504 - val_loss: 17.0280 - val_accuracy: 0.2562\n",
      "Epoch 7/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 16.5604 - accuracy: 0.2511\n",
      "Epoch 7: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 16.5582 - accuracy: 0.2510 - val_loss: 15.9935 - val_accuracy: 0.2560\n",
      "Epoch 8/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 15.5437 - accuracy: 0.2512\n",
      "Epoch 8: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 15.5414 - accuracy: 0.2513 - val_loss: 14.9806 - val_accuracy: 0.2560\n",
      "Epoch 9/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 14.5594 - accuracy: 0.2509\n",
      "Epoch 9: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 14.5552 - accuracy: 0.2509 - val_loss: 14.0230 - val_accuracy: 0.2560\n",
      "Epoch 10/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 13.6006 - accuracy: 0.2509\n",
      "Epoch 10: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 13.5987 - accuracy: 0.2509 - val_loss: 13.0722 - val_accuracy: 0.2562\n",
      "Epoch 11/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 12.6697 - accuracy: 0.2499\n",
      "Epoch 11: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 12.6678 - accuracy: 0.2498 - val_loss: 12.1652 - val_accuracy: 0.2562\n",
      "Epoch 12/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 11.7765 - accuracy: 0.2490\n",
      "Epoch 12: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 11.7732 - accuracy: 0.2488 - val_loss: 11.2793 - val_accuracy: 0.2562\n",
      "Epoch 13/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 10.9127 - accuracy: 0.2494\n",
      "Epoch 13: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 10.9108 - accuracy: 0.2494 - val_loss: 10.4240 - val_accuracy: 0.2562\n",
      "Epoch 14/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 10.0833 - accuracy: 0.2495\n",
      "Epoch 14: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 10.0817 - accuracy: 0.2493 - val_loss: 9.6257 - val_accuracy: 0.2564\n",
      "Epoch 15/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 9.2809 - accuracy: 0.2472\n",
      "Epoch 15: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 9.2788 - accuracy: 0.2476 - val_loss: 8.8328 - val_accuracy: 0.2562\n",
      "Epoch 16/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 8.5117 - accuracy: 0.2479\n",
      "Epoch 16: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 8.5101 - accuracy: 0.2477 - val_loss: 8.0861 - val_accuracy: 0.2565\n",
      "Epoch 17/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 7.7755 - accuracy: 0.2495\n",
      "Epoch 17: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 7.7740 - accuracy: 0.2497 - val_loss: 7.3595 - val_accuracy: 0.2562\n",
      "Epoch 18/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 7.0773 - accuracy: 0.2496\n",
      "Epoch 18: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 7.0758 - accuracy: 0.2494 - val_loss: 6.6965 - val_accuracy: 0.2564\n",
      "Epoch 19/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 6.4340 - accuracy: 0.2493\n",
      "Epoch 19: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 6.4327 - accuracy: 0.2494 - val_loss: 6.0526 - val_accuracy: 0.2562\n",
      "Epoch 20/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 5.8155 - accuracy: 0.2478\n",
      "Epoch 20: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 5.8155 - accuracy: 0.2478 - val_loss: 5.4748 - val_accuracy: 0.2564\n",
      "Epoch 21/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 5.2462 - accuracy: 0.2490\n",
      "Epoch 21: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 5.2449 - accuracy: 0.2490 - val_loss: 4.9243 - val_accuracy: 0.2562\n",
      "Epoch 22/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 4.8133 - accuracy: 0.2452\n",
      "Epoch 22: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 4.8123 - accuracy: 0.2451 - val_loss: 4.5415 - val_accuracy: 0.2562\n",
      "Epoch 23/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 4.3460 - accuracy: 0.2503\n",
      "Epoch 23: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 4.3450 - accuracy: 0.2503 - val_loss: 4.1132 - val_accuracy: 0.2562\n",
      "Epoch 24/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 3.9387 - accuracy: 0.2502\n",
      "Epoch 24: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 3.9370 - accuracy: 0.2501 - val_loss: 3.7026 - val_accuracy: 0.2562\n",
      "Epoch 25/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 3.5657 - accuracy: 0.2503\n",
      "Epoch 25: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 3.5650 - accuracy: 0.2502 - val_loss: 3.3440 - val_accuracy: 0.2562\n",
      "Epoch 26/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 3.2649 - accuracy: 0.2485\n",
      "Epoch 26: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 3.2642 - accuracy: 0.2486 - val_loss: 3.0774 - val_accuracy: 0.2562\n",
      "Epoch 27/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 3.0263 - accuracy: 0.2493\n",
      "Epoch 27: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 3.0250 - accuracy: 0.2493 - val_loss: 2.8573 - val_accuracy: 0.2562\n",
      "Epoch 28/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.8153 - accuracy: 0.2490\n",
      "Epoch 28: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.8153 - accuracy: 0.2491 - val_loss: 2.6818 - val_accuracy: 0.2562\n",
      "Epoch 29/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.6555 - accuracy: 0.2504\n",
      "Epoch 29: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.6554 - accuracy: 0.2501 - val_loss: 2.5018 - val_accuracy: 0.2562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.4642 - accuracy: 0.2506\n",
      "Epoch 30: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.4641 - accuracy: 0.2504 - val_loss: 2.3745 - val_accuracy: 0.2569\n",
      "Epoch 31/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 2.4184 - accuracy: 0.2475\n",
      "Epoch 31: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.4176 - accuracy: 0.2477 - val_loss: 2.3328 - val_accuracy: 0.2569\n",
      "Epoch 32/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 2.3035 - accuracy: 0.2492\n",
      "Epoch 32: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.3023 - accuracy: 0.2497 - val_loss: 2.1919 - val_accuracy: 0.2569\n",
      "Epoch 33/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.1769 - accuracy: 0.2506\n",
      "Epoch 33: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.1767 - accuracy: 0.2505 - val_loss: 2.1082 - val_accuracy: 0.2569\n",
      "Epoch 34/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0791 - accuracy: 0.2506\n",
      "Epoch 34: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0788 - accuracy: 0.2504 - val_loss: 2.0110 - val_accuracy: 0.2569\n",
      "Epoch 35/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0054 - accuracy: 0.2503\n",
      "Epoch 35: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0050 - accuracy: 0.2503 - val_loss: 1.9448 - val_accuracy: 0.2569\n",
      "Epoch 36/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9488 - accuracy: 0.2506\n",
      "Epoch 36: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9489 - accuracy: 0.2504 - val_loss: 1.8996 - val_accuracy: 0.2569\n",
      "Epoch 37/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.1448 - accuracy: 0.2473\n",
      "Epoch 37: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.1448 - accuracy: 0.2475 - val_loss: 2.1306 - val_accuracy: 0.2569\n",
      "Epoch 38/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 2.0474 - accuracy: 0.2504\n",
      "Epoch 38: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0471 - accuracy: 0.2505 - val_loss: 1.9814 - val_accuracy: 0.2569\n",
      "Epoch 39/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9577 - accuracy: 0.2508\n",
      "Epoch 39: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9576 - accuracy: 0.2505 - val_loss: 1.8938 - val_accuracy: 0.2569\n",
      "Epoch 40/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9048 - accuracy: 0.2504\n",
      "Epoch 40: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9047 - accuracy: 0.2504 - val_loss: 1.8615 - val_accuracy: 0.2569\n",
      "Epoch 41/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8721 - accuracy: 0.2507\n",
      "Epoch 41: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8722 - accuracy: 0.2505 - val_loss: 1.8368 - val_accuracy: 0.2569\n",
      "Epoch 42/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8577 - accuracy: 0.2503\n",
      "Epoch 42: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8574 - accuracy: 0.2505 - val_loss: 1.8396 - val_accuracy: 0.2569\n",
      "Epoch 43/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0719 - accuracy: 0.2495\n",
      "Epoch 43: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 2.0717 - accuracy: 0.2494 - val_loss: 2.0079 - val_accuracy: 0.2569\n",
      "Epoch 44/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9867 - accuracy: 0.2502\n",
      "Epoch 44: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9865 - accuracy: 0.2503 - val_loss: 1.9055 - val_accuracy: 0.2569\n",
      "Epoch 45/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9515 - accuracy: 0.2504\n",
      "Epoch 45: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9516 - accuracy: 0.2501 - val_loss: 1.9175 - val_accuracy: 0.2569\n",
      "Epoch 46/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.9028 - accuracy: 0.2509\n",
      "Epoch 46: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9029 - accuracy: 0.2504 - val_loss: 1.9049 - val_accuracy: 0.2569\n",
      "Epoch 47/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8911 - accuracy: 0.2500\n",
      "Epoch 47: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8912 - accuracy: 0.2501 - val_loss: 1.9757 - val_accuracy: 0.2569\n",
      "Epoch 48/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0354 - accuracy: 0.2494\n",
      "Epoch 48: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0349 - accuracy: 0.2497 - val_loss: 1.9654 - val_accuracy: 0.2569\n",
      "Epoch 49/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9311 - accuracy: 0.2506\n",
      "Epoch 49: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9310 - accuracy: 0.2506 - val_loss: 1.8799 - val_accuracy: 0.2569\n",
      "Epoch 50/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8778 - accuracy: 0.2504\n",
      "Epoch 50: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8775 - accuracy: 0.2505 - val_loss: 1.8434 - val_accuracy: 0.2569\n",
      "Epoch 51/100\n",
      "227/229 [============================>.] - ETA: 0s - loss: 1.8451 - accuracy: 0.2504\n",
      "Epoch 51: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8447 - accuracy: 0.2506 - val_loss: 1.8129 - val_accuracy: 0.2569\n",
      "Epoch 52/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9860 - accuracy: 0.2496\n",
      "Epoch 52: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.9859 - accuracy: 0.2496 - val_loss: 2.0059 - val_accuracy: 0.2569\n",
      "Epoch 53/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9662 - accuracy: 0.2506\n",
      "Epoch 53: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.9657 - accuracy: 0.2506 - val_loss: 1.8881 - val_accuracy: 0.2569\n",
      "Epoch 54/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8871 - accuracy: 0.2506\n",
      "Epoch 54: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8871 - accuracy: 0.2506 - val_loss: 1.8440 - val_accuracy: 0.2569\n",
      "Epoch 55/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8497 - accuracy: 0.2504\n",
      "Epoch 55: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8497 - accuracy: 0.2504 - val_loss: 1.8116 - val_accuracy: 0.2569\n",
      "Epoch 56/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.9030 - accuracy: 0.2502\n",
      "Epoch 56: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9031 - accuracy: 0.2499 - val_loss: 1.8498 - val_accuracy: 0.2569\n",
      "Epoch 57/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8608 - accuracy: 0.2504\n",
      "Epoch 57: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8607 - accuracy: 0.2504 - val_loss: 1.8314 - val_accuracy: 0.2569\n",
      "Epoch 58/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8701 - accuracy: 0.2504\n",
      "Epoch 58: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8704 - accuracy: 0.2504 - val_loss: 1.8566 - val_accuracy: 0.2569\n",
      "Epoch 59/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8538 - accuracy: 0.2504\n",
      "Epoch 59: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8545 - accuracy: 0.2504 - val_loss: 1.8625 - val_accuracy: 0.2569\n",
      "Epoch 60/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8733 - accuracy: 0.2504\n",
      "Epoch 60: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8731 - accuracy: 0.2503 - val_loss: 1.8326 - val_accuracy: 0.2569\n",
      "Epoch 61/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0002 - accuracy: 0.2493\n",
      "Epoch 61: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0005 - accuracy: 0.2494 - val_loss: 2.0785 - val_accuracy: 0.2569\n",
      "Epoch 62/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 2.0035 - accuracy: 0.2504\n",
      "Epoch 62: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0027 - accuracy: 0.2504 - val_loss: 1.9063 - val_accuracy: 0.2569\n",
      "Epoch 63/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8871 - accuracy: 0.2505\n",
      "Epoch 63: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8871 - accuracy: 0.2504 - val_loss: 1.8324 - val_accuracy: 0.2569\n",
      "Epoch 64/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.8560 - accuracy: 0.2496\n",
      "Epoch 64: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8561 - accuracy: 0.2501 - val_loss: 1.8441 - val_accuracy: 0.2569\n",
      "Epoch 65/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8758 - accuracy: 0.2503\n",
      "Epoch 65: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8755 - accuracy: 0.2506 - val_loss: 1.8021 - val_accuracy: 0.2569\n",
      "Epoch 66/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.8358 - accuracy: 0.2506\n",
      "Epoch 66: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8359 - accuracy: 0.2505 - val_loss: 1.8123 - val_accuracy: 0.2569\n",
      "Epoch 67/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9341 - accuracy: 0.2497\n",
      "Epoch 67: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9339 - accuracy: 0.2498 - val_loss: 1.8669 - val_accuracy: 0.2569\n",
      "Epoch 68/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9548 - accuracy: 0.2499\n",
      "Epoch 68: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9552 - accuracy: 0.2499 - val_loss: 1.9378 - val_accuracy: 0.2569\n",
      "Epoch 69/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9196 - accuracy: 0.2505\n",
      "Epoch 69: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9192 - accuracy: 0.2505 - val_loss: 1.8473 - val_accuracy: 0.2569\n",
      "Epoch 70/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.9617 - accuracy: 0.2503\n",
      "Epoch 70: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9614 - accuracy: 0.2503 - val_loss: 1.9224 - val_accuracy: 0.2569\n",
      "Epoch 71/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.9024 - accuracy: 0.2503\n",
      "Epoch 71: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9016 - accuracy: 0.2504 - val_loss: 1.8418 - val_accuracy: 0.2569\n",
      "Epoch 72/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8498 - accuracy: 0.2503\n",
      "Epoch 72: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8498 - accuracy: 0.2504 - val_loss: 1.8021 - val_accuracy: 0.2569\n",
      "Epoch 73/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8649 - accuracy: 0.2495\n",
      "Epoch 73: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8649 - accuracy: 0.2495 - val_loss: 1.9487 - val_accuracy: 0.2569\n",
      "Epoch 74/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9055 - accuracy: 0.2504\n",
      "Epoch 74: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9051 - accuracy: 0.2506 - val_loss: 1.8152 - val_accuracy: 0.2569\n",
      "Epoch 75/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8341 - accuracy: 0.2504\n",
      "Epoch 75: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8340 - accuracy: 0.2504 - val_loss: 1.7653 - val_accuracy: 0.2569\n",
      "Epoch 76/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8625 - accuracy: 0.2501\n",
      "Epoch 76: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8625 - accuracy: 0.2502 - val_loss: 1.8674 - val_accuracy: 0.2569\n",
      "Epoch 77/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8231 - accuracy: 0.2507\n",
      "Epoch 77: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8232 - accuracy: 0.2504 - val_loss: 1.7801 - val_accuracy: 0.2569\n",
      "Epoch 78/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8014 - accuracy: 0.2504\n",
      "Epoch 78: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8011 - accuracy: 0.2505 - val_loss: 1.7448 - val_accuracy: 0.2569\n",
      "Epoch 79/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9581 - accuracy: 0.2494\n",
      "Epoch 79: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9589 - accuracy: 0.2493 - val_loss: 2.0839 - val_accuracy: 0.2569\n",
      "Epoch 80/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 2.0006 - accuracy: 0.2504\n",
      "Epoch 80: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0002 - accuracy: 0.2505 - val_loss: 1.9336 - val_accuracy: 0.2569\n",
      "Epoch 81/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9574 - accuracy: 0.2500\n",
      "Epoch 81: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.9572 - accuracy: 0.2503 - val_loss: 1.9150 - val_accuracy: 0.2569\n",
      "Epoch 82/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8710 - accuracy: 0.2505\n",
      "Epoch 82: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8711 - accuracy: 0.2504 - val_loss: 1.8469 - val_accuracy: 0.2569\n",
      "Epoch 83/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8673 - accuracy: 0.2501\n",
      "Epoch 83: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8670 - accuracy: 0.2501 - val_loss: 1.8200 - val_accuracy: 0.2569\n",
      "Epoch 84/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9441 - accuracy: 0.2504\n",
      "Epoch 84: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9445 - accuracy: 0.2504 - val_loss: 2.0049 - val_accuracy: 0.2562\n",
      "Epoch 85/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.9481 - accuracy: 0.2505\n",
      "Epoch 85: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.9480 - accuracy: 0.2505 - val_loss: 1.8731 - val_accuracy: 0.2569\n",
      "Epoch 86/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8617 - accuracy: 0.2505\n",
      "Epoch 86: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8619 - accuracy: 0.2505 - val_loss: 1.8277 - val_accuracy: 0.2569\n",
      "Epoch 87/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8433 - accuracy: 0.2506\n",
      "Epoch 87: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8433 - accuracy: 0.2506 - val_loss: 1.7439 - val_accuracy: 0.2569\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/229 [============================>.] - ETA: 0s - loss: 1.8111 - accuracy: 0.2501\n",
      "Epoch 88: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8106 - accuracy: 0.2504 - val_loss: 1.7242 - val_accuracy: 0.2569\n",
      "Epoch 89/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8713 - accuracy: 0.2502\n",
      "Epoch 89: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 3s 15ms/step - loss: 1.8713 - accuracy: 0.2502 - val_loss: 1.7580 - val_accuracy: 0.2569\n",
      "Epoch 90/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8337 - accuracy: 0.2503\n",
      "Epoch 90: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8332 - accuracy: 0.2504 - val_loss: 1.7677 - val_accuracy: 0.2569\n",
      "Epoch 91/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8739 - accuracy: 0.2505\n",
      "Epoch 91: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8740 - accuracy: 0.2503 - val_loss: 1.7916 - val_accuracy: 0.2569\n",
      "Epoch 92/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8323 - accuracy: 0.2507\n",
      "Epoch 92: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8323 - accuracy: 0.2505 - val_loss: 1.7363 - val_accuracy: 0.2569\n",
      "Epoch 93/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8613 - accuracy: 0.2501\n",
      "Epoch 93: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8626 - accuracy: 0.2499 - val_loss: 2.1425 - val_accuracy: 0.2562\n",
      "Epoch 94/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 2.0355 - accuracy: 0.2505\n",
      "Epoch 94: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 2.0342 - accuracy: 0.2504 - val_loss: 1.8592 - val_accuracy: 0.2569\n",
      "Epoch 95/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.8973 - accuracy: 0.2504\n",
      "Epoch 95: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8970 - accuracy: 0.2502 - val_loss: 1.7963 - val_accuracy: 0.2569\n",
      "Epoch 96/100\n",
      "230/229 [==============================] - ETA: 0s - loss: 1.8401 - accuracy: 0.2504\n",
      "Epoch 96: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.8401 - accuracy: 0.2504 - val_loss: 1.8558 - val_accuracy: 0.2569\n",
      "Epoch 97/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8746 - accuracy: 0.2501\n",
      "Epoch 97: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.8744 - accuracy: 0.2501 - val_loss: 1.7485 - val_accuracy: 0.2569\n",
      "Epoch 98/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8259 - accuracy: 0.2506\n",
      "Epoch 98: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 17ms/step - loss: 1.8260 - accuracy: 0.2505 - val_loss: 1.7506 - val_accuracy: 0.2569\n",
      "Epoch 99/100\n",
      "228/229 [============================>.] - ETA: 0s - loss: 1.7928 - accuracy: 0.2500\n",
      "Epoch 99: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 16ms/step - loss: 1.7920 - accuracy: 0.2501 - val_loss: 1.6955 - val_accuracy: 0.2569\n",
      "Epoch 100/100\n",
      "229/229 [============================>.] - ETA: 0s - loss: 1.8410 - accuracy: 0.2503\n",
      "Epoch 100: val_accuracy did not improve from 0.25758\n",
      "229/229 [==============================] - 4s 15ms/step - loss: 1.8412 - accuracy: 0.2502 - val_loss: 1.8132 - val_accuracy: 0.2569\n",
      "training time: 367.4137303829193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://dba1e902-44e0-4559-8827-3f2397cad1f6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://dba1e902-44e0-4559-8827-3f2397cad1f6/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcnn_histo.npy has been saved!\n",
      "dcnn.json has been saved!\n",
      "Mode saved!\n"
     ]
    }
   ],
   "source": [
    "model_type = \"dcnn\" \n",
    "epochs = 100 \n",
    "batch_size = 100 \n",
    "fileName = \"dcnn\" \n",
    "num_classes = 7\n",
    "width, height = 48, 48\n",
    "\n",
    "print('Start Training Model on '+ model_type+'...')\n",
    "\n",
    "#load data\n",
    "x = np.load('./dataX.npy')\n",
    "y = np.load('./dataY.npy')\n",
    "\n",
    "print('DataSize', x.shape, y.shape) \n",
    "\n",
    "\n",
    "xc = np.expand_dims(x,-1) \n",
    "n_values = np.max(y) + 1\n",
    "yc = np.eye(n_values)[y] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xc, yc, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state= 40)\n",
    "\n",
    "#construct the training image generator for data augmentation\n",
    "data_generator = ImageDataGenerator(featurewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        rotation_range=10,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=.1,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "X_dsift = load_dsift()\n",
    "X_dsift_train, X_dsift_val, X_dsift_test = SIFT_data_split(X_dsift)\n",
    "\n",
    "dsift = Sift_layer()\n",
    "CNN =cnn_layer()\n",
    "\n",
    "MergeModel = concatenate([dsift.output, CNN.output])\n",
    "fc = FC_layer(MergeModel)\n",
    "model = Model(inputs=[CNN.input, dsift.input], outputs = fc)\n",
    "\n",
    "data_input = [X_train,X_dsift_train]\n",
    "val_data = ([X_val,X_dsift_val],y_val)\n",
    "\n",
    "checkpoint = ModelCheckpoint(fileName+\"_model.hdf5\", monitor='val_accuracy', \n",
    "    verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=100,mode='max')\n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "start = time.time()      \n",
    "        \n",
    "training_history = model.fit_generator(data_generator.flow(data_input,y_train,\n",
    "                batch_size=batch_size),\n",
    "                steps_per_epoch= len(y_train)/ batch_size,\n",
    "                epochs = epochs,\n",
    "                verbose = 1,\n",
    "                callbacks = callbacks_list,\n",
    "                validation_data = val_data,\n",
    "                shuffle = True\n",
    "    )\n",
    "print('training time:', time.time()-start)\n",
    "save_model(model, training_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7178, 2048) (7178, 2048)\n",
      "Loading models...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#load data\n",
    "x = np.load('./dataX.npy')\n",
    "y = np.load('./dataY.npy')\n",
    "\n",
    "xc = np.expand_dims(x,-1) \n",
    "n_values = np.max(y) + 1\n",
    "yc = np.eye(n_values)[y] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xc, yc, test_size=0.2, random_state=42)\n",
    "X_sift = np.load('Result/sift_histogram.npy')\n",
    "_, X_sift_test = train_test_split(X_sift, test_size=0.2, random_state=42)\n",
    "\n",
    "X_dsift = np.load('Result/d_sift.npy')\n",
    "_, X_dsift_test = train_test_split(X_dsift, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_sift_test.shape, X_dsift_test.shape)\n",
    "\n",
    "print('Loading models...')\n",
    "\n",
    "\n",
    "json_model = open(\"Result/cnn.json\", 'r')\n",
    "loaded_json_model = json_model.read()\n",
    "json_model.close()\n",
    "model_CNN = model_from_json(loaded_json_model)\n",
    "model_CNN.load_weights(\"cnn_model.hdf5\")\n",
    "\n",
    "json_model = open(\"Result/scnn.json\", 'r')\n",
    "loaded_json_model = json_model.read()\n",
    "json_model.close()\n",
    "model_sCNN = model_from_json(loaded_json_model)\n",
    "model_sCNN.load_weights(\"scnn_model.hdf5\")\n",
    "\n",
    "json_model = open(\"Result/dcnn.json\", 'r')\n",
    "loaded_json_model = json_model.read()\n",
    "json_model.close()\n",
    "model_dCNN = model_from_json(loaded_json_model)\n",
    "model_dCNN.load_weights(\"dcnn_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(p):\n",
    "    true_y =[]\n",
    "    predict_y = []\n",
    "    predicted_list = p.tolist()\n",
    "    true_y_list = y_test.tolist()\n",
    "    for i in range(len(y_test)):\n",
    "        proba_max = max(p[i])\n",
    "        current_class = max(true_y_list[i])\n",
    "        class_of_Predict_Y = predicted_list[i].index(proba_max)\n",
    "        class_of_True_Y = true_y_list[i].index(current_class)\n",
    "\n",
    "        true_y.append(class_of_True_Y)\n",
    "        predict_y.append(class_of_Predict_Y)\n",
    "    np.save(\"Fer2013_True_y\", true_y)\n",
    "    np.save(\"Fer2013_Predict_y\",predict_y)\n",
    "    print(\"Accuracy on test set :\" + str(accuracy_score(true_y,predict_y)*100) + \"%\")\n",
    "\n",
    "def ConfusionMatrix():\n",
    "    y_true = np.load('Fer2013_True_y.npy')\n",
    "    y_pred = np.load('Fer2013_Predict_y.npy')\n",
    "    print(len(y_true))\n",
    "    print(len(y_pred))\n",
    "    print(accuracy_score(y_true, y_pred))\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    title='Confusion matrix'\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j],fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 2ms/step\n",
      "225/225 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 0s 983us/step\n"
     ]
    }
   ],
   "source": [
    "predict_cnn = model_CNN.predict(X_test)\n",
    "predict_scnn = model_sCNN.predict([X_test, X_sift_test])\n",
    "predict_dcnn = model_dCNN.predict([X_test, X_dsift_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict CNN\n",
      "Accuracy on test set :61.04764558372806%\n",
      "7178\n",
      "7178\n",
      "0.6104764558372806\n",
      "[[ 528    3   78   40  170   31  135]\n",
      " [  45   25   14    2   13    1    2]\n",
      " [ 157    3  308   42  290  143  100]\n",
      " [  53    1   30 1435   85   54  107]\n",
      " [ 111    1  103   58  673   19  245]\n",
      " [  17    0   58   40   23  625   32]\n",
      " [  82    1   58   84  236   29  788]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAEYCAYAAADVrdTHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABrBklEQVR4nO2dZ3hURReA35OEHhCQAKEX6b0XaUrvSAcBAQUbNmygiIiCYEXwA3sBRBAVpRfpvUpHmoCEHhDpmHK+H3MTlpBsks1mQ3DePPfJ3rlz58zs3j075cw5oqpYLBaLxT1+KV0Bi8ViSQ1YZWmxWCwJwCpLi8ViSQBWWVosFksCsMrSYrFYEoBVlhaLxZIArLK0JBgRySAis0TkHxGZnoRyHhSRhd6sW0ohInVFZG9K18OS/Ii1s7zzEJHuwECgJHAR2AqMUNVVSSy3J/AUUFtVw5Naz9sdEVGgmKoeSOm6WFIe27O8wxCRgcAYYCSQCygAjAfaeqH4gsC+/4KiTAgiEpDSdbD4EFW1xx1yAHcBl4BObvKkwyjT484xBkjnXGsAhADPA6eBE0Af59obwL9AmCPjYWAYMNml7EKAAgHOeW/gT0zv9hDwoEv6Kpf7agMbgX+c/7Vdri0D3gRWO+UsBHLE0bao+r/kUv92QAtgH3AOeMUlf3VgLXDeyfsxkNa5tsJpy2WnvV1cyn8ZOAlMikpz7inqyKjsnOcBQoEGKf1s2CPph+1Z3lnUAtIDM9zkeRWoCVQEKmAUxhCX67kxSjcvRiH+T0SyqerrmN7qNFUNVNUv3VVERDIBY4HmqpoZoxC3xpIvOzDHyXs38AEwR0TudsnWHegD5ATSAi+4EZ0b8x7kBYYCnwM9gCpAXWCoiBRx8kYAzwE5MO9dQ+AJAFWt5+Sp4LR3mkv52TG97P6uglX1IEaRficiGYGvgW9UdZmb+lpSCVZZ3lncDYSq+2Hyg8BwVT2tqmcwPcaeLtfDnOthqjoX06sq4WF9IoGyIpJBVU+o6q5Y8rQE9qvqJFUNV9XvgT+A1i55vlbVfap6FfgBo+jjIgwzPxsGTMUowo9U9aIjfxdQHkBVN6vqOkfuYeBToH4C2vS6ql536nMTqvo5sB9YDwRjfpwsdwBWWd5ZnAVyxDOXlgc44nJ+xEmLLiOGsr0CBCa2Iqp6GTN0fQw4ISJzRKRkAuoTVae8LucnE1Gfs6oa4byOUmanXK5fjbpfRIqLyGwROSkiFzA95xxuygY4o6rX4snzOVAWGKeq1+PJa0klWGV5Z7EWuIaZp4uL45ghZBQFnDRPuAxkdDnP7XpRVReoamNMD+sPjBKJrz5RdTrmYZ0SwwRMvYqpahbgFUDiucet+YiIBGLmgb8EhjnTDJY7AKss7yBU9R/MPN3/RKSdiGQUkTQi0lxE3nGyfQ8MEZEgEcnh5J/socitQD0RKSAidwGDoy6ISC4RaePMXV7HDOcjYiljLlBcRLqLSICIdAFKA7M9rFNiyAxcAC45vd7HY1w/BRS55S73fARsVtVHMHOxnyS5lpbbAqss7zBU9QOMjeUQ4AxwFBgA/OJkeQvYBGwHdgBbnDRPZC0CpjllbeZmBeeHWVU/jlkhro+zeBKjjLNAKyfvWcxKditVDfWkTonkBczi0UVMr3dajOvDgG9F5LyIdI6vMBFpCzTDTD2A+Rwqi8iDXquxJcWwRukWi8WSAGzP0mKxWBKAVZYWi8WSAKyytFgslgRglaXFYrEkAOsIIA4yZ82uOYLz+Uxe9oxpfSYrwseLeiLxmS56l4gI37YvwN937YuI9F3bjv51hHNnQ73WOP8sBVXDb9n0dBN69cwCVW3mLZnexCrLOMgRnI9hE31h6mfoUqmAz2Rdvu5bp0Fp/H07gDl36V+fysuR2Xc/dP9cCfOZrBb31/ZqeRp+jXQlu7rNc+33cfHtoEox7DDcYrH4BgFE3B/xFSHylYicFpGdsVx7QUTU2WwRlTZYRA6IyF4RaeqSXkVEdjjXxkoChj9WWVosFt/h5+/+iJ9vMIb/NyEi+YHGwF8uaaWBrkAZ557xIhIlZALGa1Qx54h36G+VpcVi8REC4uf+iAdVXYHZERaTDzG7v1wnddsCUx0PUYeAA0B1EQkGsqjqWjW7cibi3p8CYOcsLRaLL4l/tJtDRDa5nH+mqp+5L1LaAMdUdVuM0XReYJ3LeYiTFua8jpnuFqssLRaLbxBJyFA7VFWrJrxIyYjxGdoktsuxpKmbdLfYYXgieb7tvQzp1oTXHmzOsF6tAJg6dgSDOt3PkO5NGftify5f/AeA8PAwPh82kCHdmjC48/3M/uZ/SZZ/7do16tSqTvXKFahcoQxvvvF6ksuMyf59e2lQq0r0USg4O5/87yN2bN9K0/vupUGtKjSsW4MtmzZ4TWZERAR1alahc3vj8/fcuXO0bdmESmVL0LZlE/7++2+Py37p6UepWqoATetWiU4b8EgPWjSoQYsGNahTuQQtGtSIvjZ+zLs0qFaG+2uWZ/mSRZ43CvN5NahTk1rVKlGtUjlGDB8GwIyfplOtUjmyZAhgy+ZNbstwx/MD+lOheH4a1q4cnfbuiGE0qlOVJvWq0719S06eMB74jv51mKJ5stKkXnWa1KvOoIEDktI0z0jiMDwWigKFgW0ichjIB2wRkdyYHmN+l7z5MI5dQpzXMdPdYpWlB7w8YSpvfjcv2rSobPW6jPh+IW9NWUDuAoWZ8814ADb+NoewsH956/uFDJs4h6UzpnDm+NEkyU6XLh3zFy1hw5ZtrN+0lYUL5rN+3br4b0wExYqXYNnazSxbu5nFqzaQMUNGWrZuxxtDBvPi4NdYtnYzg4YMY9iQwfEXlkAmfDyWEiVu+Ab+8L3R1G/QkN937qV+g4Z8+N5oj8vu0LUn30z99aa0j7+YzNxl65m7bD3NWrWjWSsTz23/3j3M+mU6C1Zt4dtpMxn68jNERMTmWS5hpEuXjtnzf2Ptxt9Zs2ELvy1awIb16yhVpizfTfuRe+vUi78QN3Tq3pPJ02felPbYUwP5bdUmFq7YQMOmLRjz7sjoa4UKFWHhig0sXLGBUR98nCTZHpHE1fCYqOoOVc2pqoVUtRBGEVZW1ZPATKCriKQTkcKYhZwNqnoCuCgiNZ1V8F7Ar3HJiMIqSy9QtmY9/APMjEbRspU4d/oEYIyxr1+9QkR4OGHXrhEQkIYMmTInSZaIEBhoHIWHhYURHhaWrEbfK5YtoVCRIuQvUBAR4eKFCwBc+OcfcgfniefuhHEsJIQF8+fSq8/D0WlzZ8+ke49eAHTv0Ys5s+J9luOkRu06ZM0Wuw9eVWXurz/R+gHjgW3RvNm0bteJdOnSkb9gIQoWKsq2LRs9lh3z8wpzPq+SJUtRvLin0TpuULN2XbJmy3ZTWuYsWaJfX71y2eebAuIkahiehNVwEfke4+S6hIiEiMjDceV1woj8AOwG5gNPunjRfxz4ArPocxCYF59sO2eZSAR476keIMJ9DzxIgwe633R9xawfqNHYDM+rNmzBlhWLeLZFNa5fu0r354YSeFfWJNchIiKC2tWrcPDgAR59/Emq16gR/00eMuPHabTv2AWAEaPfp1O7lrz+6stERkYyb/EKr8gY9OJzDB8xikuXLkannTl9itzBwQDkDg7mzJnTXpEVkw1rV5MjKBeFi94DwMkTx6hU9cb7GZwnb/Qw1lMiIiKoW6safx48QL/HnqBa9eT7vKIY/dZQfpz6HVmy3MUPMxdEp//112Ga1q9BYObMvPTqMGrUqpPsdbkJz4ba0ahqt3iuF4pxPgIYEUu+TZjQHwnmtupZisgDjlFpbLFabgte/eJn3pg0l+fHfMvi6RPZu2V99LWZX43D3z+AWs0eAODQrq34+fnx4dwNvPfLKuZ/9zmnj/0VV9EJxt/fn/Wbt3LgcAibNm5g185b7HO9wr///sv8ObNp80BHAL7+4lPeGvUe2/ce4q1R7/HME/3jKSF+5s+dTVDOnFSqXCX+zMnArBk/0Lp9p+jz2HaCJrVn5u/vz5oNW/jj4F9s3riR3buS5/Ny5eUhw9m48yAPdOrK159PACBnrmA2bN/PguXref2tdxjQ76HokYJvSLrpUEpyu9WuG7AKY0iaZOIJ3OUR2YJyAZAlew4qN2jKn7u3ArBq9o9sW7WYR9/8KPrLtXbBr5Sr1YCAgDRkyZ6DYhWqcHj3dq/VJWvWrNSr34CFC+d7rUxXfls4n/IVK5Ezl2nz1CmTaNXW/BC0bd+RLZs9H55GsW7tGubNnkW5EkXo26s7K5YtpV+fngTlzMXJE2Y64+SJEwQF5UyyrJiEh4czf86vtGrXMTotOE9eThy7YVVy4vgxcuUO9oq8rFmzUrdefRYtXBB/Zi/RrmMX5s36BTDzp9mymwjD5StWpmDhIvx5cL/P6oIA/v7uj9uY20ZZOoGe7sXEqu7qpDUQkWUi8qOI/CEi30VtSxKRFk7aKme70mwnfZiIfCYiC4GJIrJSRCq6yFktIuU9qeP1q1e4evlS9Otd61eQt2gJtq9dxtxJE3jm/S9Jlz5DdP67c+Vlz6Y1qCrXr17h4M7fCS5U1KP3J4ozZ85w/vx5AK5evcqSxb/dtDDiTX6ePo32nbpEn+fOnYfVK83Qe+WypRRxhq5JYdibI9lz8C927P2TryZOoV6D+/j860k0b9maKZMnAjBl8kRatGqTZFkxWb18CUXvKU5wnhsLo42atWTWL9O5fv06R48c5vChA1SoXM1jGTE/r6VLFlO8RNLnKt3x58ED0a8XzptD0WJG3tnQM9GLVUcO/8mhPw9SoFDhZK3LLXh5gceX3E5zlu2A+aq6T0TOiUiULUQlzHal48Bq4F7HaPVToJ6qHnImfV2pAtRR1asi8hDQG3hWRIoD6VQ11u6diPTHbIHi7ty32qj+cy6UcS+aoWdERDg1m7alfK0GvNS+HuH//su7A3oAZpGn9+CRNOzUiy+Gv8CrXRsDSp1WnchfrJSn7w9geln9+j5EREQEkRpJh46dadGyVZLKjI0rV66wfOlvfDB2fHTahx9P4JWXBhIRHk669On5YNwEr8uNYuALL/NQj65M+vYr8uUvwLffxQyPk3Ce7t+LdatX8ve5UGqVL8qzL71Glx69mTVjOm3a3xxap3jJ0rRs04EmdSrh7x/A8FFj8E9Cj+fUyRM8+kgf83lFRtK+Qyeat2jFzF9n8OLAZwg9c4aOD7SmfPkK/DI78SOEJx/pydrVKzl3NpSqZYry/KAhLFm0gD8P7EP8/MiXvwBvvz8OgHVrVvH+28PxDwjA39+fUe+PI1scC1/Jg9z2Q2133DYxeERkDjBGVReJyNMY+6g5wKtOOFVEZAJGYe4EPlLV+k56G6C/qrYSkWGAquobzrWMmIBapYA3gRBVjddmonCp8mq9DnkH63XIe/ja69C23zd7rbvnlyWfpqv5jNs81xa9tDkxRum+5LboWYrI3cD9QFkRUcAfY1E/FxNGNYoITJ3j+wAvR71Q1SsisgizT7QzcFt+EBbLHU8qGGq743bpE3cEJqpqQce4ND9wCIjLruEPoIiIFHLOu8SRL4ovgLHARlWNbRO+xWLxBXY1PMl0A2bESPsJE9P5FlT1KiYG9XwRWQWcAv6Jq3BV3QxcAL72Sm0tFotn2AWepKGqDWJJG4vpDbqmuW5mXaqqJZ3V8f8Bm5w8w2KWJSJ5MD8MC71Xa4vFkjgS5EjjtuV26Vl6Qj8R2QrsAu7CrI7fgoj0AtZjFooifVc9i8VyE0KqHobfFj1LT1DVDzEOP+PLNxHj3NNisaQoqdt0KNUqS4vFkgpJxcNwqywtFovvuM0XcdxhlaXFYvENYofhFovFkiDEzypLi8VicYsJG26H4XccWTOkpV3ZeAO+eY3wCN9ZNWVI49tJdj8/335BcmZJ51N5vmzf3Zl917YAb7dLiH+j8m2MVZYWi8VHCH6peBieemtusVhSHSLi9kjA/V+JyGkR2emS9q7j23a7iMwQkawu1waLyAER2SsiTV3Sq4jIDufaWEmAcKssLRaLz0iqsgS+AZrFSFsElFXV8sA+YLAjqzTGkXgZ557xIhI1BzUB47u2mHPELPMWrLK0WCw+QUQQP/dHfKjqCuBcjLSFqhrlpHUdN2KCtwWmqup1VT2EieRYXUSCgSyqulaNQ9+JGOfjbrFzlhaLxWckoPeYw4mEEMVnqvpZIkT0BaLc6ufFKM8oQpy0MOd1zHS3WGVpsVh8RgKUZainntJF5FUgHPguKimWbOom3S1WWVosFt8gJGio7VHRJtZWK6Ch3oiVE4IJTxNFPkwsrxBuDNVd091i5yyTQEREBHVrVqVLexN58O233qBU0QLUqVGFOjWqsHD+XK/ICTl6lBZNGlKlQhmqVSrH+I+Nm8+Rb75B8SL5qV29MrWrV2aBl+Q91r8vBfPlomqlcrdcG/PBe2RK50doaKhXZMXk6NGjNG10HxXLlaJyhTJ8PPYjr5YfW9uGD3uN6lUqULNaJVq3aMqJ4/F+bzzi0Uf6UiBPTqpULJss5buS3O+jp3hhgSe2MpsBLwNtVPWKy6WZQFcRSScihTELORtU9QRwUURqOqvgvYBf45NjlWUSmPC/sZQoeXMY2ieeeoZV6zezav1mmjRr4RU5AQEBjBz9Lpu37WLJijV89sl4/tizG4Ann3qWNRu2sGbDFpp6SV6Pnr35Zda8W9JDjh5lyeLfyF8g+YKrBQQEMOqd99m6Yw/LV63j00/+x57du71Wfmxte3bgi2zYvI11G3+neYuWvD1iuNfkudLzod786kEER09I7vfREwT3ijKBpkPfA2uBEiISIiIPAx8DmYFFIrJVRD4BUNVdwA/AbmA+8KSqRjhFPY4JN3MAOAjc+sDHwCpLDzkWEsLC+XPp2btvssvKHRxMxUomMnDmzJkpUbIkx48dSzZ5derWI3ssIVJffnEgb709Olm3rAUHB1Op8o22lixZiuPHvdfW2NqWJUuW6NeXr1xOtvbVqVuP7Nl9E3o2ud9HT/HCang3VQ1W1TSqmk9Vv1TVe1Q1v6pWdI7HXPKPUNWiqlpCVee5pG9S1bLOtQEuQ/c4scrSQwa/NJDhb426ZUfCZ5+Mp3b1Sjz56COc//tvr8s9cvgw27dupWr1GkbehP9Rs2pFHu//MH8ng7wo5syaSXCePJQvXyHZZMTkyOHDbN36O9WctiYnw4a+SvGiBZj2/RSGvJ48PcuUwpfvo1skeYbhvsJnylJEIpwu8i4R2SYiA0WMvyYRqSoiY+Mrwwt1KCQisQZBSwzz584mKCgnFStXuSn94X6PsXXXPlat20zu3Ll5ddCLSRV1E5cuXaJHt06Meu8DsmTJwiP9H2P7nv2s2bCF3LmDeeXlF7wqL4orV67wzuiRvOZDJXLp0iW6de7Au++Puannl1wMGz6CfQf/oku37nw6Id6w8qkGX7+P8WGVZcK46nSRywCNgRbA6xDdJX7aB3UoRBwRIxPD+nVrmDdnFuVKFuXhXg+yYvlS+vftRc5cufD398fPz49efR9hy+aNSa+xQ1hYGD26dqRz1+60bdce4CZ5vfs+wuZN3pPnyp9/HuTw4UPUrFaRUsULcywkhHtrVuHkyZPJIi8sLIxunTvQpduDtHugfbLIiIsuXbrzy4yffSozuUjJ9zE2xNkb7u64nUmR2qnqacxWowFiaCAiswFEpL7TA90qIr+LSGYR8ROR8U6vdLaIzBWRjk7+wyKSw3ldVUSWxVUOMAqo66Q952n9Xx8+kt0HjrDjj4N8OfE76tW/j8++msjJEyei88ye+QulSpfxVMRNqCpPPvoIJUqW4qlnblTbVd6smb9Quox35MWkbNlyHAk5xZ59h9iz7xB58+VjtdN79jaqymP9HqZEyVI889xAr5cfGwf2749+PWf2TEqUKOkmd+ogJd7HBCHxHLcxKWZnqap/OsPwnDEuvYBZtVotIoHANaA9pldYzsm/B/gqHhGxlTMIeEFVW8V2g4j0xyhx8udP/Irv0CGD2Ll9G4hQoEBBxoybkOgyYmPtmtV8P2UyZcqWo3Z1M2n/+vC3+HHaVLZv34aIUKBgQcZ+/IlX5D3UszsrVyzjbGgoxYrkZ8hrw3ioz8NeKTs+1qxezZTvJlG2bDlqVKkIwBtvjaRZc++s9MfWtgXz57Fv3178/PwoUKAgYz/2zucWk149urFy+TJCQ0MpWigfrw19g959k+d9Te730SMkdfuzlAQsAnlHkMglVQ2MkXYeKAGUwlFiIjIIeABjhf+zqoaIyBhgm6p+7dz3MzBFVX8UkcNAVVUNFZGqwHuq2iCOchrgRlm6UqlyVV22er1X2p4Q/H3oE9HPxw+sr/1ZRkb65pmOwtft8xX31qjK5s2bvNa4tDnv0Vyd3nebJ2R8u82e7uBJblJskkBEigARwGnXdFUdBTwCZADWiUhJ3HfQw7nRjvTxlGOxWFKSVDwMTxFlKSJBwCfAxzHtm0SkqKruUNXRwCagJLAK6ODMXeYCGrjcchiIWpbuEE85FzHGqxaLJQVIzavhvpyzzCAiW4E0mN7gJOCDWPI9KyL3YXqduzGW9WFAQ2Anxl/deuAfJ/8bwJci8oqT7q6cSCBcRLYB36jqh15tocViiROR1O0p3WfKUlXjDPyiqsuAZc7rp2LLIyIvqOolEbkb2ADscPKvBIrHUmas5WCUrsViSQFu996jO1KT16HZYtzFpwXeVNXkMfKzWCzJR+rVlalHWapqg5Sug8ViSQKCHYZbLBZLfJi44SldC8+xytJisfiI23/F2x1WWVosFp+Rmg34rbK0WCy+Qeww/I4kIlL550qYz+TlvCt9/Jm8xO6QCz6TBXBP7sD4M3mRP09f9qm8HJnT+kxW2gDfLZBEeHkrtGB7lhaLxZIgUrOyTL3r+BaLJXXhDMPdHfEWIfKViJwWkZ0uadlFZJGI7Hf+Z3O5NlhEDojIXhFp6pJeRUR2ONfGSgJWnqyytFgsPsGYDiV5b/g3QLMYaYOAxapaDFjsnCMipYGuQBnnnvEiErWTcALGHWMx54hZ5i1YZWmxWHyE4Ofn/ogPVV0BnIuR3Bb41nn9LdDOJX2qql5X1UOYSI7VRSQYyKKqax1HPhNd7okTO2dpsVh8RgJ6jzlEZJPL+Weq+lk89+RyYoGjqidEJMqheF5gnUu+ECctzHkdM90tVllaLBbfkLB5yVAvOv+NTZq6SXeLVZYWi8UnJKPp0CkRCXZ6lcHccCgeAuR3yZcPOO6k54sl3S12zjIRvPD0o1QuWYDGdW6EwP1w9FtUL1uE5g1q0LxBDZYsmg/AjOnfR6c1b1CDQkEZ2bVjW5LrcO3aNerUqk71yhWoXKEMb77xepLLBLh+/Rq92t5H1+b30qlJDT75cCQA/5w/xxM92tLuvko80aMtF/4xscnDwsIY+vxjdG5Wiw6NqvHVePfhAuIjIiKCOjWr0Ll9awCGDH6JqhVKU7taRR7s3J7z5897XPbJ4yH069KS9vdXpUOj6kz5ajwAe3fvoFe7hnRqUpNn+nbm0sUb9qdf/u992tSrQLv7KrNm+W+Jkvf8gP5ULJ6fhrUr33Ltk3Efkj97es6dDQXg980baVqvOk3rVadJ3WrMm/1romQ980Q/ShfJS70aFaPT/j53jk5tm1OzYmk6tW1+U/z6j94fTY0KpahduQxLf1uYKFneIJmc/84EHnJePwT86pLeVUTSiUhhzELOBmfIflFEajqr4L1c7okTqywTQaeuPfl22q3v6cOPPcW8ZeuZt2w99zc2i2oPdOoWnfbh+C/JV6AgZcpVSHId0qVLx/xFS9iwZRvrN21l4YL5rF+3Lv4b4yFt2nR8MmUWU+etZsqcVaxZ/hs7ft/INxM+pNq99fll6e9Uu7c+30ww/pJ/m/sLYf9e54f5a5k8azk/T/mG4yFHPJY/4eOxN0VVvK9hI9Zt3s6ajVspWqw4H7w7yuOy/f0DGDhkBD8v2cTEXxYzbeLnHNz3B8NfHsDTg95g+sJ13Ne0Nd9++hEAB/f9wYJZP/Hjog3879ufeXvIQCIiIhIsr1P3nkyaPvOW9OMhR1m5bDF5893o7JQsVYY5S9awYMUGJk2fyeCBAwgPD0+wrK4P9mLqz7NvShv34TvUrX8f67bupm79+xj34TsA7P1jN7/89AMrNmzl+59n8/LApxPVLm/gBdOh74G1QAkRCRGRhzFRWxuLyH5MmO1RAKq6C/gB4/x7PiaAYVSDHwe+wCz6HMQ4B3eLVZaJoEbtOmTNlj3R9838+QfatO/slTqICIGBZkdMWFgY4WFhXnFOICJkzGTKDQ8PIzw8DBCWL5pLqw4m1HqrDt1ZtnBOdP6rV64QHh7O9WvXSJMmDZkCPYvYcSwkhAXz59LLJYJkw0ZNCAgws0TVqtfg+LGQuG6Pl6BcuSlVriIAmQIzU/ieEpw5dZwjfx6gSo17AahZ9z4WzzMKbtmiOTRt3YG06dKRt0Ah8hcqws6tm+Iq/hZq1q5L1mzZbkl/49WXePWNkTd9XhkyZoxu5/Xr1xL9Wda691ZZ8+fMokv3ngB06d6TebNnRqe369CZdOnSUbBQYQoXKcqWZIo1HyuS9J6lqnZT1WBVTaOq+VT1S1U9q6oNVbWY8/+cS/4RqlpUVUuo6jyX9E2qWta5NiBmeJvYsMrSC0z88hOa1qvGC08/yj/n/77l+qxffqStl5QlmCFrjSoVKZAnJ/c3akz1GjW8Vm63FnVoXPUeata5j3KVqnI29AxBOU188KCcuTl39gwADZu3JUPGjDStUZyW95ahZ7+nuCtr4n9IAAa9+BzDR4yK09fh5Ilf07hpvGZwCeL40SPs3bWdshWrUrR4KZYtmgvAojm/cOrEMQDOnDxO7uAbi6M5c+fl9MkTsZaXUBbOm03u4DyULlv+lmu/b9pAw1qVaFynKiPfHxetPD3lzJnT5ModDECu3MGEhprP7OTx4+TNe2OqLjhvXk46bfYF4gXToZQk1ShLEYkQka0uR6GUrhNAjz79WLFpN/OWrSdnrty8OXTQTdd/37yBDBkyUqJUGa/J9Pf3Z/3mrRw4HMKmjRvYtXNn/DclsNzv565i3trd7Ny2hQN7d8eZd9e2zfj5+zN/3V5mrdjO5C8+JuSvQ4mWOX/ubIJy5qRS5SqxXn939EgC/APo3PXBRJcdkyuXL/HCYz15YegoAjNnYdi74/lh4md0b1mPK5cvkiZNGgBi62Qkpfd+9coVxr0/mudfGRrr9UpVq7N47e/M/m01/xvzLteuXfNYlju83S5PSOowPCVJNcoSuKqqFV2Ow0kpTES8YgkQlDMX/v7++Pn50a1nX7ZtuXm4Nuvn6V4bgscka9as1KvfgIUL53u13MxZslK1Zh3WLP+Nu3MEcea0ieBx5vRJst8dBMD8X6dTu14j0qRJQ/YcQVSoWpPd239PtKx1a9cwb/YsypUoQt9e3VmxbCn9+pgh5JTJ37Jg7hw+/2Zykr/UYWFhvPBYD5q360zD5m0AKHxPcSZM/pUpc1bQrE1H8hUsDEDO4Jt7XKdPHiMoV26PZR8+/CdH/zpM07rVqFWhOCeOH6N5g5qcPnVzZJRiJUqSMWNG9u7Z5bEsgKCgnJxyesKnTp4gRw7zmQXnzcsxl+mME8eOkSt3niTJSiypObpjalKWt+Ds71wuIptFZIFjNoCI9BORjSKyTUR+EpGMTvo3IvKBiCwFRnujDqdchmcL5vxKiZKlo88jIyOZM/Nn2jzQyRuiADhz5kz0yvDVq1dZsvi3mxZGPOXvs6FcvGDKvXbtKutXLaNQ0eLUa9Sc2T9NAWD2T1Oo37gFALnz5mPj2hWoKlevXGbH7xspXPSWuHHxMuzNkew5+Bc79v7JVxOnUK/BfXz+9SR+WzifMe+/y9QffyFjxoxJapuq8sZLT1L4nhL07DcgOv2cMzyNjIzk83Hv0vFBM2faoHELFsz6iX+vX+fYX4f569CflK3ouelfqdJl2brvKGu37WPttn0E58nLvGXryJkrN38dORS9oBNy9AgHD+wnf4GCSWgtNG3RmmlTJgEwbcokmrVs7aS34peffuD69escOXyIP/88QOWq1ZIkKzGIkKqH4anJzjIqlC7AIaAzMA5oq6pnRKQLMALoC/ysqp8DiMhbwMNOXjCRIBu5rIpFIyL9MftFb1qxjOKpfr1Yu3olf58LpUa5ojz38musW72C3Tu3IyLky1+Qke+Pi86/fs0qgvPkpUChwl55AwBOnjhBv74PERERQaRG0qFjZ1q0bJXkckNPn+T1Fx4jIiIS1UgatXyAeg2bUb5ydQYNeIhff5hE7jz5GP0/s6usc89+DHvxCTo3rYmq0qbjgxQrVTbJ9Yjiheee5t/r12nXyvg+qFq9BmPGTfCorK2b1jHn56kUK1mGLs3Ngs6AF4dy9PBBpk38HID7m7WhbeceABQtXoomLR+gQ6Nq+AcEMOjN9/D3jzM46S08+UhP1q1eybmzoVQrU5TnBw2ha88+sebduG4N48e8R0CaNPj5+THi3Y/IfneOBMt6tE8P1qxawbmzoVQsWZgXXxnKU8+9SL/e3Zky8Rvy5s/PF99+D5iV9zYPdKRutQoEBPgz6r2PEtUub3C79x7dIQlYBLotEJFLqhrocl4WWAP86ST5AydUtYmI1AfeArICgcACVX1MRL4Blqrqt8RD+YpVdPbi1V5uRdxYf5bew/qz9A5N6tdk65bNXtNumfOX1MoDv3SbZ8XAOpu9uIPHq6SmnmVMBNilqrViufYN0E5Vt4lIb6CByzXffpMsFotBrD/LlGIvECQitQBEJI2IRC05ZwZOiEgaIOnLqBaLJckI7hd3bvcheqrtWarqvyLSERgrIndh2jIG2AW8BqwHjgA7MMrTYrGkMLe5PnRLqlGWrvOVLmlbgXqxpE/AOPeMmd47OepmsVgShn8qHobHqSxFZBxu3Bap6tPJUiOLxXJHIpK6V8Pd9SwTvhnWYrFYEkAq7ljGrSxjmteISCZVtSvJFovFY+7o1XARqSUiu4E9znkFERmf7DWzWCx3FIKzIu7m73YmIaZDY4CmwFkAVd1GLIsqFovFEh9+4v64nUnQariqHo0xMetbj6EWiyX1I7f//m93JERZHhWR2oCKSFrgaZwhucVisSQUAfzu0NXwKB4DPsKEijwGLACeTM5K3Q74+wlZMqZJ6WokC8WDfbtXO6imb63Mjq4c41N5Af6+UwDXwyJ9Jis53EakYl0Z/5ylqoaq6oOqmktVg1S1h6qe9UXlLBbLnYO3XLSJyHMisktEdorI9yKSXkSyi8giEdnv/M/mkn+wiBwQkb0i0tTT+idkNbyIiMwSkTMiclpEfhWRIp4KtFgs/138RNwe8SEieTFTgVVVtSzG21hXYBCwWFWLAYudc0SktHO9DNAMGC8iHvmlS8hq+BRMhLRgIA8wHfjeE2EWi+W/jcRzJJAAjH/bACAjJuZ3WyDKNvxboJ3zui0wVVWvq+ohTDTH6p7UPSHKUlR1kqqGO8dk3GyDtFgsltgQzFqAuwPIISKbXI7+rmWo6jHgPeAv4ATwj6ouBHI58cBx/ud0bskLHHUpIsRJSzTu9oZHhepbKiKDgKkYJdkFmOOJMIvF8h8mYW7YQt05/3XmItsChYHzwHQR6eFOaixpHnX23K2Gb3YKjRL2aAxhb3oi0GKx/Hfxwmp4I+CQqp4x5cnPQG3glIgEq+oJJxbXaSd/COAaIyYfZtieaNztDfde4BiLxfKfJ2oYnkT+Amo6QQivAg0xTn8uAw8Bo5z/vzr5ZwJTROQDzJpLMWCDJ4IT5CldRMqKSGcR6RV1eCLsTqN8yaLUrlaRujWqcN+9NQAY8cZQ7q1eibo1qtC+dTNOHPfoR8wtjz7SlwJ5clKlovcChLly7do1GtSpSa1qlahWqRwjhg8D4Ny5c7Rp0YSKZUrQpkUT/v777wSX+cnrD3Jk8dtsmv7KLdee7dmQq79/zN1ZMwFQtUxB1k0dxLqpg1g/bRBt7isfnXfB58+wbcZr0deDsiXeZvSTj8dQp1oF6lavSP8+Pbh27RrvjBxOueIFaVC7Cg1qV2HRgnmJLjcuYntOohg35n2yZQzgbGioR2U/+2Q/yhTNS/2aFaPT/j53js5tm1OrUmk6t23Oeedz+umHKTSsUzX6CM6ajp3bt3raLI9Iqqd0VV0P/AhswTj29gM+wyjJxiKyH2jsnKOquzAL1LuB+cCTsQUrTFDd4wtYJiKvY2LYlAbmAs2BVara0ROBqYVKlavq0tXr3eYpX7IoS1et5+4cN6LxXbhwgSxZsgDw6fhx/LFnDx+Oi9/vSPo0CbdmWLVyBZkyBfJI315s3rozwfdFER7h3rBZVbl8+TKBgYGEhYXR5P56jH7vQ2b+OoNs2bLz/Isv8/67ozl//m/eHDEqXnlBNZ/m3spFuXzlOl+82YuqnUZGX8uXKyvjhz5IicK5qN19NGfPXyZD+jT8GxZBREQkuXNkYf20wRRp8ioREZEs+PwZBn84gy27/4pTnjuj9BPHj9GqSQNWbdxOhgwZeLhXNxo1acbRv46QKVMgTz4zMN72xCQ+o/TYnhOAkJCjPPNEf/bt3cuy1RtuuR4bMY3S165eSaZMgTz1WB+Wr9sKwPDXBpEtW3aeGvgS4z54h/Pn/+a14W/fdN+eXTt4qFtHNmzfG6esJvVrsu137wUsy1GkjLYZOdVtnq+7lb9tA5YlpGfZEdPVPamqfYAKQLpkrVUqJkpRAly+fDlZnJ3WqVuP7Nmzx5/RQ0SEwEDTYwsLCyMsLAwRYc6smTzYwwwqHuzRi9kzf3VXzE2s3nKQc/9cuSX9nRc68OpHv+D6o331WhgRjkJPlzYN8f2gJ5bw8HCuXb1KeHg4V69cIXdwHq+Wn1Befel5hr01KknPSK1765I1W7ab0hbMnUXn7j0B6Ny9J/PnzLzlvhk/TuOBjp09lusJIglaDb9tSYiyvKqqkUC4iGTBTJxao3SMUmnfujkNalfnmy8/j05/8/UhlClWiOnTvueV14alXAWTQEREBLWrV6ZI/tzc17AR1arX4MzpU+QODgYgd3AwoWdOx1OKe1rWL8fx0+fZse/YLdeqlS3I5h9fZdP0V3h6xNRo5Qnw6bAerJs6iEH9miVaZnCevDzx9HNULF2EsvfkJ8tdWbivYWMAvvxsPPVrVuLpxx+JHrp6g9iek7mzZxGcJy/lylfwmpwozpw5Ta7c5nPKlTuY0DNnbsnz688/0q5jF6/Ljo/UHLAsIcpyk4hkBT7HrJBvwcMJ0tgQkUsxznuLyMfeKj85mb94BcvXbmT6L7P54rMJrF61AoDX3niLXfsP06lLNz7/5H8pXEvP8Pf3Z82GLfxx8C82b9zI7l2JH+67I0P6NLz8cFOGT4jdCm3jziNU6TiCOj3e4cW+TUiX1qxF9nnlG6p1Hkmjvh9yb6WidG+VOPvi83//zfw5s9i8Yz879v/FlctXmD71O3o/8igbt+9l6ZrN5ModzNBXXkxyG6OI7Tn54J2RDE6hH9ItmzaQIWMGSpVOnjlvd5jQEnEftzMJ2Rv+hKqeV9VPMBOnDznD8f88wXnM8C0oZ05atW7Llk0bb7resUs3Zv46IyWq5jWyZs1K3Xr1WbRwAUE5c3HyxAkATp44QY6gnPHcHTdF8gVRMO/dbJg2mD/mvEHenFlZO+Vlct19cyDOvYdOcfnqv5S5x7zXx8/8A8ClK9eZNm8T1coUTJTc5csWU6BgIXIEBZEmTRpatmnHxvVryZkzF/7+/vj5+dGz98P8vtl7UVViPidrVq7gyJHD1K1RmfIli3L8WAj1a1fj1MmTXpEXFJSTUyfN53Tq5AlyBAXddP2Xn37ggQ4p06u8I4fhIlI55gFkBwKc18mOiLQWkfUi8ruI/CYiuZz0YSIySUSWOBvn+znpDURkhYjMEJHdIvKJiPiJyMMi8qFLuf0cUwKPuXz5MhcvXox+vWTxIkqVLsPBA/uj88yfM4vixUskRUyKcObMGc6fPw/A1atXWbpkMcVLlKBFq9Z8N3kiAN9NnkjL1m08lrHrwHEKNhxMyZavU7Ll6xw7fZ5a3Udz6uxFCua5G39/82gWCM5G8UK5OHL8LP7+ftEr5gEBfrSoV5ZdB08kSm6+fPnZvHEDV65cQVVZsWwJxUqU5OTJG+XMnfULJUuXcVNKwontOalUpSr7j5xg+x8H2f7HQfLkzcfyNRvJlTu3V2Q2ad6aH6ZMAuCHKZNo2qJ19LXIyEhm/fIT7Tr4dr4yitQ8DHdnlP6+m2sK3O+lOmQQka0u59kxtlEAq4Caqqoi8gjwEvC8c608UBPIBPwuIlHjueqYlfsjGFOB9pjdR9tF5CVVDQP6cLORPQDO1qr+APnyF3Bb6TOnT9GjqzEIiAgPp0PnrjRq0oxe3Tqxf/8+/Pz8yJ+/AB+M9X4Ejl49urFy+TJCQ0MpWigfrw19g959H/Za+adOnuDRR/oQERFBZGQk7Tt0onmLVlSvUYuHHuzKpG++Il/+AkycMi3BZX77dm/qVilGjqyBHJj/Jm9+Mpdvf1kba97alYrwQp8mhIVHEBmpPDNyGmfPXyZj+rTM/N+TpAnwx9/fj6Xr/+Crn1cnqm1VqtWgdbv2NKxTnYCAAMpVqECvPv14bkB/dm7fhoiQv0Ah3vPS5xbXc+ItHuvbgzWrVnDubCiVShXmxcFDeWrgi/R/qDtTJn1D3nz5+fzbG64c1q5eSXCevBQsnDLLDgmyVbxNidd0KNkrIHLJNSa4iPTGeBQZICLlMEo7GEiLsdxvJiLDAD9VHercMxH4GbP9abiq1nPS+wLlVfVZEfkcY/q0B5ikqtXc1SshpkPeJDGmQ0klPtMhb2P9WXoPX/qz9LbpUK57ymq39390m+ejdqVStelQSjIO+FhVy2F6guldrsXU8hpP+hdAb0yv8mvvVtNisSSE1ByD53ZXlndhvLOD2cLkSlvH6efdGKP5qNWV6iJSWET8ME4/VkG05X9+oDvWxZzF4nPMinfqnbO83ZXlMIxXkZVAzP1gGzDej9YBb6pq1L7CtZitTjuBQ4DrcvQPwGpV9Z4RncViSTD+fu6P25l4Y/CIUfcPAkVUdbiIFAByq6pXbC1d5yud82+Ab5zXv3JjQ3xM9qlq/1jSr6hqXHYRdYAP47hmsViSkdQesCwhunw8UAvo5pxfBFKVpbWIZBWRfZjdSItTuj4Wy38Vv3iO25mERHesoaqVReR3AFX9W0xI3BRDVYfFkb4MWBZL+nmgeHLWyWKxuCfKKD21khBlGeYE+FEAEQkCfGt7YrFY7ghS8Sg8QT3fsZhFkpwiMgKzujzS/S0Wi8VyK6nZdCjenqWqficimzFu2gRop6p7kr1mFovljsJLntJTjISshhcArgCzXNNUNW7vqxaLxRKTVNB7dEdC5izncCNwWXpMVLW9mKDlFovFkmAkMdHBbzMSMgwv53rueBy6xQnFnUakqk/34fpyb/jl6x6FIPGYE6s/8qm8UUsP+FTea419Z2hx/O9rPpMVEeldvxHGztIL5Rj/ul8AZTEdub6YDtw0oBBwGOgctflERAYDDwMRwNOqusATuYk2bVLVLYBbJxQWi8USG17yZ/kRMF9VS2LC3OwBBgGLVbUYsNg5R0RKA10xI+FmwHjHuifRJGTO0jWCkx9QGbjVT73FYrG4wRs9Sye0TT2MUxxU9V/gXxFpi/ERAfAtxt76ZaAtMFVVrwOHROQAxo1j7P4B3ZCQnmVmlyMdZg6zbWIFWSyW/zjxhJRwbDBziMgmlyPmluYimM7a145T8C9EJBOQS1VPADj/o9z45wWOutwf4qQlGrc9S6e7Gqiq3gtIYrFY/pMIEBB/1zI0Hn+WAZjR7VOqul5EPsIZcrsRGxOPJmPdhZUIcIKR+ySEhMViufPxQsCyECDEcbkI8CNGR50SkWAjQ4IxUWij8ud3uT8fcBwPcDcMj/IqtFVEZopITxFpH3V4IsxisfyXEfziOeJDVU8CR0UkKrhVQ2A3JhRNlM/bh7jhrWwm0FVE0olIYaAYHkanTcicZXbgLCbmTiugtfP/P8ezT/ajTNG81K9ZMTpt5owfqVejAsFZ07F1y+bo9HPnztK+VWOK5MnG4Bee8Wo9Hn2kLwXy5KRKRe+FMo2tbX+fO0fnts2pVak0nds2j46lvWXzRhrWqUrDOlW5/94qzJ31S5Llly9VlNrVKlK3ZhXuq1MDgB3bttK4Qe3otM2bPPcKeO3SBWaMfJrPH2vO54+14Nie3/l19HN8/VQ7vn6qHRP63s/XT7UD4Pje7dHpXw1oy741izyW+3j/vhTKl4tqlW5Y4O3Yvo3769WmeuXydHqgDRcuXPC4/BPHQujdsTmt61emzX1VmfTFzQ7Bvv7kI8rkDeTvc8Yd7LGjR6hcNAftG9eifeNavPGy70J+iHjNn+VTwHcish2oiNl+PQpoLCL7MVFoRwGo6i6MH9vdmJhcTzoj5kTjbs4yp7MSvpMbRulRpGzgnhSiS/de9O33BE89diMScMnSZfhq8g+8+OyTN+VNly49L786jD927+KPPbu8Wo+eD/XmsScG8EjfXl4rM7a2jfvwHerWv4+nBr7EuA/eYdyH7/Da8LcpWaoMC5atIyAggFMnT3D/vVVp0rwVAQEJ2eMQN7Pm/cbdOXJEn78+ZBAvDX6Nxk2bs3D+XF4fMojZ85d4VPbiz0ZQpEpdHnhlLBFh/xJ2/RptX77h2nTJF6NIl8mE4Q0qWIyHxvyIn38Al86d5uun2nFPjfvw8098+x7s2ZtHHx9Av743HP0/+Vg/Rox6l7r16jPxm68Y88G7DB32pkftCggI4KXX36Z0uYpcvnSRTs3qUqve/dxTvBQnjoWwZsUSgvPmv+me/AUL8/OiRC8GewVv+LNU1a1AbPOaDePIPwIYkVS57nS5PxDoHJldXkcd/zlq3VuXrNmy3ZRWvEQp7il2a7jbTJkyUaPWvaRLn/6Wa0mlTt16ZM+e3atlxta2BXNn0bl7TwA6d+/J/Dkm6GbGjBmjFeO1a9eSLRyAiESHkb1w4QK5c+fxqJzrVy5xdNcmyjcxURb906QlfWCW6Ouqyh+r5lOqXksA0qTPEK0Yw//9N0mucurUrUe2bDd/Vvv37aVO3XoA3N+wMb/O+Nnj8oNy5aZ0uYoAZArMTJFiJTjthPUdPexlnn/1rdsmXIPglTnLFMPdT+UJVR3us5pYbjvOnDlNrtzBAOTKHUzomRvmtVs2beDZJ/sRcvQvPv706yT3KkWE9m2aIyL0frgfvfv2Y+Q7H9ChbQtee+UlNDKS+UtWelT2+ZNHyZglO3PHDOb0ob3kvqcMDfu/Qtr0GQEI2bWJTFnvJnveQtH3HN+7jbkfvcqF08dpNXC0R73KuChdpixzZs2kVZu2zPhpOsdCjsZ/UwI4dvQIe3Zuo3ylqixZOIdcwXkoWabcrfn+OkKHJrUJzJyZp18aSpUa93pFfkJIzY403PUsb9tWicirIrJLRLaLyFYRqZHA+wqJyM7krt9/gcpVq7Ni/TbmL13D2A/e4dq1pG3Dm794BcvXbGT6jNl88ekEVq9awVdffMrI0e+za99hRox+n6cf7+dR2ZER4Zw8uJtKLbrRZ+wM0qTLwLrpn0df3718TnSvMoo8JSrwyPjZ9PpwOuumf0b4v9eT1D5Xxn/6JZ99Mp46Naty8dJF0qZNui/ty5cv8Wy/Bxn0xmj8AwL4bOy7DHhhyC35gnLm5rcNe/hp4Rpeen0ULz3Zl0sXPZ8zTQxC6vaU7q5+sY7/UxoRqYVZYKqsquWBRtxsdGrxEkFBOTnlDOlOnTxBjqCgW/IUL1GKjJky8cfupM3LBgebIXZQzpy0atOWLZs28v13E2nd9gEA2rXvyJbNG90VESeZc+Qmc45c5ClRAYAS9zbl1MHdgFGk+9YuomS9FrHemyN/UdKkz8CZI/s8kh0bJUqWZObcBaxat4lOnbtRuEjRJJUXFhbGs/0epOUDXWjcoi1HD//Jsb8O075xLRrXKM2pE8fo2LQOZ06fIm26dGTNfjcAZcpXIn+hwhz+00d76e/U6I6qes6XFUkEwRjD1esAqhqqqsdFZKiIbBSRnSLymRNoDRGpIiLbRGQt8KS7gi0306R5a36YMgmAH6ZMommL1gAcOXyI8PBwAI7+dYSD+/eRv2BBj+Vcvnw5em7y8uXLLFm8iFKlyxAcnIfVK5cDsGLZEooULeZR+YHZgsiSI5izIX+a+m9bS44CRkEd3rqWu/MVJkuO3NH5z58MITLCtO+f08c4d+wQd+XM53H7YnL6tDEBjIyM5J1RI3i4n+d+aVSVoc8/QZF7StD70acAKF6qLCu3H2bR+t0sWr+bXMF5+XHBKoJy5uLc2TNERJjF4KNHDnHk0EHyFSiU5DYlBAH8RdwetzPem4jxHQuBoU4Ast+Aaaq6HPg4ao5VRCZhep+zgK8x1v7LReTdpAh+rG8P1qxawbmzoVQqVZgXBw8la7ZsvPrSc5wNPUOPzm0pW64CU2fMAaBquWJcunCBf8P+Zf6cmUydMYcSJUsnpQoA9OrRjZXLlxEaGkrRQvl4begb9O77cJLKjK1tTw18kf4PdWfKpG/Imy8/n39rwq1vWLeacR++S5o0afATP0a9P5a7784Rj4S4OXP6FD26msWXiIhwOnTuSqMmzcgUGMjgFwcSHh5O+vTpGPPxBI9lNHpsCLPfe5GI8DCy5s5Pi2eNs/89K+ZQqt7NlnAhuzez7sfP8fcPQPz8aPz462S8K1tsxcZL757dWbliGWdDQyleJD+vvjaMS5cu8fkn4wFo0+4Bej7UJ55S4mbLxrXM/Ol7ipcqQ/vGtQB4dtAw6jVsGmv+TetW8/F7b+HvH4C/vz9D3/6IrNm8u1jojttbHbpHVFOfFZCzDbMucB/GXdwgTNTJl4CMGNvQccAEYIeqFnDuKw9MUdVYDRSdfaj9AfLlL1Bl007fufq6K2Man8n650qYz2QBpAvw7WzUneyi7UjoFZ/J6ty8Lju3bfGafitSury+NXmu2zwPVsm/OZ7tjilGauxZ4hiVLgOWicgOjMIsD1RV1aMiMgzjqFhIhE2oqn4GfAZQoVKV1PcrYrHcxgi3/1DbHbf7AtQtiEgJEXGdvKqIcfwJECoigUBHiA6B+4+I1HGuP+ireloslltJzQs8qbFnGQiMc7wlhwMHMEPn88AOjJdk12XTPsBXInIF8MhDssVi8Q63tzp0T6pTlqq6Gagdy6UhzhFb/gouScOSp2YWi8UdIqTqYXiqU5YWiyX1crsPtd1hlaXFYvEZqVdVWmVpsVh8RJRRemrFKkuLxeIzUrGutMrSYrH4CkFS8UDcKkuLxeITUvswPNUZpVssllRKwkLhJqwoEX8nFO5s5zy7iCwSkf3O/2wueQeLyAER2SsisW+aTwBWWVosFp/hRU/pzwB7XM4HAYtVtRiw2DlHREoDXYEyQDNgvONbItHYYXgc+PsJgenvzLfH144t/HzsHXtII89cuXnKRysP+kzW03WK+ExWWi8/J94ahotIPqAlJq7OQCe5LdDAef0txnfEy076VMel4yEROQBUBxIdhMj2LC0Wi8+QeP4SyBiMh7FIl7RcqnoCwPmf00nPy83OwUOctERjlaXFYvEZCRiG5xCRTS5H/5vvl1bAaWcbc4JExpLmkUexO3OcabFYbjsSOAwPjcef5b1AGxFpgXHDmEVEJgOnRCRYVU+ISDBw2skfArjGAs4HHPek/rZnabFYfER8g/D4h+GqOlhV86lqIczCzRJV7QHMBKKCsz8E/Oq8ngl0FZF0IlIYKAZs8KT2tmdpsVh8Q/LGBh8F/CAiDwN/AZ0AVHWXiPwA7Ma4dHzScR6eaKyytFgsPsHbRumqugyz6o2qniWOiLSqOgKzcp4krLL0kMf792Xe3DkEBeVk4+87AOj1YFf27zNO2//55zx33ZWVtRt/97rshQvm88LAZ4iIiKB330d48aVBXpdRvlRRAgMz4+/vT0BAAEtXrWfHtq0MfOYJrl27TkBAAO+NGUeVqtW9Ii8iIoL691YnT548/PDzLM6dO0efnl3568gRChQsyDeTp5Etm2dBw1wJOXqU/g/35tSpk/j5+dHn4X48MeBp3hw2lDmzZ+Ln50dQUBCffP41wXnyeCTj6qULzHj/FU4d3o8ItH9hFLtWLuCPdUvxD0hD9jwF6PDiKDIEZuHvkyGM6duMHPkLA5C/VEXaPfumR3KvXbtGs0YNuH79OuHh4bR7oAOvDh3Gq4NfYt6c2aRNm5bCRYow4bOvyJo1q0cykkrq3b+TSgOW+YLKVarqyrVxx6letXIFgYGB9Ov7ULSydGXwS8+T5a67GPzq0ATJ80+gLWJERATlShdnzrxF5M2Xjzo1q/Ht5O8pVTrhUSOv/Rv/KKR8qaIsXbmeu3PciNrYvnUzHh/wDI2bNmfh/LmMHfMes+cvibeshNhZfvzRh/y+ZRMXL17gh59n8dorL5MtW3YGvvgyH7w7mvPn/2b4iFHxlgPgTtzJEyc4efIEFStV5uLFi9StVY2p038mT958ZMmSBYAJ/xvHH3t281ECo0mOXfXnTec/jn6JguWqUq1FZ8LD/iXs+jVC/thGkUq18PcPYP7n7wDQrN9L/H0yhIlD+vPMF+4DeUXhzs5SVbl8+TKBgYGEhYXR5P56jH7vQy5euED9++4nICCA1141P6xvJuC9rFe7Ols2b/KafitVrpJ+/ctSt3lq3ZPttg1YZhd4PKRO3XpkiyOEqKry80/T6dS5m9flbtywgaJF76FwkSKkTZuWTl26MnvWr/Hf6AVEJDq+94ULF8id27OeV0yOhYSwYP5cevW5Ec537uyZdO/RC4DuPXoxx0ttzB0cTMVKlQHInDkzJUqW5PixY9GKEkzsck+d1F67fJHDOzZStXknAALSpCVDYBaKVa2Lv78ZyOUvVZELZ04msSW3IiIEBgYCEBYWRlhYGCJCw8ZNCAgwsqtVr8HxkBCvy04ofiJuj9sZOwxPBlavWknOnLm4p5j3d5IcP36MfPluWELkzZuPDRvWe12OiNC+TXNEhN4P96N3336MfOcDOrRtwWuvvIRGRjJ/yUqvyBr04nMMHzGKS5cuRqedOX2K3MHBgFFwZ86cjut2jzly+DDbt26lavUaALwxdAjffzeJLHfdxZwFiz0q89yJo2S8Kzs/vfsyJw/+QZ7iZWn1xBDSZsgYnWfz/B8p36Bl9PnfJ0P4+NE2pMsUSOM+z1GoXDWP2xQREUHdWtX48+AB+j32BNWctkUx6duv6dCxs8flJ5XbWx26J1l7liLyqojsEpHtIrJVRGrEf5dHcuY6AcxuC6ZP+55OnbsmS9mxTZskh6v++YtXsHzNRqbPmM0Xn05g9aoVfPXFp4wc/T679h1mxOj3efrxfkmXM3c2QTlzUqlyFS/UOuFcunSJHt06Meq9D6J7la8Pf4s/Dh6hc9fufDbhfx6VGxkRwYn9u6jRujsDPp1J2vQZWD710+jrS78bj59/ABUatgEgc/YgXvpuOQM+nUmLx17hh5EDuXb5YlzFx4u/vz9rNmzhj4N/sXnjRnbv2hl97d1RIwkICKBLtxQMcirxHLcxyaYsRaQW0AqorKrlgUbcvO3I3b0J6vGKwU9VWzhhb1Oc8PBwZv46gw6duiRL+Xnz5iMk5MbbeOxYCHk8XIhwR3CwKTMoZ05atWnLlk0b+f67ibRu+wAA7dp3ZMvmuOd0E8q6tWuYN3sW5UoUoW+v7qxYtpR+fXoSlDMXJ0+cAMw8Y1BQznhKSjhhYWH06NqRzl2707Zd+1uud+7SjV9/+dmjsu8Kyk2WoNzkL1URgLL1mnF8/y4Atiz8mb3rltJ58PvRP3ABadOR8S6zcJW3eFmyBxcgNOSwR7JdyZo1K3Xr1WfRQhPQ9LtJ3zJv3hy+/GZyisXBEUndw/Dk7FkGY6zxrwOoaqiqHheRwyKSA0BEqorIMuf1MBH5TEQWAhNFpLeI/Coi8x3XSq87+QqJyB4RGQ9sAfJHlSkimURkjohsE5GdItLFuaeKiCwXkc0issCx8E8Wli7+jeIlSpI3X75kKb9qtWocOLCfw4cO8e+//zJ92lRatmrjVRmXL1+Onpu8fPkySxYvolTpMgQH52H1yuUArFi2hCJFkz7NMOzNkew5+Bc79v7JVxOnUK/BfXz+9SSat2zNlMkTAZgyeSItvNRGVeXJRx+hRMlSPPXMc9HpBw7sj349d84sipco4VH5mbMHcVdQMGeOmkWfg1vWkrPgPezbsIIVUz+j55ufkDZ9huj8l8+fJTLCLLidO/4XoceOkD04f6xlx8eZM2c4f/48AFevXmXpksUUL1GCRQvn8+H77zLtx1/ImDGj+0KSmVTcsUzWOcuFwFAR2Qf8BkxT1eXx3FMFqKOqV0WkN8Y7SFngCrBRROYAoUAJoI+qPgE3DUObAcdVtaWTfpeIpAHGAW1V9YyjQEcAfWMKd/ah9gfIX6CA24r27tmdlSuWcTY0lOJF8vPqa8N4qM/D/Dh9WrINwQECAgL48KOPad2yKRERETzUuy+ly5Txqowzp0/Ro2tHACIiwunQuSuNmjQjU2Agg18cSHh4OOnTp2NMAleLPWHgCy/zUI+uTPr2K/LlL8C3303zSrlr16zm+ymTKVO2HLWrm4We14e/xcRvvmL/vn34+fmRv0ABPhrnedtaDXiNH95+noiwMLIH56fDi6MY/2R7IsL+5auXewM3TIQObd/I4m8/ws8/APHzo+2zb5AxS1aP5J46eYJHH+lDREQEkZGRtO/QieYtWlGhdHGuX79O25bGlWO16jUSvNLvdW53jeiGZDUdcvzG1QXuAx7F+JgbBlRV1VARqQq8p6oNRGQYoKr6hnNvb+B+Ve3lnA8HzgG/AEtVtbCLnMNAVSA7sAD4AZitqitFpCywBoiy7/AHTqhqE3d1j890yNsk1HTIGyTEdMib+NpFm4/F3WI6lJz40kWbt02HSpevrN/Nct9fqlwoy21rOpSsq+HOtqJlwDIR2YHZsxnOjeF/+hi3XI5ZRBznMfNFydsnIlWAFsDbzpB+BrBLVWt51AiLxeIVBN//kHmT5FzgKSEirpNaFYEjwGHMcBugQzzFNHbcxWcA2gGr45GZB7iiqpOB94DKwF4gyFlwQkTSiIh3x60WiyVhpOJJy+TsWQYC4xyTnnDgAGY+sBTwpYi8AsRnILgKmATcA0xR1U0iUshN/nLAuyISCYQBj6vqvyLSERgrIndh2jwG2OVpwywWi2fY6I6x4DjnrB3LpZVA8VjyD4sl72lVHRAj32HMoo9rWiHn5QLniFn2VqBe/LW2WCzJSWoehtsdPBaLxTekgqG2O25bZamq3wDfpHA1LBaLF7HDcIvFYomH1L4abpWlxWLxHVZZWiwWS/yk5mG49WdpsVh8hp+4P+JDRPKLyFLHP8QuEXnGSc8uIotEZL/zP5vLPYNF5IDjY6Kpx3X39EaLxWJJNEk3Sg8HnlfVUkBN4EkRKY3ZSr1YVYsBi51znGtdgTIY3xHjnW3YicYOw+MgUtWne6gzpb9zP4qw8Eifyjt94bpP5flyv/aj07f7TNaRv694tTyjD5M2DFfVE8AJ5/VFEdkD5AXaAg2cbN9itlm/7KRPdbyfHRKRAxgHPWsTK9v2LC0Wi2+IZwie2JVyZzdfJcxOwFyOIo1SqFEOUPNysx/dECct0dy53RmLxXL7Eb9CzCEim1zOP1PVz24pRiQQ+Al4VlUvuHFoHNsFj1ytWWVpsVh8hCRkGB4an4s2x0ftT8B3qhrl0v6UiASr6gnHuXdU0KYQwNWbcj7geOLrbofhFovFR0QZpSdxNVyAL4E9qvqBy6WZGBeQOP9/dUnvKiLpRKQwUAzY4En9bc/SYrH4jqSbWd4L9AR2iMhWJ+0VYBTwg4g8DPwFdAJQ1V0i8gOwG7OS/qTjZzfRWGVpsVh8hhdWw1cRt8ptGMc9IzChZJKEVZZJYMLHY5j87deICKXKlGXchC94+83XWTBvDmnTpqFQ4aKMm/AFd2XN6lW5jz7Sl3lOCNnNW3fGf4MHlC9VlMDAzPj7+xMQEMDSVevZsW0rA595gmvXrhMQEMB7Y8ZRpWr1JMuK7X1Mn9440f/4ow8YNuRl9h46wd05cnhU/oljIQx+ph+hZ04hfn50frAPPR95krHvDGfJwjmI+HF3jiBGfvgpOXObWHZ7d+9k2MtPc+nSBfz8/PhhzgrSpY/p2D9+Qo4epf/DvTl16iR+fn70ebgfTwx4mh3bt/HMU09w+dIlChQsyJffTI4OyZsYgrOk46k6haLPcwam5cftJ9lz6hJ9q+cjjb8fEap8vSGEP89ewV/gkZoFKJw9A35+wqo/zzFzl/djssdFat4bbucsPeTE8WN8/sn/+G3FOlZt2EpkRAQzfpxGg/sbsWrDVlas+52i9xRjzPujvS6750O9+XX2fK+XG5NZ835j5brNLF1lfDS/PmQQLw1+jZXrNjN4yOu8PmRQkmXE9T4CHAs5yvKlv5Evv/vgcfEREBDAS6+/zezlW5g6aylTvvmcA/v20PfxZ/nlt/XMWLSW+o2aMf7DtwETzvjlpx/m9VEfMWvpJr6dPo+ANGk8lj1y9Lts3raLJSvW8Nkn4/ljz24GPN6f4W+OZP3mbbRu046PPnjPo/JPXLjOK3P38srcvbw6by/XIyLZdPQ83SoF8/OOk7wydy8/bjtBt8omtHGNgllJ4y8MmrOXIXP3cn+xHOTIlNYj2YlGTDhcd8ftjFWWSSA8PJxrV68SHh7OlStXyB2ch/saNiYgwHTYq1arwfHjIV6XW6duPbJnz+71cuNDRKJD5F64cIHcub0Trzy29xFgyKAXeP3Nt5Mc5zooV25Kl6sIQKbAzBQpVoLTJ08QmPlGT+7qlSvRclYvX0zxUmUpWaYcAFmz342/v0ebPsgdHEzFSiaKZObMmSlRsiTHjx1j/7693FvX+KO+v2Fjj+OUu1I2d2ZOX7xO6OUwFMiQxtQ5Y1p/zl8JA4zNTLoAP/wE0vr7ER4ZydUwXwawS71xJeww3EOC8+Tlyaefo2LpIqRPn4EGDRtxX8PGN+X5btI3tOvQKYVqmDREhPZtmiMi9H64H7379mPkOx/QoW0LXnvlJTQykvlLViZZTlzv47w5swjOk4ey5Sp4oTU3OHb0CHt2bqN8JWOdMmbUMGb++D2BWbLwzfS5ABz58wCC0K97W86dDaVF2448/MRz7opNEEcOH2b71q1UrV6DUmXKMmf2TFq1bsuMn3/kWMjR+AuIh5oFs7Lm8HkAJm06xssNi9K9ch5E4I0FJi76hiPnqZLvLv7XoSxpA4TJm45z2Uc71VK7izaf9ixFREXkfZfzF5wQuJ6UlVVEnvDw3sMi4tkEmMP5v/9m3pxZbN6xn537/+LK5Sv8MPW76OsfvPs2AQEBdOrSPSliUoz5i1ewfM1Gps+YzRefTmD1qhV89cWnjBz9Prv2HWbE6Pd5+vF+SZYT2/s4bcokPnzvbQa9OizpDXHh8uVLPNPvQQa/MTq6V/nsoGEs2bSXVg904buvPwUgPCKcLRvX8s7HXzL5l0X8Nm8Wa1cuTZLsS5cu0aNbJ0a99wFZsmRh/Kdf8Pkn46lbqxqXLl4kTdqkDYX9/YQq+e5i/V/nAWhUPAeTNx3j6Rm7mbzpOP1qmqmMojkyEanKgJ928tyMPbQoHURQoI+G4dhheGK4DrRPqqJyyArEqiw93SifGJYvW0zBgoXIERREmjRpaNWmHRvXm+2mU7+byMJ5c/jky4lJHkKmFMHOUDgoZ05atWnLlk0b+f67ibRu+wAA7dp3ZMvmpMdVj+19nDL5W/46fJj6tatQqcw9HD8Wwv11q3Pq1EmP5YSFhfFsvwdp9UAXGrdoe8v1lg90ZtFcY5qXOzgP1WrWIVv2HGTIkJF69zdh985tSZLdo2tHOnftTtt27QEoUaIkv85ZwMq1G+nYpStFihT1uHyAinkyc/jcFS5cCwegbpHsbDz6DwDr/zpP0bszAlC7UFa2H79IhMKF6+HsO32ZItkzJkl2YpB4/m5nfK0sw4HPgFvGNCISJCI/ichG57jXSR8mIi+45Nvp7AkdBRQVka0i8q6INHBcN00Bdjh5fxGRzY4rp/7ebEi+fPnZtHEDV65cQVVZsWwJxUuUZPGiBYz98D0mT5tBxoy+ewi9yeXLl6PnJi9fvsySxYsoVboMwcF5WL1yOQArli2hSNFi7opJELG9j63atOOPQ8f5fdcBft91gDx587Fk5QZy5crtkQxV5bXnn6DIPSXo/ehT0emH/zwQ/XrpwjkUKWri6N1bvxF79+zk6tUrhIeHs3HdKu4pVtJj2U8++gglSpbiqWduPPZnTpsV6MjISN59ewR9H0na41mrULboITjA31fDKJUrEIAyuQM5edE4Fwm9HEbp3CY9nb8fxXJk4viFa0mSnRhSc88yJeYs/wdsF5F3YqR/BHyoqqtEpAAmSmMpN+UMAsqqakUAEWmA8SZSVlUPOXn6quo5J+74RhH5SVXPeqMRVarVoHW79txfpzoBAQGUq1CBXn36Uad6Ba5fv07Hts2i873/0XhviIymV49urFy+jNDQUIoWysdrQ9+gd9+HvVb+mdOn6NG1IwAREeF06NyVRk2akSkwkMEvDiQ8PJz06dMx5uMJSZYV1/voTbZsXMvMn76neKkyPNC4FmCG3z9P/ZZDB/fj5+dHnrwFeH3URwDclTUbD/V/is4t6iEi1Lu/KfUbNfNI9to1q/l+ymTKlC1H7epmoef14W9x8MABPvvEPBdt2j1Az4f6eNy+tP5C2eDMfLn+xrznF+uO0qtqXvz8hLCISL5wri3aF8qjtQowulUJBGH5n2c5et43yjI1KER3iKpHe8o9EyZySVUDRWQ4Jq73VSBQVYeJyGlu3rMZBJQEngcuqep7Thk7gVZOntmqWtZJbwC8rqr3ucgbBjzgnBYCmqrqOhE5DFRV1dAY9euPiW1OvvwFqmzdfdBLLY8fX7po86XrOYCISN89Y+B7F235787gM1m+dNE2f2h3zv6522vqrWLlKrpo+Xq3eXJmSbM5vr3hKUVKrYaPAbYAX7uk+QG1VPWqa0YRCefm6QJ3lsGXXe5rADRyyrwiIsviuRfHu8lnYD7YeNpgsVgSSWruWaaInaWqngN+AFzHjguBAVEnIlLReXkYqOykVQYKO+kXgcxuxNwF/O0oypIYr8oWiyUFSc1zlilplP4+4Loq/jRQVUS2i8hu4DEn/Scgu7Np/nFgH4Az97jaWfB5N5by5wMBIrIdeBNYlzzNsFgsCSO+tfDbW1v6dBiuqoEur08BGV3OQ4EusdxzFWgSR3kxjRiXuVy7DjSP475Ciai2xWLxAsLt33t0h93BY7FYfIZVlhaLxZIAbvehtjussrRYLD5BPAhKdjthlaXFYvEdVllaLBZL/KTmYbj1Z2mxWHyGN+KGi0gzEdkrIgdEJOkeqBOIVZYWi8V3JNH3r+NR7H8Ys8DSQDcRKZ1Mtb0JqywtFovP8IJRenXggKr+qar/AlOBW33uJQN2zjIOtv2+JTRH5jRHEnlbDiA03lze406WZ9uW8vIKerMSv2/ZvCBj2nh92aYXkU0u5585PhuiyAu4upUPAWp4q47usMoyDlQ1KLH3iMgmX3pMuZPl2balXnlxoaqe+bm7mdi6nz5xemOH4RaLJTURAuR3Oc/Hza4dkw2rLC0WS2piI1BMRAqLSFqgKzDTF4LtMNy7fBZ/FivvNpTla3l3ctuSFVUNF5EBmEgK/sBXqrrLF7J96indYrFYUit2GG6xWCwJwCpLi8ViSQBWWVosDuIEeY/6b7G4YpXlHU5yfPHvYGVSBEBV9Q5u4y2ISGbn/3+mzZ5glaWPEJFgEfHz1QMpIhlEJKPzxc/nzbLVWRUUkeYikj++/MmBiGTzcnmBwCQRGQ3/DYUphoLAJhGp8l9oc1KwyjKZcRRkDuBnTFjeZDc/cB74KsBgEekOvCUiub0sow3wJCb2u09xFPRwEcnmjS+3iPip6iWgB1BHRF6GlFWYsckVEa9+X9VwBPgG+FpEKlqFGTdWWSYzqhrpBGObCjwiIpl8IFOB7UAF4GNghqqedDy2JBkRaQQ8AExR1dAU+HJlAwoBgd74cqtqpPOyDOZ9e1xEXnGu+Vx5iIg4chuLyBAReUZECrnU0ysyopSvqr4NTAK+F5FKVmHGjlWWyYiIFHSG3/7AFCACY0ibbPNDLuVexoQNng+0EpFgVY1IYplRZMZsM6slIrl90Vt26hEEoKrbga3AhyKS1hvyRaQXMALTyxoGNBeRYY48nyoPR15LYCSwE2gNvOitOkQpY1WNjJrOUNV3gc+xCjNOrLJMJkSkLjANGAR8ghmu5nTOSQ4F49IjaYf54r+Kicf+N/Cekye/iLRIbJnO6woikklVZwCvA3cDLUQkp5ebEls9CgIjRORrZ0FiEqYXmCOqnkkUkREYq6rrgYnAQKCTiLwOyfN5xUM9TO89EsgEjHQ+2wxJLdjl83wO84PznYgUVtUPgPHARBGplgJtvq2xyjIZEJFawLOYL9xI4Bow2vl/v4jkTQ65zpepBTAUWOrETj+LcZZ6RETWAAuBC4kpE0BEnnbKeVNEhgPbgAlAA6BjVK/Pm0QpQDF7gI9j3surmB+CocBDQHfXeiam3Bgo8JzTU43EtG8j0EhE7k5KOxJTJ5dpmkyYKZTngO6qeszpbbbxxtyliDwJtAGeAKoCn4tILVUdC3wHfCwi6ZIq545CVe3hxQMoAXwBPBYjvTTQHjOs6puM8j8CmgG5ME5RvwUaAdmBzkBdD8rsACwH0mHmXtdgesuBTtmfA3clU3uaATOAUUBjJy2305ZVwByghIdldwQeASo45+8C64F7gF7O53i3D56ZqG3HzYEhmKmaqsB+YKBzrR5mWqVeUmS4nA/FeO95DpiF+THfHvV8ANmSu92p7UjxCtxJB8bX3r3OF/gnIJ/rNed/FefhzOBl2cWd/0Mc2cud15OArwD/xLTDpT3+QEvnizUA+A3jbHWho4izeLstLvWoDixylPUbwFjgecDPuV7UaVvDBJaX0eX1s8BKzHTCUuAxzFD8TeAHYAVQ3ofPTmNgN3Cvy3vfyFFgkzE93ZZekDMQMz0jQHFgscu1fZjRQ3pftTs1HdbrUBJxmScsjzGleRp4DXgYMzydpqonXG7Ji+n1ebMOgcBoEdmpqq+JSD0gVFV3i0gJ4GtH7l8JKMtPb6y6BgFnVHWOc60K0EVVz4rIccxwPqOqnvRmexxZ+YAPgNWq+pOIzAfqAF0wivuIqh4UkTDMAshi1/nVWMprCTQWkXdxFqdUta6IvADcBVTCDMWHOp9nelW95u12udTnHkzvbaOIpMH0cEeq6moR6QBUwyjI2pjnxU9V9ydRZmtMO1922njOSW+HaftGYHRytjs1Y+csk4jz0N0H9MPM370L7MHM+xQGHhKRPE4+waxS91DVJNknxph3u4LpeZUSkeGqusJRlO0wvcxRqhqvonTaE+mU/yRmZfjDqFVhzBTDUBHpgQk58HZyKEqHy5hhdjcRqaGql1V1AUbRFXHqGIB5hr906h6XomyFme9cpqrHML21gY4CbYXpwZ4AngKecOYErydTu6IoAaQRkUBVDcNYLTwqIouA+pi52eYAqnrQE0XpOufozJM3wYx8zjjJVzHPaR/M8zMioc/Jf5KU7tqm9gMzrD4C1AUexAwVP8fM7zXArC4WTibZdYEazms/oCzwI6Z3BGbyvpHzWuIpK7fL626YoWk+zBDw+6g8mDnLH3Dm+bzYlqihf2WMAiuIsad8EuO7sAVm2L0LqBzzPnftctpSzTnP4Hw2+YAXgFec9F6YBaucPnhmotqaDTgM3A+kdZ6X0s612sBaIIeHMjI5710xzELOixg70l+cZzLA5f3I5It2p/YjxSuQWg+XB74h8LHzOo3zQC7GLLSkxWWeLBnq8CimR1TdRX4fzNzT4ESU0xLYAAQ5510wCwyPYuYmAzBzXPmi5CRTe+4HDmJ6O1swq93lMIsR5zALPVFKz62SdCkzm9OGckB6jA3lb5ie3AaMac5XwB84874+en6aYXqQ/TEjkYYu1xph5i9bJ1FGS4w1xD5uzPOWc34UxibX53inHnYYnkhiMTs5CbQWkeaqGqbGa/PvGPu/J4Hr3jbuFZGSItJeVT/FzI9OFpHqaoZzRzBu9pcksKxmGNvPoaoaNTy77NzfSVWbqGo4Zk7taRFJ58jxKs7c6qPAg6r6IDAcM0eZmxumQpHAxUQWfR7TM30POIDZ+TMVs5CzCLNotQJopar7ktqOhODM/T6ImTn4DHgb+ExE6jtTAIUxq+Czkvjs/In58YnAKEkwinkcxpJhdBLK/u+R0to6NR6Y3uR4zMJCRsxq7QLM3uIaGEXzIvCuF2VG9WTrYb7kR4C2TtrDwA7MfOlhoE4Cy8yOUUDtnPN7MPOUWYH3gXmYoe9jmMWGMsnwXvphhsWDMV/kl12uPYXppftjFpteweyESk8Ce5ZOOYFALYy5UTqX9G+BDj5+drJgzJMWx0jvgfnhrZ+YtrmR0x6o6Lx3HTHztPc512o6z6kdeifmPU3pCqSWw0VZ1XQe9pGOMnkGswe7CcZcZxbmV7wFpoeX0RsPvyO7DmabXy1gDKaH9IDLtY5Ag0SW2RIz5C3vKKbnnPSsmN7Xj5g5Sq8qSpf3M53zPyPG5m+cy49AZec9zOCc5wCye0l+J2AzUNSHz47rD15I1Hvtkq93Yj8/NzKHOc9K1LRFH0xP8z3nc87tDTn/pSPFK5CaDqAkZoW2lXN+H2b+ZyCOES9mfu9+zBxYOS/JjZpvegGzJS8qfQCmR9kOSJuE8pthepiDnHNxuSY4iwFefB9djbB/xtiDNsLM8b6EsX+chOmhJ2neLhbZwRgby11AWR8+Ow0wNp1dMdteK2N+dAfE9f54KKegy+vnMfOyUQqzJWYzQSlftftOOuycpRtEpISIdJUbPhsFY4/2JICqLsUMCysC/UUkI+YLXxRoo6o7kig/ar4q2Pm/BcgqIqUd+R8DRzGrnQU8laOq84GmQG8RuUtV1dliiBrCPS3blaj2OOU3xczVjcO8X6Mx0wHvYFZsrwGTVHWW671e4DxmZ0xbVd3ppTLdIiL3YkYCYZje43OYqYTHgQEi8qxrfnU0mwdyKgODxLjPQ1XfxyyK/Soi96qxl31SVfd41pL/NlZZxoHz5eyP6eGMFpF3MPOETwN7ROR/jhH0Soyd3xxVvaKqV4AvNYmLBY5xeJT3mQWOkfZh4BLQTEQaiUg5zBcwI6Z36zGqugjzJd4gItlV9d+klBcTZ+/4yyKS1Ukqh1l1T4exIBgLPOMYZH+EWcGtISKNnfp5pEBioqpXVXWOqh7wRnnx4fzQDsXYuo7EjAbOY7ZubgH6Aps8LDvmD8gRzI9nfee5QY37taMY36bp1EPPUxYbCtctItKEGw4bPsD0SCIwizmtMMPTp70sM3rniIjUwdhs9lHVdU5aFcywuRqmx/kwxibxfuBFTaLPQxFpixkuVsXpWCalPJdy6wI9MQ4x3gXCMWY93wHPqOpOxyD7LszWv0DMvOIUVT3tjTr4GhGpjfEJ0BHz3HRX1X+ckcFETO/2mIdlu3qDegjzo3MJM7/8PGZB7HfgX8yzMVpVDyetRf9tbM/SDaq6EAjFPORtMfODj2K+7DkxfiKLeUueGN+Co8SJiYJ54Mc7154QkSjfhtMxc1/NMbtZRgDfJlVRAqjqrxhnDZHeUpQO64BPMQtHz2CmK85jzJTOOz8CxzC7m/5xlMjYVKwoq2N2xWx2/h8EXnK2pkbt3vKG96DHMGZduzAbCBpjtrfuxcxRDgLGWUWZdGzPMg6i9kg7D31rzKrwd8CHmFXFxsBCVV3mRZnZMbsp/DErv1cwJjwZMNMBxzA9yXGqukKMU+GxwCdJnR9NDkSkMHBOVf9xzgMwu1LOYRY3hmGsCiph5i2fd5T1TT2n1IaIFMeYju1V1fecdt+HGYIXwkynjFPV3zwouwBwVlUvi3EdNxYzNdQJ4/+ylbrYwTpTKueS2CQLVlnGixjHtt9hTHOeVWMIftNw2QsyohRz1P/nMcP8hzEKMoOqnneUz49Af1Xd7A3ZyYmY8BM/YiwFVER+wfzQfI/ZUnkKY8pyF5BZVY+kZiUJ4DjFKI75IciA+QHY6xib18EsxoWp6mAnf4LbKyK5MLamRzE/kJdEZIwjJydmBHTVeX42e/OH3GKH4fHiDANfxRhlz4Ro5eYtRVkceENExgJjHYcHYzGrmB8BFR1F2RZjZvNmalCUAE7PqStwUEQWANtUdaCqbsS4sSuMUSr/qAmc5bWFnJRARIoCv2K2GA7C/DA8ICL3OFMkqzDbLINEZFDUIl4iRJzBeAbKA/RxFnhOYObUezmKsjNmd9ARrzXMAtieZYJwegufYBZ2fvTG3KBTbgmMV6AvMXaOxTA7L9pglPMAjAHzm8BpIK+qrk9tvS8RaYh579I4PcyoVdz7geOp3ZQl6vMQE8XzBYw9bn9Mb68vxpXdd6q63+lh1gX+UNVTCSy/GMbWdq/z3rXCzFdvU9VPRWQ8xqLgKGYXVr/bcVomtWOVZQJx5i4DVHWNl8orjRnev66qM13SX8XYcdbEPPwvYJRKB8csKVUiJtzFRxg/kqEpXZ+kIiLBGCe5h8TErznkpN+N+ZGriunxFcAswHyoqgc9kHM3pkcZilkoigA+w4TTuAc44SjMspgNEaGqGpLkBlpuwSrLFMIxC1qhqn7OeQZ1fFyKyAeYXkkvzEJPxjthNdMxRJ8IlFTVv1O6Pp4iIiUxUyLDgWUYF3AzVPUV53oQxtj+Lsye7+tq4pJ7Ku9+jKekZzD2qdkwZkL/YoLG/QZ8462pIUvs2DnLFEJVVwEtReSgiNztzDeldy6vx/yQRarq6TtBUQKocd7bF7OXPlUiIoUwi1bvq+pUNc6PG2KiXL4IoMZ70yaMA+FCSVGUTnlLMDusnsD0WgdilHQBbqyy2+BiyYwNK5GCqOo8ERmA2TVTzcXE4zrG9jAtEO6tOdLbAb0RoiJVzbu6cB/GY9CXzvxjZcyCyzTgWRGJxAyZ2wCPq3HZl2RUdZGYEBg7gZqq+q2IzMT4MM0YZZ5lST6sskxhXBTmJqCIs+gzCmOm5NUth7cTqVRRglnhfsSZUuiCMdupiBmWH8SYDVUAPvCWooxCVec4ynidmLC1Z71ZvsU9VlneBjgK80kRuQIcwjh+nZ/S9bLEykbMDqrRGGfCH2F6e4Uwe/SHAFeiVv29/aPgPCtpgd9EpMqdNOq43bELPLcRjolNFlWdkdJ1sbgn5s4YEWmA2Y3UATiZ3D1nMYHOkjQXakkcVlnehqTi+bz/HI4NbmOMu7lXouZkLXcedjX8NsQqytSBoyirY1anh1hFeWdje5YWSxJwFObdqnrSjgjubKyytFgslgRgh+EWi8WSAKyytFgslgRglaXFYrEkAKssLRaLJQFYZWkBQEQiRGSriOwUkeliwvp6WtY3ItLRef2F444urrwNxAT2SqyMw47/yASlx8iTKGNuERnm7Mu2/IexytISxVVVraiqZTGuvx5zvSgm3k+iUdVHVHW3mywNgEQrS4vF11hlaYmNlcA9Tq9vqYhMAXaIiL+IvCsiG0Vku4g8CmbHkYh8LCK7RWQOxhcnzrVlIlLVed1MRLaIyDYRWey4O3sMeM7p1dYVkSAR+cmRsVFE7nXuvVtEForI7yLyKRAzZvYtiMgvIrJZRHaJSP8Y19536rLY8T+JiBQVkfnOPSsdv5UWC2AdaVhiICYSYXNMrBgwO1TKOh7B+2Pi5VQTkXTAahFZiInOWALjmDYXsBv4Kka5QZgY6PWcsrKr6jkR+QS4pKrvOfmmYLyKrxITyXABUAoTy3yVqg4XkZaYsA3x0deRkQHYKCI/OZ56MgFbVPV5ERnqlD0A44H8MSf8Qw1MGOL7PXgbLXcgVllaosggIlud1ysxcYFqAxuiQiYATYDyUfORGE/gxTBxgr5X1QjguIgsiaX8mhjP8IcANO7wrI2A0hIdpocsYuKo18PEJ4pyVZYQT+tPi8gDzuv8Tl3PYuIdTXPSJwM/i4nnXRuY7iLbOtS1RGOVpSWKq6pa0TXBURqXXZOApxyP5675WgDxbQWTBOQBMzVUKyrERoy6JHi7meMFqJFT1hURWQakjyO7OnLPx3wPLJYo7JylJTEsAB539kMjIsVFJBOwAujqzGkGY7yJx2QtUF9M7HNEJLuTfhHI7JJvIWZIjJOvovNyBSbEKyLSHBOHxh13AX87irIkpmcbhR8Q1TvujhneXwAOiUgnR4aISKoNf2HxPlZZWhLDF5j5yC0ishP4FDM6mQHsB3YAE4DlMW904tL0xwx5t3FjGDwLE1t7q4jUBZ4GqjoLSLu5sSr/BlBPRLZgpgP+iqeu84EAEdmOCSW8zuXaZaCMiGzGzEkOd9IfBB526rcLaJuA98TyH8E60rBYLJYEYHuWFovFkgCssrRYLJYEYJWlxWKxJACrLC0WiyUBWGVpsVgsCcAqS4vFYkkAVllaLBZLAvg/eRE3R2bDhFsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Predict CNN')\n",
    "accuracy(predict_cnn)\n",
    "ConfusionMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict SIFT_CNN\n",
      "Accuracy on test set :62.17609361939259%\n",
      "7178\n",
      "7178\n",
      "0.6217609361939259\n",
      "[[ 551    3   77   51  105   37  161]\n",
      " [  46   25   18    2    6    4    1]\n",
      " [ 137    7  363   58  190  138  150]\n",
      " [  49    1   35 1481   49   38  112]\n",
      " [ 136    2  140   54  538   24  316]\n",
      " [  18    0   75   49   11  610   32]\n",
      " [  90    1   61   95  114   22  895]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Predict SIFT_CNN')\n",
    "accuracy(predict_scnn)\n",
    "ConfusionMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict DSIFT_CNN\n",
      "Accuracy on test set :24.630816383393704%\n",
      "7178\n",
      "7178\n",
      "0.24630816383393703\n",
      "[[   0    0    3  979    0    0    3]\n",
      " [   0    0    0  102    0    0    0]\n",
      " [   0    1    0 1036    0    2    4]\n",
      " [   0    0    0 1761    0    0    4]\n",
      " [   0    0    0 1204    0    0    6]\n",
      " [   0    1    1  788    0    2    3]\n",
      " [   0    0    2 1271    0    0    5]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Predict DSIFT_CNN')\n",
    "accuracy(predict_dcnn)\n",
    "ConfusionMatrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
